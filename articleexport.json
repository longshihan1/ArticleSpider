[{"title": "Redis基础、高级特性与性能调优", "url": "http://blog.jobbole.com/114445/", "create_date": "2018/10/16", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2016/04/49961db8952e63d98b519b76a2daa5e2.png"], "fav_nums": 5, "comment_nums": 0, "vote_nums": 2, "tags": "IT技术,Redis", "object_id": "55c50c6d3dec21284eb5d9158373da19", "content": "Redis是一个开源的，基于内存的结构化数据存储媒介，可以作为数据库、缓存服务或消息服务使用。\nRedis支持多种数据结构，包括字符串、哈希表、链表、集合、有序集合、位图、Hyperloglogs等。\nRedis具备LRU淘汰、事务实现、以及不同级别的硬盘持久化等能力，并且支持副本集和通过Redis Sentinel实现的高可用方案，同时还支持通过Redis Cluster实现的数据自动分片能力。<p/>\nRedis的主要功能都基于单线程模型实现，也就是说Redis使用一个线程来服务所有的客户端请求，同时Redis采用了非阻塞式IO，并精细地优化各种命令的算法时间复杂度，这些信息意味着：<p/>\nRedis是线程安全的（因为只有一个线程），其所有操作都是原子的，不会因并发产生数据异常\nRedis的速度非常快（因为使用非阻塞式IO，且大部分命令的算法时间复杂度都是O(1))\n使用高耗时的Redis命令是很危险的，会占用唯一的一个线程的大量处理时间，导致所有的请求都被拖慢。（例如时间复杂度为O(N)的KEYS命令，严格禁止在生产环境中使用）\nRedis的数据结构和相关常用命令\n本节中将介绍Redis支持的主要数据结构，以及相关的常用Redis命令。本节只对Redis命令进行扼要的介绍，且只列出了较常用的命令。如果想要了解完整的Redis命令集，或了解某个命令的详细使用方法，请参考官方文档：https://redis.io/commands<p/>\nRedis采用Key-Value型的基本数据结构，任何二进制序列都可以作为Redis的Key使用（例如普通的字符串或一张JPEG图片）\n关于Key的一些注意事项：<p/>\n不要使用过长的Key。例如使用一个1024字节的key就不是一个好主意，不仅会消耗更多的内存，还会导致查找的效率降低\nKey短到缺失了可读性也是不好的，例如u1000flw比起user:1000:followers来说，节省了寥寥的存储空间，却引发了可读性和可维护性上的麻烦\n最好使用统一的规范来设计Key，比如object-type:id:attr，以这一规范设计出的Key可能是user:1000或comment:1234:reply-to\nRedis允许的最大Key长度是512MB（对Value的长度限制也是512MB）\nString\nString是Redis的基础数据类型，Redis没有Int、Float、Boolean等数据类型的概念，所有的基本类型在Redis中都以String体现。<p/>\n与String相关的常用命令：<p/>\nSET：为一个key设置value，可以配合EX/PX参数指定key的有效期，通过NX/XX参数针对key是否存在的情况进行区别操作，时间复杂度O(1)\nGET：获取某个key对应的value，时间复杂度O(1)\nGETSET：为一个key设置value，并返回该key的原value，时间复杂度O(1)\nMSET：为多个key设置value，时间复杂度O(N)\nMSETNX：同MSET，如果指定的key中有任意一个已存在，则不进行任何操作，时间复杂度O(N)\nMGET：获取多个key对应的value，时间复杂度O(N)\n上文提到过，Redis的基本数据类型只有String，但Redis可以把String作为整型或浮点型数字来使用，主要体现在INCR、DECR类的命令上：<p/>\nINCR：将key对应的value值自增1，并返回自增后的值。只对可以转换为整型的String数据起作用。时间复杂度O(1)\nINCRBY：将key对应的value值自增指定的整型数值，并返回自增后的值。只对可以转换为整型的String数据起作用。时间复杂度O(1)\nDECR/DECRBY：同INCR/INCRBY，自增改为自减。\nINCR/DECR系列命令要求操作的value类型为String，并可以转换为64位带符号的整型数字，否则会返回错误。\n也就是说，进行INCR/DECR系列命令的value，必须在[-2^63 ~ 2^63  1]范围内。<p/>\n前文提到过，Redis采用单线程模型，天然是线程安全的，这使得INCR/DECR命令可以非常便利的实现高并发场景下的精确控制。<p/>\n例1：库存控制\n在高并发场景下实现库存余量的精准校验，确保不出现超卖的情况。<p/>\n设置库存总量：<p/>\r\nSET inv:remain \"100\" \r\n库存扣减+余量校验：<p/>\r\nDECR inv:remain \r\n当DECR命令返回值大于等于0时，说明库存余量校验通过，如果返回小于0的值，则说明库存已耗尽。<p/>\n假设同时有300个并发请求进行库存扣减，Redis能够确保这300个请求分别得到99到-200的返回值，每个请求得到的返回值都是唯一的，绝对不会找出现两个请求得到一样的返回值的情况。<p/>\n例2：自增序列生成\n实现类似于RDBMS的Sequence功能，生成一系列唯一的序列号<p/>\n设置序列起始值：<p/>\r\nSET sequence \"10000\" \r\n获取一个序列值：<p/>\r\nINCR sequence \r\n直接将返回值作为序列使用即可。<p/>\n获取一批（如100个）序列值：<p/>\r\nINCRBY sequence 100 \r\n假设返回值为N，那么[N  99 ~ N]的数值都是可用的序列值。<p/>\n当多个客户端同时向Redis申请自增序列时，Redis能够确保每个客户端得到的序列值或序列范围都是全局唯一的，绝对不会出现不同客户端得到了重复的序列值的情况。<p/>\nRedis的List是链表型的数据结构，可以使用LPUSH/RPUSH/LPOP/RPOP等命令在List的两端执行插入元素和弹出元素的操作。虽然List也支持在特定index上插入和读取元素的功能，但其时间复杂度较高（O(N)），应小心使用。<p/>\n与List相关的常用命令：<p/>\nLPUSH：向指定List的左侧（即头部）插入1个或多个元素，返回插入后的List长度。时间复杂度O(N)，N为插入元素的数量\nRPUSH：同LPUSH，向指定List的右侧（即尾部）插入1或多个元素\nLPOP：从指定List的左侧（即头部）移除一个元素并返回，时间复杂度O(1)\nRPOP：同LPOP，从指定List的右侧（即尾部）移除1个元素并返回\nLPUSHX/RPUSHX：与LPUSH/RPUSH类似，区别在于，LPUSHX/RPUSHX操作的key如果不存在，则不会进行任何操作\nLLEN：返回指定List的长度，时间复杂度O(1)\nLRANGE：返回指定List中指定范围的元素（双端包含，即LRANGE key 0 10会返回11个元素），时间复杂度O(N)。应尽可能控制一次获取的元素数量，一次获取过大范围的List元素会导致延迟，同时对长度不可预知的List，避免使用LRANGE key 0 -1这样的完整遍历操作。\n应谨慎使用的List相关命令：<p/>\nLINDEX：返回指定List指定index上的元素，如果index越界，返回nil。index数值是回环的，即-1代表List最后一个位置，-2代表List倒数第二个位置。时间复杂度O(N)\nLSET：将指定List指定index上的元素设置为value，如果index越界则返回错误，时间复杂度O(N)，如果操作的是头/尾部的元素，则时间复杂度为O(1)\nLINSERT：向指定List中指定元素之前/之后插入一个新元素，并返回操作后的List长度。如果指定的元素不存在，返回-1。如果指定key不存在，不会进行任何操作，时间复杂度O(N)\n由于Redis的List是链表结构的，上述的三个命令的算法效率较低，需要对List进行遍历，命令的耗时无法预估，在List长度大的情况下耗时会明显增加，应谨慎使用。<p/>\n换句话说，Redis的List实际是设计来用于实现队列，而不是用于实现类似ArrayList这样的列表的。如果你不是想要实现一个双端出入的队列，那么请尽量不要使用Redis的List数据结构。<p/>\n为了更好支持队列的特性，Redis还提供了一系列阻塞式的操作命令，如BLPOP/BRPOP等，能够实现类似于BlockingQueue的能力，即在List为空时，阻塞该连接，直到List中有对象可以出队时再返回。针对阻塞类的命令，此处不做详细探讨，请参考官方文档（https://redis.io/topics/data-types-intro） 中Blocking operations on lists一节。<p/>\nHash即哈希表，Redis的Hash和传统的哈希表一样，是一种field-value型的数据结构，可以理解成将HashMap搬入Redis。\nHash非常适合用于表现对象类型的数据，用Hash中的field对应对象的field即可。\nHash的优点包括：<p/>\n可以实现二元查找，如查找ID为1000的用户的年龄\n比起将整个对象序列化后作为String存储的方法，Hash能够有效地减少网络传输的消耗\n当使用Hash维护一个集合时，提供了比List效率高得多的随机访问命令\n与Hash相关的常用命令：<p/>\nHSET：将key对应的Hash中的field设置为value。如果该Hash不存在，会自动创建一个。时间复杂度O(1)\nHGET：返回指定Hash中field字段的值，时间复杂度O(1)\nHMSET/HMGET：同HSET和HGET，可以批量操作同一个key下的多个field，时间复杂度：O(N)，N为一次操作的field数量\nHSETNX：同HSET，但如field已经存在，HSETNX不会进行任何操作，时间复杂度O(1)\nHEXISTS：判断指定Hash中field是否存在，存在返回1，不存在返回0，时间复杂度O(1)\nHDEL：删除指定Hash中的field（1个或多个），时间复杂度：O(N)，N为操作的field数量\nHINCRBY：同INCRBY命令，对指定Hash中的一个field进行INCRBY，时间复杂度O(1)\n应谨慎使用的Hash相关命令：<p/>\nHGETALL：返回指定Hash中所有的field-value对。返回结果为数组，数组中field和value交替出现。时间复杂度O(N)\nHKEYS/HVALS：返回指定Hash中所有的field/value，时间复杂度O(N)\n上述三个命令都会对Hash进行完整遍历，Hash中的field数量与命令的耗时线性相关，对于尺寸不可预知的Hash，应严格避免使用上面三个命令，而改为使用HSCAN命令进行游标式的遍历，具体请见 https://redis.io/commands/scan<p/>\nRedis Set是无序的，不可重复的String集合。<p/>\n与Set相关的常用命令：<p/>\nSADD：向指定Set中添加1个或多个member，如果指定Set不存在，会自动创建一个。时间复杂度O(N)，N为添加的member个数\nSREM：从指定Set中移除1个或多个member，时间复杂度O(N)，N为移除的member个数\nSRANDMEMBER：从指定Set中随机返回1个或多个member，时间复杂度O(N)，N为返回的member个数\nSPOP：从指定Set中随机移除并返回count个member，时间复杂度O(N)，N为移除的member个数\nSCARD：返回指定Set中的member个数，时间复杂度O(1)\nSISMEMBER：判断指定的value是否存在于指定Set中，时间复杂度O(1)\nSMOVE：将指定member从一个Set移至另一个Set\n慎用的Set相关命令：<p/>\nSMEMBERS：返回指定Hash中所有的member，时间复杂度O(N)\nSUNION/SUNIONSTORE：计算多个Set的并集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数\nSINTER/SINTERSTORE：计算多个Set的交集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数\nSDIFF/SDIFFSTORE：计算1个Set与1或多个Set的差集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数\n上述几个命令涉及的计算量大，应谨慎使用，特别是在参与计算的Set尺寸不可知的情况下，应严格避免使用。可以考虑通过SSCAN命令遍历获取相关Set的全部member（具体请见 https://redis.io/commands/scan ），如果需要做并集/交集/差集计算，可以在客户端进行，或在不服务实时查询请求的Slave上进行。<p/>\nSorted Set\nRedis Sorted Set是有序的、不可重复的String集合。Sorted Set中的每个元素都需要指派一个分数(score)，Sorted Set会根据score对元素进行升序排序。如果多个member拥有相同的score，则以字典序进行升序排序。<p/>\nSorted Set非常适合用于实现排名。<p/>\nSorted Set的主要命令：<p/>\nZADD：向指定Sorted Set中添加1个或多个member，时间复杂度O(Mlog(N))，M为添加的member数量，N为Sorted Set中的member数量\nZREM：从指定Sorted Set中删除1个或多个member，时间复杂度O(Mlog(N))，M为删除的member数量，N为Sorted Set中的member数量\nZCOUNT：返回指定Sorted Set中指定score范围内的member数量，时间复杂度：O(log(N))\nZCARD：返回指定Sorted Set中的member数量，时间复杂度O(1)\nZSCORE：返回指定Sorted Set中指定member的score，时间复杂度O(1)\nZRANK/ZREVRANK：返回指定member在Sorted Set中的排名，ZRANK返回按升序排序的排名，ZREVRANK则返回按降序排序的排名。时间复杂度O(log(N))\nZINCRBY：同INCRBY，对指定Sorted Set中的指定member的score进行自增，时间复杂度O(log(N))\n慎用的Sorted Set相关命令：<p/>\nZRANGE/ZREVRANGE：返回指定Sorted Set中指定排名范围内的所有member，ZRANGE为按score升序排序，ZREVRANGE为按score降序排序，时间复杂度O(log(N)+M)，M为本次返回的member数\nZRANGEBYSCORE/ZREVRANGEBYSCORE：返回指定Sorted Set中指定score范围内的所有member，返回结果以升序/降序排序，min和max可以指定为-inf和+inf，代表返回所有的member。时间复杂度O(log(N)+M)\nZREMRANGEBYRANK/ZREMRANGEBYSCORE：移除Sorted Set中指定排名范围/指定score范围内的所有member。时间复杂度O(log(N)+M)\n上述几个命令，应尽量避免传递[0 -1]或[-inf +inf]这样的参数，来对Sorted Set做一次性的完整遍历，特别是在Sorted Set的尺寸不可预知的情况下。可以通过ZSCAN命令来进行游标式的遍历（具体请见 https://redis.io/commands/scan ），或通过LIMIT参数来限制返回member的数量（适用于ZRANGEBYSCORE和ZREVRANGEBYSCORE命令），以实现游标式的遍历。<p/>\nBitmap和HyperLogLog\nRedis的这两种数据结构相较之前的并不常用，在本文中只做简要介绍，如想要详细了解这两种数据结构与其相关的命令，请参考官方文档https://redis.io/topics/data-types-intro 中的相关章节<p/>\nBitmap在Redis中不是一种实际的数据类型，而是一种将String作为Bitmap使用的方法。可以理解为将String转换为bit数组。使用Bitmap来存储true/false类型的简单数据极为节省空间。<p/>\nHyperLogLogs是一种主要用于数量统计的数据结构，它和Set类似，维护一个不可重复的String集合，但是HyperLogLogs并不维护具体的member内容，只维护member的个数。也就是说，HyperLogLogs只能用于计算一个集合中不重复的元素数量，所以它比Set要节省很多内存空间。<p/>\n其他常用命令\nEXISTS：判断指定的key是否存在，返回1代表存在，0代表不存在，时间复杂度O(1)\nDEL：删除指定的key及其对应的value，时间复杂度O(N)，N为删除的key数量\nEXPIRE/PEXPIRE：为一个key设置有效期，单位为秒或毫秒，时间复杂度O(1)\nTTL/PTTL：返回一个key剩余的有效时间，单位为秒或毫秒，时间复杂度O(1)\nRENAME/RENAMENX：将key重命名为newkey。使用RENAME时，如果newkey已经存在，其值会被覆盖；使用RENAMENX时，如果newkey已经存在，则不会进行任何操作，时间复杂度O(1)\nTYPE：返回指定key的类型，string, list, set, zset, hash。时间复杂度O(1)\nCONFIG GET：获得Redis某配置项的当前值，可以使用*通配符，时间复杂度O(1)\nCONFIG SET：为Redis某个配置项设置新值，时间复杂度O(1)\nCONFIG REWRITE：让Redis重新加载redis.conf中的配置\n数据持久化\nRedis提供了将数据定期自动持久化至硬盘的能力，包括RDB和AOF两种方案，两种方案分别有其长处和短板，可以配合起来同时运行，确保数据的稳定性。<p/>\n必须使用数据持久化吗？\nRedis的数据持久化机制是可以关闭的。如果你只把Redis作为缓存服务使用，Redis中存储的所有数据都不是该数据的主体而仅仅是同步过来的备份，那么可以关闭Redis的数据持久化机制。\n但通常来说，仍然建议至少开启RDB方式的数据持久化，因为：<p/>\nRDB方式的持久化几乎不损耗Redis本身的性能，在进行RDB持久化时，Redis主进程唯一需要做的事情就是fork出一个子进程，所有持久化工作都由子进程完成\nRedis无论因为什么原因crash掉之后，重启时能够自动恢复到上一次RDB快照中记录的数据。这省去了手工从其他数据源（如DB）同步数据的过程，而且要比其他任何的数据恢复方式都要快\n现在硬盘那么大，真的不缺那一点地方\n采用RDB持久方式，Redis会定期保存数据快照至一个rbd文件中，并在启动时自动加载rdb文件，恢复之前保存的数据。可以在配置文件中配置Redis进行快照保存的时机：<p/>\r\nsave [seconds] [changes] \r\n意为在[seconds]秒内如果发生了[changes]次数据修改，则进行一次RDB快照保存，例如<p/>\r\nsave 60 100 \r\n会让Redis每60秒检查一次数据变更情况，如果发生了100次或以上的数据变更，则进行RDB快照保存。\n可以配置多条save指令，让Redis执行多级的快照保存策略。\nRedis默认开启RDB快照，默认的RDB策略如下：<p/>\r\n1234\r\nsave 900 1save 300 10save 60 10000 \r\n也可以通过BGSAVE命令手工触发RDB快照保存。<p/>\nRDB的优点：<p/>\n对性能影响最小。如前文所述，Redis在保存RDB快照时会fork出子进程进行，几乎不影响Redis处理客户端请求的效率。\n每次快照会生成一个完整的数据快照文件，所以可以辅以其他手段保存多个时间点的快照（例如把每天0点的快照备份至其他存储媒介中），作为非常可靠的灾难恢复手段。\n使用RDB文件进行数据恢复比使用AOF要快很多。\nRDB的缺点：<p/>\n快照是定期生成的，所以在Redis crash时或多或少会丢失一部分数据。\n如果数据集非常大且CPU不够强（比如单核CPU），Redis在fork子进程时可能会消耗相对较长的时间（长至1秒），影响这期间的客户端请求。\n采用AOF持久方式时，Redis会把每一个写请求都记录在一个日志文件里。在Redis重启时，会把AOF文件中记录的所有写操作顺序执行一遍，确保数据恢复到最新。<p/>\nAOF默认是关闭的，如要开启，进行如下配置：<p/>\r\nappendonly yes \r\nAOF提供了三种fsync配置，always/everysec/no，通过配置项[appendfsync]指定：<p/>\nappendfsync no：不进行fsync，将flush文件的时机交给OS决定，速度最快\nappendfsync always：每写入一条日志就进行一次fsync操作，数据安全性最高，但速度最慢\nappendfsync everysec：折中的做法，交由后台线程每秒fsync一次\n随着AOF不断地记录写操作日志，必定会出现一些无用的日志，例如某个时间点执行了命令SET key1 abc，在之后某个时间点又执行了SET key1 bcd，那么第一条命令很显然是没有用的。大量的无用日志会让AOF文件过大，也会让数据恢复的时间过长。\n所以Redis提供了AOF rewrite功能，可以重写AOF文件，只保留能够把数据恢复到最新状态的最小写操作集。\nAOF rewrite可以通过BGREWRITEAOF命令触发，也可以配置Redis定期自动进行：<p/>\r\nauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb \r\n上面两行配置的含义是，Redis在每次AOF rewrite时，会记录完成rewrite后的AOF日志大小，当AOF日志大小在该基础上增长了100%后，自动进行AOF rewrite。同时如果增长的大小没有达到64mb，则不会进行rewrite。<p/>\nAOF的优点：<p/>\n最安全，在启用appendfsync always时，任何已写入的数据都不会丢失，使用在启用appendfsync everysec也至多只会丢失1秒的数据。\nAOF文件在发生断电等问题时也不会损坏，即使出现了某条日志只写入了一半的情况，也可以使用redis-check-aof工具轻松修复。\nAOF文件易读，可修改，在进行了某些错误的数据清除操作后，只要AOF文件没有rewrite，就可以把AOF文件备份出来，把错误的命令删除，然后恢复数据。\nAOF的缺点：<p/>\nAOF文件通常比RDB文件更大\n性能消耗比RDB高\n数据恢复速度比RDB慢\n内存管理与数据淘汰机制\n最大内存设置\n默认情况下，在32位OS中，Redis最大使用3GB的内存，在64位OS中则没有限制。<p/>\n在使用Redis时，应该对数据占用的最大空间有一个基本准确的预估，并为Redis设定最大使用的内存。否则在64位OS中Redis会无限制地占用内存（当物理内存被占满后会使用swap空间），容易引发各种各样的问题。<p/>\n通过如下配置控制Redis使用的最大内存：<p/>\r\nmaxmemory 100mb \r\n在内存占用达到了maxmemory后，再向Redis写入数据时，Redis会：<p/>\n根据配置的数据淘汰策略尝试淘汰数据，释放空间\n如果没有数据可以淘汰，或者没有配置数据淘汰策略，那么Redis会对所有写请求返回错误，但读请求仍然可以正常执行\n在为Redis设置maxmemory时，需要注意：<p/>\n如果采用了Redis的主从同步，主节点向从节点同步数据时，会占用掉一部分内存空间，如果maxmemory过于接近主机的可用内存，导致数据同步时内存不足。所以设置的maxmemory不要过于接近主机可用的内存，留出一部分预留用作主从同步。\n数据淘汰机制\nRedis提供了5种数据淘汰策略：<p/>\nvolatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key\nallkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰\nvolatile-random：随机淘汰数据，只淘汰设定了有效期的key\nallkeys-random：随机淘汰数据，所有的key都可以被淘汰\nvolatile-ttl：淘汰剩余有效期最短的key\n最好为Redis指定一种有效的数据淘汰策略以配合maxmemory设置，避免在内存使用满后发生写入失败的情况。<p/>\n一般来说，推荐使用的策略是volatile-lru，并辨识Redis中保存的数据的重要性。对于那些重要的，绝对不能丢弃的数据（如配置类数据等），应不设置有效期，这样Redis就永远不会淘汰这些数据。对于那些相对不是那么重要的，并且能够热加载的数据（比如缓存最近登录的用户信息，当在Redis中找不到时，程序会去DB中读取），可以设置上有效期，这样在内存不够时Redis就会淘汰这部分数据。<p/>\n配置方法：<p/>\r\nmaxmemory-policy volatile-lru   #默认是noeviction，即不进行数据淘汰 \r\nPipelining\nPipelining\nRedis提供许多批量操作的命令，如MSET/MGET/HMSET/HMGET等等，这些命令存在的意义是减少维护网络连接和传输数据所消耗的资源和时间。\n例如连续使用5次SET命令设置5个不同的key，比起使用一次MSET命令设置5个不同的key，效果是一样的，但前者会消耗更多的RTT(Round Trip Time)时长，永远应优先使用后者。<p/>\n然而，如果客户端要连续执行的多次操作无法通过Redis命令组合在一起，例如：<p/>\r\n1234\r\nSET a \"abc\"INCR bHSET c name \"hi\" \r\n此时便可以使用Redis提供的pipelining功能来实现在一次交互中执行多条命令。\n使用pipelining时，只需要从客户端一次向Redis发送多条命令（以rn）分隔，Redis就会依次执行这些命令，并且把每个命令的返回按顺序组装在一起一次返回，比如：<p/>\r\n12345\r\n$ (printf \"PINGrnPINGrnPINGrn\"; sleep 1) | nc localhost 6379+PONG+PONG+PONG \r\n大部分的Redis客户端都对Pipelining提供支持，所以开发者通常并不需要自己手工拼装命令列表。<p/>\nPipelining的局限性\nPipelining只能用于执行连续且无相关性的命令，当某个命令的生成需要依赖于前一个命令的返回时，就无法使用Pipelining了。<p/>\n通过Scripting功能，可以规避这一局限性<p/>\n事务与Scripting\nPipelining能够让Redis在一次交互中处理多条命令，然而在一些场景下，我们可能需要在此基础上确保这一组命令是连续执行的。<p/>\n比如获取当前累计的PV数并将其清0<p/>\r\n12345\r\n> GET vCount12384> SET vCount 0OK \r\n如果在GET和SET命令之间插进来一个INCR vCount，就会使客户端拿到的vCount不准确。<p/>\nRedis的事务可以确保复数命令执行时的原子性。也就是说Redis能够保证：一个事务中的一组命令是绝对连续执行的，在这些命令执行完成之前，绝对不会有来自于其他连接的其他命令插进去执行。<p/>\n通过MULTI和EXEC命令来把这两个命令加入一个事务中：<p/>\r\n12345678910\r\n> MULTIOK> GET vCountQUEUED> SET vCount 0QUEUED> EXEC1) 123842) OK \r\nRedis在接收到MULTI命令后便会开启一个事务，这之后的所有读写命令都会保存在队列中但并不执行，直到接收到EXEC命令后，Redis会把队列中的所有命令连续顺序执行，并以数组形式返回每个命令的返回结果。<p/>\n可以使用DISCARD命令放弃当前的事务，将保存的命令队列清空。<p/>\n需要注意的是，Redis事务不支持回滚：\n如果一个事务中的命令出现了语法错误，大部分客户端驱动会返回错误，2.6.5版本以上的Redis也会在执行EXEC时检查队列中的命令是否存在语法错误，如果存在，则会自动放弃事务并返回错误。\n但如果一个事务中的命令有非语法类的错误（比如对String执行HSET操作），无论客户端驱动还是Redis都无法在真正执行这条命令之前发现，所以事务中的所有命令仍然会被依次执行。在这种情况下，会出现一个事务中部分命令成功部分命令失败的情况，然而与RDBMS不同，Redis不提供事务回滚的功能，所以只能通过其他方法进行数据的回滚。<p/>\n通过事务实现CAS\nRedis提供了WATCH命令与事务搭配使用，实现CAS乐观锁的机制。<p/>\n假设要实现将某个商品的状态改为已售：<p/>\r\nif(exec(HGET stock:1001 state) == \"in stock\")    exec(HSET stock:1001 state \"sold\"); \r\n这一伪代码执行时，无法确保并发安全性，有可能多个客户端都获取到了in stock的状态，导致一个库存被售卖多次。<p/>\n使用WATCH命令和事务可以解决这一问题：<p/>\r\n1234567\r\nexec(WATCH stock:1001);if(exec(HGET stock:1001 state) == \"in stock\") {    exec(MULTI);    exec(HSET stock:1001 state \"sold\");    exec(EXEC);} \r\nWATCH的机制是：在事务EXEC命令执行时，Redis会检查被WATCH的key，只有被WATCH的key从WATCH起始时至今没有发生过变更，EXEC才会被执行。如果WATCH的key在WATCH命令到EXEC命令之间发生过变化，则EXEC命令会返回失败。<p/>\nScripting\n通过EVAL与EVALSHA命令，可以让Redis执行LUA脚本。这就类似于RDBMS的存储过程一样，可以把客户端与Redis之间密集的读/写交互放在服务端进行，避免过多的数据交互，提升性能。<p/>\nScripting功能是作为事务功能的替代者诞生的，事务提供的所有能力Scripting都可以做到。Redis官方推荐使用LUA Script来代替事务，前者的效率和便利性都超过了事务。<p/>\n关于Scripting的具体使用，本文不做详细介绍，请参考官方文档 https://redis.io/commands/eval<p/>\nRedis性能调优\n尽管Redis是一个非常快速的内存数据存储媒介，也并不代表Redis不会产生性能问题。\n前文中提到过，Redis采用单线程模型，所有的命令都是由一个线程串行执行的，所以当某个命令执行耗时较长时，会拖慢其后的所有命令，这使得Redis对每个任务的执行效率更加敏感。<p/>\n针对Redis的性能优化，主要从下面几个层面入手：<p/>\n最初的也是最重要的，确保没有让Redis执行耗时长的命令\n使用pipelining将连续执行的命令组合执行\n操作系统的Transparent huge pages功能必须关闭：\n<p/>\r\necho never > /sys/kernel/mm/transparent_hugepage/enabled \r\n如果在虚拟机中运行Redis，可能天然就有虚拟机环境带来的固有延迟。可以通过./redis-cli intrinsic-latency 100命令查看固有延迟。同时如果对Redis的性能有较高要求的话，应尽可能在物理机上直接部署Redis。\n检查数据持久化策略\n考虑引入读写分离机制\n长耗时命令\nRedis绝大多数读写命令的时间复杂度都在O(1)到O(N)之间，在文本和官方文档中均对每个命令的时间复杂度有说明。<p/>\n通常来说，O(1)的命令是安全的，O(N)命令在使用时需要注意，如果N的数量级不可预知，则应避免使用。例如对一个field数未知的Hash数据执行HGETALL/HKEYS/HVALS命令，通常来说这些命令执行的很快，但如果这个Hash中的field数量极多，耗时就会成倍增长。\n又如使用SUNION对两个Set执行Union操作，或使用SORT对List/Set执行排序操作等时，都应该严加注意。<p/>\n避免在使用这些O(N)命令时发生问题主要有几个办法：<p/>\n不要把List当做列表使用，仅当做队列来使用\n通过机制严格控制Hash、Set、Sorted Set的大小\n可能的话，将排序、并集、交集等操作放在客户端执行\n绝对禁止使用KEYS命令\n避免一次性遍历集合类型的所有成员，而应使用SCAN类的命令进行分批的，游标式的遍历\nRedis提供了SCAN命令，可以对Redis中存储的所有key进行游标式的遍历，避免使用KEYS命令带来的性能问题。同时还有SSCAN/HSCAN/ZSCAN等命令，分别用于对Set/Hash/Sorted Set中的元素进行游标式遍历。SCAN类命令的使用请参考官方文档：https://redis.io/commands/scan<p/>\nRedis提供了Slow Log功能，可以自动记录耗时较长的命令。相关的配置参数有两个：<p/>\r\nslowlog-log-slower-than xxxms  #执行时间慢于xxx毫秒的命令计入Slow Logslowlog-max-len xxx  #Slow Log的长度，即最大纪录多少条Slow Log \r\n使用SLOWLOG GET [number]命令，可以输出最近进入Slow Log的number条命令。\n使用SLOWLOG RESET命令，可以重置Slow Log<p/>\n网络引发的延迟\n尽可能使用长连接或连接池，避免频繁创建销毁连接\n客户端进行的批量数据操作，应使用Pipeline特性在一次交互中完成。具体请参照本文的Pipelining章节\n数据持久化引发的延迟\nRedis的数据持久化工作本身就会带来延迟，需要根据数据的安全级别和性能要求制定合理的持久化策略：<p/>\nAOF + fsync always的设置虽然能够绝对确保数据安全，但每个操作都会触发一次fsync，会对Redis的性能有比较明显的影响\nAOF + fsync every second是比较好的折中方案，每秒fsync一次\nAOF + fsync never会提供AOF持久化方案下的最优性能\n使用RDB持久化通常会提供比使用AOF更高的性能，但需要注意RDB的策略配置\n每一次RDB快照和AOF Rewrite都需要Redis主进程进行fork操作。fork操作本身可能会产生较高的耗时，与CPU和Redis占用的内存大小有关。根据具体的情况合理配置RDB快照和AOF Rewrite时机，避免过于频繁的fork带来的延迟\nRedis在fork子进程时需要将内存分页表拷贝至子进程，以占用了24GB内存的Redis实例为例，共需要拷贝24GB / 4kB * 8 = 48MB的数据。在使用单Xeon 2.27Ghz的物理机上，这一fork操作耗时216ms。<p/>\n可以通过INFO命令返回的latest_fork_usec字段查看上一次fork操作的耗时（微秒）<p/>\nSwap引发的延迟\n当Linux将Redis所用的内存分页移至swap空间时，将会阻塞Redis进程，导致Redis出现不正常的延迟。Swap通常在物理内存不足或一些进程在进行大量I/O操作时发生，应尽可能避免上述两种情况的出现。<p/>\n/proc/<pid>/smaps文件中会保存进程的swap记录，通过查看这个文件，能够判断Redis的延迟是否由Swap产生。如果这个文件中记录了较大的Swap size，则说明延迟很有可能是Swap造成的。<p/>\n数据淘汰引发的延迟\n当同一秒内有大量key过期时，也会引发Redis的延迟。在使用时应尽量将key的失效时间错开。<p/>\n引入读写分离机制\nRedis的主从复制能力可以实现一主多从的多节点架构，在这一架构下，主节点接收所有写请求，并将数据同步给多个从节点。\n在这一基础上，我们可以让从节点提供对实时性要求不高的读请求服务，以减小主节点的压力。\n尤其是针对一些使用了长耗时命令的统计类任务，完全可以指定在一个或多个从节点上执行，避免这些长耗时命令影响其他请求的响应。<p/>\n关于读写分离的具体说明，请参见后续章节<p/>\n主从复制与集群分片\nRedis支持一主多从的主从复制架构。一个Master实例负责处理所有的写请求，Master将写操作同步至所有Slave。\n借助Redis的主从复制，可以实现读写分离和高可用：<p/>\n实时性要求不是特别高的读请求，可以在Slave上完成，提升效率。特别是一些周期性执行的统计任务，这些任务可能需要执行一些长耗时的Redis命令，可以专门规划出1个或几个Slave用于服务这些统计任务\n借助Redis Sentinel可以实现高可用，当Master crash后，Redis Sentinel能够自动将一个Slave晋升为Master，继续提供服务\n启用主从复制非常简单，只需要配置多个Redis实例，在作为Slave的Redis实例中配置：<p/>\r\nslaveof 192.168.1.1 6379  #指定Master的IP和端口 \r\n当Slave启动后，会从Master进行一次冷启动数据同步，由Master触发BGSAVE生成RDB文件推送给Slave进行导入，导入完成后Master再将增量数据通过Redis Protocol同步给Slave。之后主从之间的数据便一直以Redis Protocol进行同步<p/>\n使用Sentinel做自动failover\nRedis的主从复制功能本身只是做数据同步，并不提供监控和自动failover能力，要通过主从复制功能来实现Redis的高可用，还需要引入一个组件：Redis Sentinel<p/>\nRedis Sentinel是Redis官方开发的监控组件，可以监控Redis实例的状态，通过Master节点自动发现Slave节点，并在监测到Master节点失效时选举出一个新的Master，并向所有Redis实例推送新的主从配置。<p/>\nRedis Sentinel需要至少部署3个实例才能形成选举关系。<p/>\n关键配置：<p/>\r\n12345\r\nsentinel monitor mymaster 127.0.0.1 6379 2  #Master实例的IP、端口，以及选举需要的赞成票数sentinel down-after-milliseconds mymaster 60000  #多长时间没有响应视为Master失效sentinel failover-timeout mymaster 180000  #两次failover尝试间的间隔时长sentinel parallel-syncs mymaster 1  #如果有多个Slave，可以通过此配置指定同时从新Master进行数据同步的Slave数，避免所有Slave同时进行数据同步导致查询服务也不可用 \r\n另外需要注意的是，Redis Sentinel实现的自动failover不是在同一个IP和端口上完成的，也就是说自动failover产生的新Master提供服务的IP和端口与之前的Master是不一样的，所以要实现HA，还要求客户端必须支持Sentinel，能够与Sentinel交互获得新Master的信息才行。<p/>\n为何要做集群分片：<p/>\nRedis中存储的数据量大，一台主机的物理内存已经无法容纳\nRedis的写请求并发量大，一个Redis实例以无法承载\n当上述两个问题出现时，就必须要对Redis进行分片了。\nRedis的分片方案有很多种，例如很多Redis的客户端都自行实现了分片功能，也有向Twemproxy这样的以代理方式实现的Redis分片方案。然而首选的方案还应该是Redis官方在3.0版本中推出的Redis Cluster分片方案。<p/>\n本文不会对Redis Cluster的具体安装和部署细节进行介绍，重点介绍Redis Cluster带来的好处与弊端。<p/>\nRedis Cluster的能力\n能够自动将数据分散在多个节点上\n当访问的key不在当前分片上时，能够自动将请求转发至正确的分片\n当集群中部分节点失效时仍能提供服务\n其中第三点是基于主从复制来实现的，Redis Cluster的每个数据分片都采用了主从复制的结构，原理和前文所述的主从复制完全一致，唯一的区别是省去了Redis Sentinel这一额外的组件，由Redis Cluster负责进行一个分片内部的节点监控和自动failover。<p/>\nRedis Cluster分片原理\nRedis Cluster中共有16384个hash slot，Redis会计算每个key的CRC16，将结果与16384取模，来决定该key存储在哪一个hash slot中，同时需要指定Redis Cluster中每个数据分片负责的Slot数。Slot的分配在任何时间点都可以进行重新分配。<p/>\n客户端在对key进行读写操作时，可以连接Cluster中的任意一个分片，如果操作的key不在此分片负责的Slot范围内，Redis Cluster会自动将请求重定向到正确的分片上。<p/>\nhash tags\n在基础的分片原则上，Redis还支持hash tags功能，以hash tags要求的格式明明的key，将会确保进入同一个Slot中。例如：{uiv}user:1000和{uiv}user:1001拥有同样的hash tag {uiv}，会保存在同一个Slot中。<p/>\n使用Redis Cluster时，pipelining、事务和LUA Script功能涉及的key必须在同一个数据分片上，否则将会返回错误。如要在Redis Cluster中使用上述功能，就必须通过hash tags来确保一个pipeline或一个事务中操作的所有key都位于同一个Slot中。<p/>\n有一些客户端（如Redisson）实现了集群化的pipelining操作，可以自动将一个pipeline里的命令按key所在的分片进行分组，分别发到不同的分片上执行。但是Redis不支持跨分片的事务，事务和LUA Script还是必须遵循所有key在一个分片上的规则要求。<p/>\n主从复制 vs 集群分片\n在设计软件架构时，要如何在主从复制和集群分片两种部署方案中取舍呢？<p/>\n从各个方面看，Redis Cluster都是优于主从复制的方案<p/>\nRedis Cluster能够解决单节点上数据量过大的问题\nRedis Cluster能够解决单节点访问压力过大的问题\nRedis Cluster包含了主从复制的能力\n那是不是代表Redis Cluster永远是优于主从复制的选择呢？<p/>\n并不是。<p/>\n软件架构永远不是越复杂越好，复杂的架构在带来显著好处的同时，一定也会带来相应的弊端。采用Redis Cluster的弊端包括：<p/>\n维护难度增加。在使用Redis Cluster时，需要维护的Redis实例数倍增，需要监控的主机数量也相应增加，数据备份/持久化的复杂度也会增加。同时在进行分片的增减操作时，还需要进行reshard操作，远比主从模式下增加一个Slave的复杂度要高。\n客户端资源消耗增加。当客户端使用连接池时，需要为每一个数据分片维护一个连接池，客户端同时需要保持的连接数成倍增多，加大了客户端本身和操作系统资源的消耗。\n性能优化难度增加。你可能需要在多个分片上查看Slow Log和Swap日志才能定位性能问题。\n事务和LUA Script的使用成本增加。在Redis Cluster中使用事务和LUA Script特性有严格的限制条件，事务和Script中操作的key必须位于同一个分片上，这就使得在开发时必须对相应场景下涉及的key进行额外的规划和规范要求。如果应用的场景中大量涉及事务和Script的使用，如何在保证这两个功能的正常运作前提下把数据平均分到多个数据分片中就会成为难点。\n所以说，在主从复制和集群分片两个方案中做出选择时，应该从应用软件的功能特性、数据和访问量级、未来发展规划等方面综合考虑，只在确实有必要引入数据分片时再使用Redis Cluster。\n下面是一些建议：<p/>\n需要在Redis中存储的数据有多大？未来2年内可能发展为多大？这些数据是否都需要长期保存？是否可以使用LRU算法进行非热点数据的淘汰？综合考虑前面几个因素，评估出Redis需要使用的物理内存。\n用于部署Redis的主机物理内存有多大？有多少可以分配给Redis使用？对比(1)中的内存需求评估，是否足够用？\nRedis面临的并发写压力会有多大？在不使用pipelining时，Redis的写性能可以超过10万次/秒（更多的benchmark可以参考 https://redis.io/topics/benchmarks ）\n在使用Redis时，是否会使用到pipelining和事务功能？使用的场景多不多？\n综合上面几点考虑，如果单台主机的可用物理内存完全足以支撑对Redis的容量需求，且Redis面临的并发写压力距离Benchmark值还尚有距离，建议采用主从复制的架构，可以省去很多不必要的麻烦。同时，如果应用中大量使用pipelining和事务，也建议尽可能选择主从复制架构，可以减少设计和开发时的复杂度。<p/>\nRedis Java客户端的选择\nRedis的Java客户端很多，官方推荐的有三种：Jedis、Redisson和lettuce。<p/>\n在这里对Jedis和Redisson进行对比介绍<p/>\nJedis：<p/>\n轻量，简洁，便于集成和改造\n支持连接池\n支持pipelining、事务、LUA Scripting、Redis Sentinel、Redis Cluster\n不支持读写分离，需要自己实现\n文档差（真的很差，几乎没有……）\nRedisson：<p/>\n基于Netty实现，采用非阻塞IO，性能高\n支持异步请求\n支持连接池\n支持pipelining、LUA Scripting、Redis Sentinel、Redis Cluster\n不支持事务，官方建议以LUA Scripting代替事务\n支持在Redis Cluster架构下使用pipelining\n支持读写分离，支持读负载均衡，在主从复制和Redis Cluster架构下都可以使用\n内建Tomcat Session Manager，为Tomcat 6/7/8提供了会话共享功能\n可以与Spring Session集成，实现基于Redis的会话共享\n文档较丰富，有中文文档\n对于Jedis和Redisson的选择，同样应遵循前述的原理，尽管Jedis比起Redisson有各种各样的不足，但也应该在需要使用Redisson的高级特性时再选用Redisson，避免造成不必要的程序复杂度提升。<p/>\nJedis：\ngithub：https://github.com/xetorthio/jedis\n文档：https://github.com/xetorthio/jedis/wiki<p/>\nRedisson：\ngithub：https://github.com/redisson/redisson\n文档：https://github.com/redisson/redisson/wiki<p/>\n 5 收藏\n", "front_img_path": "full/c766feed221138f7946130756cddfc7e86e388b4.jpg"},{"title": "命令行小技巧：读取文件的不同方式", "url": "http://blog.jobbole.com/114458/", "create_date": "2018/10/21", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/10/683fc47a8ae84cd7b957dda2db9cf665.jpg"], "fav_nums": 3, "comment_nums": 1, "vote_nums": 2, "tags": "IT技术,,Linux", "object_id": "a9bd45a89752136f0978fe478eb5119f", "content": "原文出处： Paul W. Frields   译文出处：Linux中国/distant1219   <img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/683fc47a8ae84cd7b957dda2db9cf665.jpg\" alt=\"\" /><p/>\n作为图形操作系统，Fedora 的使用是令人愉快的。你可以轻松地点击完成任何任务。但你可能已经看到了，在底层还有一个强大的命令行。想要在 shell 下体验，只需要在 Fedora 系统中打开你的终端应用。这篇文章是向你展示常见的命令行使用方法的系列文章之一。<p/>\n在这部分，你将学习如何以不同的方式读取文件，如果你在系统中打开一个终端完成一些工作，你就有可能需要读取一两个文件。<p/>\n一应俱全的大餐\n对命令行终端的用户来说， cat 命令众所周知。 当你 cat 一个文件，你很容易的把整个文件内容展示在你的屏幕上。而真正发生在底层的是文件一次读取一行，然后一行一行写入屏幕。<p/>\n假设你有一个文件，叫做 myfile， 这个文件每行只有一个单词。为了简单起见，每行的单词就是这行的行号，就像这样：<p/>\r\n12345\r\nonetwothreefourfive\r\n所以如果你 cat 这个文件，你就会看到如下输出：<p/>\r\n123456\r\n$ cat myfileonetwothreefourfive\r\n并没有太惊喜，不是吗？ 但是有个有趣的转折，只要使用 tac 命令，你可以从后往前 cat 这个文件。（请注意， Fedora 对这种有争议的幽默不承担任何责任！）<p/>\r\n123456\r\n$ tac myfilefivefourthreetwoone\r\ncat 命令允许你以不同的方式装饰输出，比如，你可以输出行号：<p/>\r\n123456\r\n$ cat -n myfile     1 one     2 two     3 three     4 four     5 five\r\n还有其他选项可以显示特殊字符和其他功能。要了解更多, 请运行 man cat 命令， 看完之后，按 q 即可退出回到 shell。<p/>\n挑选你的食物\n通常，文件太长会无法全部显示在屏幕上，您可能希望能够像文档一样查看它。 这种情况下，可以试试 less 命令：<p/>\r\n$ less myfile\r\n你可以用方向键，也可以用 PgUp/PgDn 来查看文件， 按 q 就可以退回到 shell。<p/>\n实际上，还有一个 more 命令，其基于老式的 UNIX 系统命令。如果在退回 shell 后仍想看到该文件的内容，则可能需要使用它。而 less 命令则让你回到你离开 shell 之前的样子，并且清除屏幕上你看到的所有的文件内容。<p/>\n一点披萨或甜点\n有时，你所需的输出只是文件的开头。 比如，有一个非常长的文件，当你使用 cat 命令时，会显示这个文件所有内容，前几行的内容很容易滚动过去，导致你看不到。head 命令会帮你获取文件的前几行：<p/>\r\n$ head -n 2 myfileonetwo\r\n同样，你会用 tail 命令来查看文件的末尾几行：<p/>\r\n1234\r\n$ tail -n 3 myfilethreefourfive\r\n当然，这些只是在这个领域的几个简单的命令。但它们可以让你在阅读文件时容易入手。<p/>\n <p/>\n 3 收藏\n 1 评论\n", "front_img_path": "full/d76b6f5b61d6d3e3d080011c5e602c185332f67e.jpg"},{"title": "在 Linux 上自定义 bash 命令提示符", "url": "http://blog.jobbole.com/114528/", "create_date": "2018/11/27", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/11/c7935165018518bb315e8a1b4635c92c.jpg"], "fav_nums": 0, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,Linux", "object_id": "f99a5ed0a8bfeccf6fb44e18b5ae0347", "content": "原文出处： SK   译文出处：Linux/Hank Chow   <img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/c7935165018518bb315e8a1b4635c92c.jpg\" /><p/>\n众所周知，bash（the Bourne-Again Shell）是目前绝大多数 Linux 发行版使用的默认 shell。本文将会介绍如何通过添加颜色和样式来自定义 bash 命令提示符的显示。尽管很多插件或工具都可以很轻易地满足这一需求，但我们也可以不使用插件和工具，自己手动自定义一些基本的显示方式，例如添加或者修改某些元素、更改前景色、更改背景色等等。<p/>\n在 Linux 中自定义 bash 命令提示符\n在 bash 中，我们可以通过更改 $PS1 环境变量的值来自定义 bash 命令提示符。<p/>\n一般情况下，bash 命令提示符会是以下这样的形式：<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/7aaedc8fbfc81eb232038d7c0e7f4500.png\" alt=\"\" /><p/>\n在上图这种默认显示形式当中，“sk” 是我的用户名，而 “ubuntuserver” 是我的主机名。<p/>\n只要插入一些以反斜杠开头的特殊转义字符串，就可以按照你的喜好修改命令提示符了。下面我来举几个例子。<p/>\n在开始之前，我强烈建议你预先备份 ~/.bashrc 文件。<p/>\r\n$ cp ~/.bashrc ~/.bashrc.bak\r\n更改 bash 命令提示符中的 username@hostname 部分\n如上所示，bash 命令提示符一般都带有 “username@hostname” 部分，这个部分是可以修改的。<p/>\n只需要编辑 ~/.bashrc 文件：<p/>\r\n$ vi ~/.bashrc\r\n在文件的最后添加一行：<p/>\r\nPS1=\"ostechnix> \"\r\n将上面的 “ostechnix” 替换为任意一个你想使用的单词，然后按 ESC 并输入 :wq 保存、退出文件。<p/>\n执行以下命令使刚才的修改生效：<p/>\r\n$ source ~/.bashrc\r\n你就可以看见 bash 命令提示符中出现刚才添加的 “ostechnix” 了。<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/b45e8c59a7bd67578d84a37dbab3467b.png\" alt=\"\" /><p/>\n再来看看另一个例子，比如将 “username@hostname” 替换为 “Hello@welcome>”。<p/>\n同样是像刚才那样修改 ~/.bashrc 文件。<p/>\r\nexport PS1=\"Hello@welcome> \"\r\n然后执行 source ~/.bashrc 让修改结果立即生效。<p/>\n以下是我在 Ubuntu 18.04 LTS 上修改后的效果。<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/fa8fce5a97ff1d22bae33ad0d08aabaf.png\" alt=\"\" /><p/>\n仅显示用户名\n如果需要仅显示用户名，只需要在 ~/.bashrc 文件中加入以下这一行。<p/>\r\nexport PS1=\"\\u \"\r\n这里的 \\u 就是一个转义字符串。<p/>\n下面提供了一些可以添加到 $PS1 环境变量中的用以改变 bash 命令提示符样式的转义字符串。每次修改之后，都需要执行 source ~/.bashrc 命令才能立即生效。<p/>\n显示用户名和主机名\n<p/>\r\nexport PS1=\"\\u\\h \"\r\n命令提示符会这样显示：<p/>\r\nskubuntuserver\r\n显示用户名和完全限定域名\n<p/>\r\nexport PS1=\"\\u\\H \"\r\n在用户名和主机名之间显示其它字符\n如果你还需要在用户名和主机名之间显示其它字符（例如 @），可以使用以下格式：<p/>\r\nexport PS1=\"\\u@\\h \"\r\n命令提示符会这样显示：<p/>\r\nsk@ubuntuserver\r\n显示用户名、主机名，并在末尾添加 $ 符号\n<p/>\r\nexport PS1=\"\\u@\\h\\\\$ \"\r\n综合以上两种显示方式\n<p/>\r\nexport PS1=\"\\u@\\h> \"\r\n命令提示符最终会这样显示：<p/>\r\nsk@ubuntuserver>\r\n相似地，还可以添加其它特殊字符，例如冒号、分号、星号、下划线、空格等等。<p/>\n显示用户名、主机名、shell 名称\n<p/>\r\nexport PS1=\"\\u@\\h> \"\r\n显示用户名、主机名、shell 名称以及 shell 版本\n<p/>\r\nexport PS1=\"\\u@\\h>\\v \"\r\nbash 命令提示符显示样式：<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/32e4267b3f8289d8a0a2efd906158e8c.png\" alt=\"\" /><p/>\n显示用户名、主机名、当前目录\n<p/>\r\nexport PS1=\"\\u@\\h\\w \"\r\n如果当前目录是 $HOME ，会以一个波浪线（~）显示。<p/>\n在 bash 命令提示符中显示日期\n除了用户名和主机名，如果还想在 bash 命令提示符中显示日期，可以在 ~/.bashrc 文件中添加以下内容：<p/>\r\nexport PS1=\"\\u@\\h>\\d \"\r\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/ca4113dff833b905464166d2e386febe.png\" alt=\"\" /><p/>\n在 bash 命令提示符中显示日期及 12 小时制时间\n<p/>\r\nexport PS1=\"\\u@\\h>\\d\\@ \"\r\n显示日期及 hh:mm:ss 格式时间\n<p/>\r\nexport PS1=\"\\u@\\h>\\d\\T \"\r\n显示日期及 24 小时制时间\n<p/>\r\nexport PS1=\"\\u@\\h>\\d\\A \"\r\n显示日期及 24 小时制 hh:mm:ss 格式时间\n<p/>\r\nexport PS1=\"\\u@\\h>\\d\\t \"\r\n以上是一些常见的可以改变 bash 命令提示符的转义字符串。除此以外的其它转义字符串，可以在 bash 的 man 手册 PROMPTING 章节中查阅。<p/>\n你也可以随时执行以下命令查看当前的命令提示符样式。<p/>\r\n$ echo $PS1\r\n在 bash 命令提示符中去掉 username@hostname 部分\n如果我不想做任何调整，直接把 username@hostname 部分整个去掉可以吗？答案是肯定的。<p/>\n如果你是一个技术方面的博主，你有可能会需要在网站或者博客中上传自己的 Linux 终端截图。或许你的用户名和主机名太拉风、太另类，不想让别人看到，在这种情况下，你就需要隐藏命令提示符中的 “username@hostname” 部分。<p/>\n如果你不想暴露自己的用户名和主机名，只需要按照以下步骤操作。<p/>\n编辑 ~/.bashrc 文件：<p/>\r\n$ vi ~/.bashrc\r\n在文件末尾添加这一行：<p/>\r\nPS1=\"\\W> \"\r\n输入 :wq 保存并关闭文件。<p/>\n执行以下命令让修改立即生效。<p/>\r\n$ source ~/.bashrc\r\n现在看一下你的终端，“username@hostname” 部分已经消失了，只保留了一个 ~> 标记。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/5aa807ba4af0efc13e02cf04081e97a6.png\" alt=\"\" /><p/>\n如果你想要尽可能简单的操作，又不想弄乱你的 ~/.bashrc 文件，最好的办法就是在系统中创建另一个用户（例如 “user@example”、“admin@demo”）。用带有这样的命令提示符的用户去截图或者录屏，就不需要顾虑自己的用户名或主机名被别人看见了。<p/>\n警告：在某些情况下，这种做法并不推荐。例如像 zsh 这种 shell 会继承当前 shell 的设置，这个时候可能会出现一些意想不到的问题。这个技巧只用于隐藏命令提示符中的 “username@hostname” 部分，仅此而已，如果把这个技巧挪作他用，也可能会出现异常。<p/>\n为 bash 命令提示符着色\n目前我们也只是变更了 bash 命令提示符中的内容，下面介绍一下如何对命令提示符进行着色。<p/>\n通过向 ~/.bashrc 文件写入一些配置，可以修改 bash 命令提示符的前景色（也就是文本的颜色）和背景色。<p/>\n例如，下面这一行配置可以令某些文本的颜色变成红色：<p/>\r\nexport PS1=\"\\u@\\[\\e[31m\\]<p/>\\h\\[\\e[m\\]<p/> \"\r\n添加配置后，执行 source ~/.bashrc 立即生效。<p/>\n你的 bash 命令提示符就会变成这样：<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/4c11fb5da85c504dd4db95bd8924b39a.png\" alt=\"\" /><p/>\n类似地，可以用这样的配置来改变背景色：<p/>\r\nexport PS1=\"\\u@\\[\\e[31;46m\\]<p/>\\h\\[\\e[m\\]<p/> \"\r\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/bb7654b0d1486e06edd70071ea2c2f88.png\" alt=\"\" /><p/>\n添加 emoji\n大家都喜欢 emoji。还可以按照以下配置把 emoji 插入到命令提示符中。<p/>\r\nPS1=\"\\W ♤ >\"\r\n需要注意的是，emoji 的显示取决于使用的字体，因此某些终端可能会无法正常显示 emoji，取而代之的是一些乱码或者单色表情符号。<p/>\n自定义 bash 命令提示符有点难，有更简单的方法吗？\n如果你是一个新手，编辑 $PS1 环境变量的过程可能会有些困难，因为命令提示符中的大量转义字符串可能会让你有点晕头转向。但不要担心，有一个在线的 bash $PS1 生成器可以帮助你轻松生成各种 $PS1 环境变量值。<p/>\n就是这个网站：<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/fb1b889429b6c998e604497278a67b18.png\" alt=\"EzPrompt\" /><p/>\n只需要直接选择你想要的 bash 命令提示符样式，添加颜色、设计排序，然后就完成了。你可以预览输出，并将配置代码复制粘贴到 ~/.bashrc 文件中。就这么简单。顺便一提，本文中大部分的示例都是通过这个网站制作的。<p/>\n我把我的 ~/.bashrc 文件弄乱了，该如何恢复？\n正如我在上面提到的，强烈建议在更改 ~/.bashrc 文件前做好备份（在更改其它重要的配置文件之前也一定要记得备份）。这样一旦出现任何问题，你都可以很方便地恢复到更改之前的配置状态。当然，如果你忘记了备份，还可以按照下面这篇文章中介绍的方法恢复为默认配置。<p/>\n如何将 ~/.bashrc 文件恢复到默认配置\n这篇文章是基于 ubuntu 的，但也适用于其它的 Linux 发行版。不过事先声明，这篇文章的方法会将 ~/.bashrc 文件恢复到系统最初时的状态，你对这个文件做过的任何修改都将丢失。<p/>\n感谢阅读！<p/>\n <p/>\n", "front_img_path": "full/db73d3d8124c52d1ef11c39b6c5e548e088ac054.jpg"},{"title": "一文带你了解 Vim 的起源", "url": "http://blog.jobbole.com/114461/", "create_date": "2018/10/24", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2012/04/vim-logo.png"], "fav_nums": 5, "comment_nums": 0, "vote_nums": 2, "tags": "IT技术,Vim", "object_id": "503ae7fbe22cb704c56f29af0f417fa6", "content": "本文由 伯乐在线 - Abandon_first 翻译，艾凌风 校稿。未经许可，禁止转载！英文出处：TwoBitHistory。欢迎加入翻译组。我最近偶然发现了一种名为 Intel HEX 的文件格式。据我所知，Intel HEX 文件（使用.hex 扩展名）通过将二进制图像编码成十六进制数字行，使二进制图像不那么晦涩难懂。显然，当人们需要对微控制器进行编程或者将数据烧录进 ROM 时会用到这种文件。无论如何，当我第一次在 Vim 中打开一个 HEX 文件时，我发现了一些震惊的东西。至少对我来说，这种文件格式是非常深奥难懂的，但 Vim 已经掌握了它。HEX 文件的每一行都是一条被划分为不同字段的记录—— Vim 已经预先将每个字段显示成不同的颜色。set ft 吗? 我充满敬畏地发问。filetype=hex，Vim 得意地回答。<p/>\nVim 无所不在且受众极其广泛，以至于其支持 HEX 文件也应该在预料之中。Mac OS 中预装了 Vim，同时，Linux 世界中也有很多 Vim 的支持者。即使那些讨厌 Vim 的人也对它很熟悉，因为太多的流行命令行工具默认使用 Vim，不熟悉 Vim 的用户往往身陷其中，这已经变成了一个 meme。包括 Facebook 在内的一些大型网站，当你按下 j 键时，会向下滚动，而当你按下 k 键时，会向上滚动——这意味着 Vim 通过数字文化传播达到了难以想象的高水准。<p/>\n然而，Vim 也是谜一般的存在。例如，与人尽皆知的由 Facebook 开发和维护的 React 不同，Vim没有明显的发起人。尽管它如此常见和重要，但是似乎没有任何委员会或组织为 Vim 做出决策。你可以花几分钟去浏览 Vim 网站，但却无法得知是谁创建了 Vim 或者为什么创建。如果只启动 Vim 不打开任何文件，你会看到 Vim 的启动消息，表明 Vim 是由”Bram Moolenaar 等人“开发的。但这并不能说明什么，Bram Moolenaar 到底是谁，他的神秘同伙又是谁？<p/>\n当我们求索上述问题的时候，也许更重要的是，为什么退出 Vim 需要输入：wq？当然，这是一个“写”操作，然后是一个“退出”操作，但这不是一个特别容易直观理解的约定。谁决定了复制文本应该被称为“ yanking ”？为什么：%s/foo/bar/gc是“查找和替换”的缩写？Vim 的特性如此武断，不可能是被编造出来的，那么它们又从何而来呢？<p/>\n就像众多情况一样，答案是从那个古老的计算机熔炉——贝尔实验室开始。从某种意义上说，Vim 只是一款被称为“ wq 文本编辑器”软件的最新版本。自 Unix 时代诞生以来，这个软件一直在不断地被开发和改进。<p/>\nKen Thompson 创建了行编辑器\n1966 年，贝尔实验室聘用了 Ken Thompson 。Thompson 刚刚在加州大学伯克利分校完成了电气工程和计算机科学的硕士学位。在伯克利他使用一个名为 QED 的文本编辑器，该编辑器在 1965 到 1966 年间被开发用于伯克利分时系统。1 Thompson 到达贝尔实验室后做的第一件事就是为麻省理工学院兼容分时系统重写 QED。他后来又为 Multics 项目写了另一个版本的QED。在重写过程中，他对程序进行了扩展，以便用户可以在文件中搜索某一行，并使用正则表达式进行替换。2<p/>\n与伯克利的分时系统一样，由麻省理工学院、通用电气和贝尔实验室合作的 Multics 项目试图创建一个可行的商业分时操作系统。最终，AT&T 认为这个项目毫无进展并退出。在没有分时系统的情况下，Thompson 和贝尔实验室资深研究员 Dennis Ritchie，开始怀念分时系统所提供的“交互式计算的感觉”，并着手创建他们自己的版本，该版本最终发展成为 Unix。3 1969 年 8 月，在妻子和幼子外出去加州度假时，Thompson “给操作系统、shell、编辑器和汇编程序分别分配了一个星期”，将新系统的基本组件组合在一起。4<p/>\n这个编辑器被称为 ed 。它是基于 QED 的，但并不完全是 QED 的复现。 Thompson 决定放弃某些 QED 的功能，弱化了对常规的表达式的支持，因此 ed 只能理解相对简单的正则表达式。QED 允许用户打开多个缓冲区同时编辑多个文件，但是 ed 一次只使用一个缓冲区。QED 可以执行包含命令的缓冲区，而 ed 则不能。这些简化可能是必要的。Dennis Ritchie 曾说过，去掉 QED 的高级正则表达式是“并不大的损失”。5<p/>\ned 现在是 POSIX 规范的一部分，所以如果你有一个符合 POSIX 的系统，你的电脑上就安装了 ed 。现在，许多 ed 命令都是 Vim 的一部分，因此，这就值得摆弄一番了。例如，你必须使用 w 命令来写入磁盘缓冲区，必须使用 q 命令来退出编辑器。这两个命令可以写在同一行命令中，也就是 wq。ed 与 Vim 一样，是一个模态编辑器；若要从命令模式进入输入模式，取决于你试图如何转换文本，需使用 insert 命令（i）、append 命令（a）或 change 命令（c）。ed 还引入了s/foo/bar/g语法来查找和替换或“替换”文本。<p/>\n考虑到所有这些相似之处，你可能会认为大部分 Vim 用户可以流畅地使用 ed。但 ed 在另一个重要方面，和 Vim 一点也不相似。ed 是一个真正的行编辑。它被广泛应用于电传打字机时代。当 Ken Thompson 和 Dennis Ritchie 在 Unix 上调试程序时看起来是这样的：<p/>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8f/Ken_Thompson_%28sitting%29_and_Dennis_Ritchie_at_PDP-11_%282876612463%29.jpg\" alt=\"Ken Thompson interacting with a PDP-11 via teletype.\" /><p/>\ned 不允许你编辑开放缓冲区中那些被其他行围绕的行，也不允许移动光标，因为 ed 在每次修改的时候都必须重新打印整个文件。在1969年， ed 没有任何机制来“清除”屏幕上的内容，因为”屏幕“就是一张纸，所有已经输出的东西都像是已经用墨水打印出来了。在必要的时候，你可以使用列表命令（l）要求 ed 打印出一系列的行，但是大多数时候，你都是在你看不到的文本上操作。因此，使用 ed 就像是尝试用一个低电量的手电筒在黑暗房间中摸索。每次你只能看到那么一点儿，所以必须尽最大努力去记住每件东西的位置。<p/>\n下面有一个 ed 会话的例子。我添加了注释（在字符 #之后）来解释了每一行，不过如果这些注释真的被输入，ed 并不会把它们当作注释并且会报错：<p/>\r\n1234567891011121314151617181920212223\r\n[sinclairtarget 09:49 ~]$ edi                           # Enter input modeHello world! Isn't it a nice day?.                           # Finish input1,2l                        # List lines 1 to 2Hello world!$$2d                          # Delete line 2,l                          # List entire bufferHello world!$Isn't it a nice day?$s/nice/terrible/g           # Substitute globally,lHello world!$Isn't it a terrible day?$w foo.txt                   # Write to foo.txt38                          # (bytes written)q                           # Quit[sinclairtarget 10:50 ~]$ cat foo.txtHello world!Isn't it a terrible day?\r\n正如你所看到的，ed 并不是一个特别友好的程序。<p/>\nBill Joy 创建了文本编辑器\n对 Thompson 和 Ritchie 来说， ed 已经足够好了。但是其他人则认为它很难用，而且它作为一个淋漓尽致地表现 Unix 对新手敌意的例子而臭名昭著。6在 1975 年，一个名叫 George Coulouris 的人在伦敦玛丽皇后学院的 Unix 系统上开发了一个改进版 ed 。Coulouris 利用他在玛丽女王学院的视频显示器开发他的编辑器。与 ed 不同的是，Coulouris 的程序允许用户编辑在屏幕中的一行代码，通过一次次击键的方式来操作行（想象一下在 Vim 中每次编辑一行）。 Thompson 拜访玛丽女王学院时，看到 Coulouris 已经写好的程序，驳斥道他不需要在编辑文件的时候看到它的状态。受此启发，Coulouris 将他的程序命名为 em，或者“为凡人而生的编辑器”。7<p/>\n<img class=\"aligncenter\" src=\"https://upload.wikimedia.org/wikipedia/en/b/b6/Portrait_of_George_Coulouris_%28computer_scientist%29.jpg\" alt=\"Portrait of George Coulouris (computer scientist).jpg\" width=\"234\" height=\"312\" /><p/>\n（George Coulouris）<p/>\n1976年，Coulouris 把 em 引入了加州大学伯克利分校，在那里他用了一个夏天的时间在 CS 系访学。这是 Ken Thompson 离开伯克利去贝尔实验室工作十年之后的事了。在伯克利，Coulouris 遇到了 Bill Joy，一名伯克利软件发行公司（BSD）的研究生。Coulouris 斯向Joy 展示了 em， Joy 以 Coulouris 的源代码为基础，为扩展 ed 建立了一个名为 ex 的改进版 ed。1978年，1.1 版本的 ex 与第 1 个版本的 BSD Unix 捆绑在一起。ex 在很大程度上与 ed 兼容，但它增加了两种模式：一种“开放”模式，这种模式可以使 em 单行编辑成为可能，还有一种“可见”模式，这种模式会占据整个屏幕，并且可以像我们今天所习惯的那样，对整个文件进行实时编辑。<p/>\n<img class=\"aligncenter\" src=\"https://upload.wikimedia.org/wikipedia/commons/e/e2/Bill_joy.jpg\" alt=\"Bill joy.jpg\" /><p/>\n（Bill Joy）<p/>\n1979 年的第 2 版 BSD 引入了一个名为 vi 的可执行文件，它只在可视模式下打开 ex 。8<p/>\nex/vi （后来称为 vi）建立了我们现在使用的 Vim 中大多数的约定，但这些约定当时并不是 ed 的一部分。Bill Joy 使用的视频终端是 Lear Siegler ADM-3A，它的键盘没有光标键。而是，h、j、k 和 l 键上绘制光标键，所以 Bill Joy 在vi 中就使用这些键来进行光标移动。ADM-3A 键盘上 escape 键位置是今天我们所使用的键盘上的 tab 键，这也就解释了为什么这样一个难以够着的键会被用来实现像退出当前模式这么常见的操作。前缀命令的 : 字符同样也来自 i，它在常规模式下（即运行 ex 进入的模式）使用 : 作为提示。这解决了一个 ed 中被长期诟病的问题，也就是一旦启动之后，没有任何反馈信息向用户致以问候。在可见模式下，保存和退出需要使用现在仍在使用的经典 wq。“Yanking”和“puttng”、标记、以及用于设置选项的 set 命令都是原始 vi 的一部分。我们今天在 Vim 中使用的的基本文本编辑过程，都是 vi 中使用的特性。<p/>\n<img src=\"https://vintagecomputer.ca/wp-content/uploads/2015/01/LSI-ADM3A-full-keyboard.jpg\" alt=\"A Lear Siegler ADM-3A keyboard.\" /><p/>\nvi 是除 ed 之外唯一与 BSD Unix 捆绑的文本编辑器。在那个时候，Emacs 可能会花费数百美元（这是在 GNU Emacs 之前），所以 vi 变得非常流行。但是 vi 是 ed 的直接衍生版本，这意味着如果没有 AT&T 的源代码，源代码就不能被修改。这促使一些人创建了 vi 的开源版本。 STEVIE （专门为 VI 爱好者的 ST 编辑器）出现于1987年，Elvis 出现于 1990 年，nvi 出现于 1994 年。其中一些克隆版本添加了额外的功能，如语法高亮和窗口分离。尤其是 Elvis ，它的许多功能被整合到 Vim 中，因为许多 Elvis 用户推动了这些功能的加入。9)<p/>\nBram Moolenaar 创建了 Vim\n“Vim”现在是“改进版 Vi”的缩写，而最初代表的是“模拟版 Vi”。和其他许多“vi克隆版本”一样，Vim 始于在一个无法使用 vi 的平台上复现 vi 的一个尝试。在荷兰 Venlo 一家影印公司工作的软件工程师 Bram Moolenaar 想要为他全新的 Amiga 2000 准备一款类似于 vi 的编辑器。Moolenaar 已经习惯了在大学时使用的 Unix 系统上的 vi ，当时他 已经对vi了如指掌。10 所以在 1988 年，Moolenaar 使用当时的 STEVIE vi克隆版本开始在 Vim 上工作。<p/>\n<img class=\"aligncenter\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Bram_Moolenaar_in_2007.jpg/800px-Bram_Moolenaar_in_2007.jpg\" width=\"340\" height=\"450\" /><p/>\n（Bram Moolenaar，2006 年加入 Google）<p/>\nMoolenaar 接触到 STEVIE 缘于其曾经出现在一个叫 Fred Fish 的磁盘上。Fred Fish 是一名美国程序员，每个月都会寄出一张软盘，内含为 Amiga 平台提供的精选可用开源软件。任何人只要支付邮费就可以得到一张这样的磁盘。有若干版本的 STEVIE 曾在 Fred Fish 磁盘上发布。Moolenaar 使用的 STEVIE 版本在 Fred Fish 256 号磁盘上发布。11（令人失望的是，Fred Fish 磁盘似乎与 Freddi Fish 没有任何关系。）<p/>\nMoolenaar 喜欢 STEVIE，但很快就注意到其缺失了很多 vi 命令。12 因此，在第一次发布 Vim 时，Moolenaar 优先考虑了 vi 的兼容性。当时已经有其他人编写了一系列的 vi 宏，当运行一个合适的 vi 兼容编辑器时，可以求解一个随机生成的迷宫。Moolenaar 能够让这些宏在 Vim 中运行。1991年，Vim 以 Vi模拟为名第一次发布于 Fred Fish 591 号磁盘。13 Moolenaar 添加了一些特性（包括多级撤销和解决编译器错误的“quickfix”模式），这意味着 Vim 已经完成了对 Vi 的超越。在 1993 年通过 FTP 发布 Vim 2.0 之前，Vim 都仍以 Vi模拟 的身份存在。<p/>\n在众多互联网合作者的帮助下，Moolenaar 稳健地在 Vim 中加入了一些功能。Vim 2.0 引入了对wrap选项的支持，以及对长行文本进行水平滚动的支持。受到了vi克隆nvi的启发，Vim 3.0 增加了对分割窗口和缓冲区的支持。Vim 现在还将每个缓冲区保存到交换文件中以避免程序崩溃造成文件丢失。Vimscript 支持语法高亮显示，第一次出现是在 Vim 5.0 中。与此同时，Vim 的受欢迎程度也在不断增长。它被移植到 MS-DOS、 Windows、Mac，甚至被移植到 Unix 与原来的 vi竞争。<p/>\n2006 年，Vim 被 Linux Journal 读者评为最受欢迎的编辑器。14 如今，根据 2018 年 Stack Overflow 的开发者调查，Vim 是最受欢迎的文本模式（即终端模拟器）编辑器，受用于 25.8% 的软件开发人员(和 40% 的 Sysadmin / DevOps 人员)。15 在 1980 年代末和整个 1990 年代，程序员一度发起了“编辑器战争”，将 Emacs 用户与 vi （即最终的 Vim ）用户进行了对比。虽然 Emacs 肯定仍有一些追随者，但有些人认为编辑器战争已经以 Vim 获胜而结束。16 2018年 Stack Overflow 的开发者调查显示只有 4.1% 的受访者使用 Emacs，也验证了这个事实。<p/>\nVim 是如何变得如此成功的？显然，人们喜欢 Vim 所提供的特性。但我认为，Vim 背后的悠久历史表明了它的优势远不仅仅体现在其功能集上。Vim 的代码库可以追溯到 1988 年，当时 Moolenaar 开始研究它。另一方面，“ wq 文本编辑器”——关于 Unix-y 文本编辑器应该如何工作的更广泛的愿景——可以追溯到半个世纪以前。“ wq 文本编辑器”有一些不同的具体表达方式，但在某种程度上要感谢 Bill Joy 和 Bram Moolenaar 对向后兼容性非比寻常的关注，才使好的想法逐渐积累起来。从这个意义上说，“ wq 文本编辑器”是运行时间最长、最成功的开源项目之一，得益于计算机世界中一些最伟大的思想贡献。我不认为“创业公司无视所有先例来创造颠覆性的新软件”的开发方式都是不妥的，但 Vim 提醒我们，这种协作和增量的方式同样能产生奇迹。<p/>\n <p/>\n@TwoBitHistory 每两周更新一次类似文章，如果你喜欢本文请在 Twitter 上关注或者订阅 RSS，以便及时知晓更新发布。伯乐在线已获授权同步翻译中文版，敬请关注<p/>\nButler Lampson, “Systems,” Butler Lampson, accessed August 5, 2018, http://bwlampson.site/Systems.htm.\nDennis Ritchie, “An Incomplete History of the QED Editor,” accessed August 5, 2018, https://www.bell-labs.com/usr/dmr/www/qed.html.\nPeter Salus, “The Daemon, the GNU, and the Penguin,” Groklaw, April 14, 2005, accessed August 5, 2018, http://www.groklaw.net/article.php?story=20050414215646742.\nibid.\nDennis Ritchie, “An Incomplete History of the QED Editor,” accessed August 5, 2018, https://www.bell-labs.com/usr/dmr/www/qed.html.\nDonald Norman, “The Truth about Unix: The User Interface Is Horrid,” Datamation, accessed August 5, 2018, http://www.ceri.memphis.edu/people/smalley/ESCI7205_misc_files/The_truth_about_Unix_cleaned.pdf.\nGeorge Coulouris, “George Coulouris: A Bit of History,” George Coulouris’ Homepage, September 1998, accessed August 5, 2018, http://www.eecs.qmul.ac.uk/~gc/history/index.html.\n“Second Berkeley Software Distribution Manual,” Roguelife, accessed August 5, 2018, http://roguelife.org/~fujita/COOKIES/HISTORY/2BSD/vi.u.html.\nSven Guckes, “VIM Wishlist,” Vmunix, May 15, 1995, accessed August 5, 2018, https://web.archive.org/web/20080520075925/http://www.vmunix.com/vim/wish.html.\nBram Moolenaar, “Vim 25” (lecture, Zurich, November 2, 2016), December 13, 2016, accessed August 5, 2018, https://www.youtube.com/watch?v=ayc_qpB-93o&t=4m58s\nibid. (?t=6m15s)\nibid. (?t=7m6s)\n“Fish Disks 1  1120,” Amiga Stuff, accessed August 5, 2018, http://www.amiga-stuff.com/pd/fish.html.\n“2005 Linux Journal Reader’s Choice Awards,” Linux Journal, September 28, 2005, accessed August 5, 2018, https://www.linuxjournal.com/article/8520#N0x850cd80.0x87983bc.\n“Stack Overflow Developer Survey 2018,” Stack Overflow, accessed August 5, 2018, https://insights.stackoverflow.com/survey/2018/#development-environments-and-tools.\nBruce Byfield, “The End of the Editor Wars,” Linux Magazine, May 11, 2015, accessed August 5, 2018, http://www.linux-magazine.com/Online/Blogs/Off-the-Beat-Bruce-Byfield-s-Blog/The-End-of-the-Editor-Wars.\n 5 收藏\n", "front_img_path": "full/4c6abd763d27eeeb4c7e7665f213388ec74df623.jpg"},{"title": "MySQL 更改数据库数据存储目录", "url": "http://blog.jobbole.com/114479/", "create_date": "2018/11/05", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2015/11/e78e36715813f49e9e62fe0c6050075c.png"], "fav_nums": 2, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,MySQL,数据库", "object_id": "b01599f760a20847250346416be84645", "content": " <p/>\n5：修改配置文件my.cnf<p/>\n并不是所有版本都包含有my.cnf这个配置文件，在MySQL 5.5版本，我就找不到my.cnf这个配置文件， 而有些MySQL版本该文件位于/usr/my.cnf，如果/etc/目录下没有my.cnf配置文件，请到/usr/share/mysql/下找到*.cnf文件，拷贝其中一个到/etc/并改名为my.cnf中。命令如下：<p/>\n<img title=\"clip_image003\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/13bbfad29ea85c8b40006710b7275b63.png\" alt=\"clip_image003\" width=\"650\" height=\"130\" /><p/>\r\n[root@DB-Server mysql]# cp /usr/share/mysql/my-medium.cnf /etc/my.cnf\r\n编辑/etc/my.cnf文件，修改参数socket<p/>\n                         MySQL 5.5 版本<p/>\n<img title=\"clip_image004\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/08b0c8cd51a5b4577634c2ad22b69f76.png\" alt=\"clip_image004\" width=\"650\" height=\"333\" /><p/>\r\n123456789101112131415161718192021\r\n# The following options will be passed to all MySQL clients[client]#password       = your_passwordport            = 3306socket          = /u01/mysqldata/mysql/mysql.sock # Here follows entries for some specific programs # The MySQL server[mysqld]port            = 3306socket          = /u01/mysqldata/mysql/mysql.sockskip-external-lockingkey_buffer_size = 16Mmax_allowed_packet = 1Mtable_open_cache = 64sort_buffer_size = 512Knet_buffer_length = 8Kread_buffer_size = 256Kread_rnd_buffer_size = 512Kmyisam_sort_buffer_size = 8M\r\n<img title=\"clip_image005\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/bbead4cc2ef2124a8c2124f6497425d4.png\" alt=\"clip_image005\" width=\"650\" height=\"308\" /><p/>\n <p/>\n6：修改启动脚本/etc/init.d/mysql<p/>\n将参数datadir修改为datadir=/u01/mysqldata/mysql/<p/>\n<img title=\"clip_image006\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/3f02af33620a5568586c971e1c3e6981.png\" alt=\"clip_image006\" width=\"650\" height=\"321\" /><p/>\n <p/>\n7：启动MySQL服务并验证MySQL数据库路径<p/>\r\n12345\r\n[root@DB-Server ~]# service mysql startStarting MySQL..[  OK  ][root@DB-Server ~]# mysqladmin -u root -p variables | grep datadirEnter password: | datadir        | /u01/mysqldata/mysql/\r\n我的疑问：<p/>\n1： 在修改数据库的存储目录前，/var/lib/mysql/目录下根本没有mysql.sock文件，安装上面配置后，就会生成mysql.sock文件。<p/>\n关于mysql.sock文件，搜索了一下资料：mysql.sock是用于socket连接的文件。也就是只有你的守护进程启动起来这个文件才存在。但是你的mysql程序（这个程序是客户端，服务器端是mysqld）可以选择是否使用mysql.sock文件来连接（因为这个方法只适合在Unix主机上面连接本地的mysqld），对于非本地的任何类型的主机。那么这个文件是否一定需要的呢？ 这个需要进一步了解清楚。<p/>\n2：我在网上看有些网友总结的修改MySQL数据路径，有些需要给新建的目录的权限做一些处理，而有些有不用对目录权限进行授权，我没有处理，也没有什么问题。到底要不要对新的数据库目录授权呢？<p/>\n3：我在MySQL_5.6.20这个版本测试时，不修改my.cnf，只修改启动脚本/etc/init.d/mysql，也完全没有啥问题。也没有myssql.sock文件生成。<p/>\n4: 注意如果没有禁用selinux, 修改MySQL的数据路径后启动MySQL服务会遇到一些错误。关于这个的解释是后台服务都需要有对相应目录的对应权限，而 mysql 的默认路径/var/lib/mysql 已经添加了相应的策略，修改路径后由于没有相应的策略，导致后台进程读取文件被selinux阻止，从而出现权限错误。 所以要么关闭Selinux或修改文件安全上下文。<p/>\r\n12345678910111213\r\n[root@DB-Server mysql]# /etc/init.d/mysql start Starting MySQL....The server quit without updating PID file (/u01/mysqldata/mysql//DB-Server.localdomain.pid).[FAILED] [root@DB-Server mysql]# [root@DB-Server mysql]# chcon -R -t mysqld_db_t /u01/mysqldata/mysql/ [root@DB-Server mysql]# /etc/init.d/mysql start Starting MySQL.[ OK ] [root@DB-Server mysql]#\r\n参考资料：<p/>\nhttp://database.ctocio.com.cn/tips/449/7566949.shtml<p/>\nwww.linuxidc.com/Linux/2012-12/75647.htm<p/>\nhttp://blog.csdn.net/hellyhe/article/details/8309470<p/>\n 2 收藏\n", "front_img_path": "full/35011d6168be00e949624c665041dc724e3ad786.jpg"},{"title": "Linux 系统上交换空间的介绍", "url": "http://blog.jobbole.com/114447/", "create_date": "2018/10/16", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/10/826020081afe2f59d84dafce6da65189.jpg"], "fav_nums": 1, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,Linux", "object_id": "232ea959696606719883982b948de247", "content": "原文出处： David Both   译文出处：Linux中国/heguangzhi   学习如何修改你的系统上的交换空间的容量，以及你到底需要多大的交换空间。<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/10/826020081afe2f59d84dafce6da65189.jpg\" /><p/>\n当今无论什么操作系统交换Swap空间是非常常见的。Linux 使用交换空间来增加主机可用的虚拟内存。它可以在常规文件或逻辑卷上使用一个或多个专用交换分区或交换文件。<p/>\n典型计算机中有两种基本类型的内存。第一种类型，随机存取存储器 (RAM)，用于存储计算机使用的数据和程序。只有程序和数据存储在 RAM 中，计算机才能使用它们。随机存储器是易失性存储器；也就是说，如果计算机关闭了，存储在 RAM 中的数据就会丢失。<p/>\n硬盘是用于长期存储数据和程序的磁性介质。该磁介质可以很好的保存数据；即使计算机断电，存储在磁盘上的数据也会保留下来。CPU（中央处理器）不能直接访问硬盘上的程序和数据；它们必须首先复制到 RAM 中，RAM 是 CPU 访问代码指令和操作数据的地方。在引导过程中，计算机将特定的操作系统程序（如内核、init 或 systemd）以及硬盘上的数据复制到 RAM 中，在 RAM 中，计算机的处理器 CPU 可以直接访问这些数据。<p/>\n交换空间是现代 Linux 系统中的第二种内存类型。交换空间的主要功能是当全部的 RAM 被占用并且需要更多内存时，用磁盘空间代替 RAM 内存。<p/>\n例如，假设你有一个 8GB RAM 的计算机。如果你启动的程序没有填满 RAM，一切都好，不需要交换。假设你在处理电子表格，当添加更多的行时，你电子表格会增长，加上所有正在运行的程序，将会占用全部的 RAM 。如果这时没有可用的交换空间，你将不得不停止处理电子表格，直到关闭一些其他程序来释放一些 RAM 。<p/>\n内核使用一个内存管理程序来检测最近没有使用的内存块（内存页）。内存管理程序将这些相对不经常使用的内存页交换到硬盘上专门指定用于“分页”或交换的特殊分区。这会释放 RAM，为输入电子表格更多数据腾出了空间。那些换出到硬盘的内存页面被内核的内存管理代码跟踪，如果需要，可以被分页回 RAM。<p/>\nLinux 计算机中的内存总量是 RAM + 交换分区，交换分区被称为虚拟内存.<p/>\nLinux 交换分区类型\nLinux 提供了两种类型的交换空间。默认情况下，大多数 Linux 在安装时都会创建一个交换分区，但是也可以使用一个特殊配置的文件作为交换文件。交换分区顾名思义就是一个标准磁盘分区，由 mkswap 命令指定交换空间。<p/>\n如果没有可用磁盘空间来创建新的交换分区，或者卷组中没有空间为交换空间创建逻辑卷，则可以使用交换文件。这只是一个创建好并预分配指定大小的常规文件。然后运行 mkswap 命令将其配置为交换空间。除非绝对必要，否则我不建议使用文件来做交换空间。（LCTT 译注：Ubuntu 近来的版本采用了交换文件而非交换空间，所以我对于这种说法保留看法）<p/>\n当总虚拟内存（RAM 和交换空间）变得快满时，可能会发生频繁交换。系统花了太多时间在交换空间和 RAM 之间做内存块的页面切换，以至于几乎没有时间用于实际工作。这种情况的典型症状是：系统变得缓慢或完全无反应，硬盘指示灯几乎持续亮起。<p/>\n使用 free 的命令来显示 CPU 负载和内存使用情况，你会发现 CPU 负载非常高，可能达到系统中 CPU 内核数量的 30 到 40 倍。另一个情况是 RAM 和交换空间几乎完全被分配了。<p/>\n事实上，查看 SAR（系统活动报告）数据也可以显示这些内容。在我的每个系统上都安装 SAR ，并将这些用于数据分析。<p/>\n交换空间的正确大小是多少？\n许多年前，硬盘上分配给交换空间大小是计算机上的 RAM 的两倍（当然，这是大多数计算机的 RAM 以 KB 或 MB 为单位的时候）。因此，如果一台计算机有 64KB 的 RAM，应该分配 128KB 的交换分区。该规则考虑到了这样的事实情况，即 RAM 大小在当时非常小，分配超过 2 倍的 RAM 用于交换空间并不能提高性能。使用超过两倍的 RAM 进行交换，比实际执行有用的工作的时候，大多数系统将花费更多的时间。<p/>\nRAM 现在已经很便宜了，如今大多数计算机的 RAM 都达到了几十亿字节。我的大多数新电脑至少有 8GB 内存，一台有 32GB 内存，我的主工作站有 64GB 内存。我的旧电脑有 4 到 8GB 的内存。<p/>\n当操作具有大量 RAM 的计算机时，交换空间的限制性能系数远低于 2 倍。Fedora 28 在线安装指南 定义了当前关于交换空间分配的方法。下面内容是我提出的建议。<p/>\n下表根据系统中的 RAM 大小以及是否有足够的内存让系统休眠，提供了交换分区的推荐大小。建议的交换分区大小是在安装过程中自动建立的。但是，为了满足系统休眠，您需要在自定义分区阶段编辑交换空间。<p/>\n表 1: Fedora 28 文档中推荐的系统交换空间<p/>\n在上面列出的每个范围之间的边界(例如，具有 2GB、8GB 或 64GB 的系统 RAM)，请根据所选交换空间和支持休眠功能请谨慎使用。如果你的系统资源允许，增加交换空间可能会带来更好的性能。<p/>\n当然，大多数 Linux 管理员对多大的交换空间量有自己的想法。下面的表2 包含了基于我在多种环境中的个人经历所做出的建议。这些可能不适合你，但是和表 1 一样，它们可能对你有所帮助。<p/>\n表 2: 作者推荐的系统交换空间<p/>\n这两个表中共同点，随着 RAM 数量的增加，超过某一点增加更多交换空间只会导致在交换空间几乎被全部使用之前就发生频繁交换。根据以上建议，则应尽可能添加更多 RAM，而不是增加更多交换空间。如类似影响系统性能的情况一样，请使用最适合你的建议。根据 Linux 环境中的条件进行测试和更改是需要时间和精力的。<p/>\n向非 LVM 磁盘环境添加更多交换空间\n面对已安装 Linux 的主机并对交换空间的需求不断变化，有时有必要修改系统定义的交换空间的大小。此过程可用于需要增加交换空间大小的任何情况。它假设有足够的可用磁盘空间。此过程还假设磁盘分区为 “原始的” EXT4 和交换分区，而不是使用逻辑卷管理（LVM）。<p/>\n基本步骤很简单:<p/>\n关闭现有的交换空间。\n创建所需大小的新交换分区。\n重读分区表。\n将分区配置为交换空间。\n添加新分区到 /etc/fstab。\n打开交换空间。\n应该不需要重新启动机器。<p/>\n为了安全起见，在关闭交换空间前，至少你应该确保没有应用程序在运行，也没有交换空间在使用。free 或 top 命令可以告诉你交换空间是否在使用中。为了更安全，您可以恢复到运行级别 1 或单用户模式。<p/>\n使用关闭所有交换空间的命令关闭交换分区：<p/>\r\nswapoff -a\r\n现在查看硬盘上的现有分区。<p/>\r\nfdisk -l\r\n这将显示每个驱动器上的分区表。按编号标识当前的交换分区。<p/>\n使用以下命令在交互模式下启动 fdisk：<p/>\r\nfdisk /dev/<device name>\r\n例如：<p/>\r\nfdisk /dev/sda\r\n此时，fdisk 是交互方式的，只在指定的磁盘驱动器上进行操作。<p/>\n使用 fdisk 的 p 子命令验证磁盘上是否有足够的可用空间来创建新的交换分区。硬盘上的空间以 512 字节的块以及起始和结束柱面编号的形式显示，因此您可能需要做一些计算来确定分配分区之间和末尾的可用空间。<p/>\n使用 n 子命令创建新的交换分区。fdisk 会问你开始柱面。默认情况下，它选择编号最低的可用柱面。如果你想改变这一点，输入开始柱面的编号。<p/>\nfdisk 命令允许你以多种格式输入分区的大小，包括最后一个柱面号或字节、KB 或 MB 的大小。例如，键入 4000M ，这将在新分区上提供大约 4GB 的空间，然后按回车键。<p/>\n使用 p 子命令来验证分区是否按照指定的方式创建的。请注意，除非使用结束柱面编号，否则分区可能与你指定的不完全相同。fdisk 命令只能在整个柱面上增量的分配磁盘空间，因此你的分区可能比你指定的稍小或稍大。如果分区不是您想要的，你可以删除它并重新创建它。<p/>\n现在指定新分区是交换分区了 。子命令 t 允许你指定定分区的类型。所以输入 t，指定分区号，当它要求十六进制分区类型时，输入 82，这是 Linux 交换分区类型，然后按回车键。<p/>\n当你对创建的分区感到满意时，使用 w 子命令将新的分区表写入磁盘。fdisk 程序将退出，并在完成修改后的分区表的编写后返回命令提示符。当 fdisk 完成写入新分区表时，会收到以下消息:<p/>\r\n123456\r\nThe partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table.The new table will be used at the next reboot.Syncing disks.\r\n此时，你使用 partprobe 命令强制内核重新读取分区表，这样就不需要执行重新启动机器。<p/>\r\npartprobe\r\n使用命令 fdisk -l 列出分区，新交换分区应该在列出的分区中。确保新的分区类型是 “Linux swap”。<p/>\n修改 /etc/fstab 文件以指向新的交换分区。如下所示：<p/>\r\nLABEL=SWAP-sdaX   swap        swap    defaults        0 0\r\n其中 X 是分区号。根据新交换分区的位置，添加以下内容：<p/>\r\n/dev/sdaY         swap        swap    defaults        0 0\r\n请确保使用正确的分区号。现在，可以执行创建交换分区的最后一步。使用 mkswap 命令将分区定义为交换分区。<p/>\r\nmkswap /dev/sdaY\r\n最后一步是使用以下命令启用交换空间：<p/>\r\nswapon -a\r\n你的新交换分区现在与以前存在的交换分区一起在线。您可以使用 free 或top 命令来验证这一点。<p/>\n在 LVM 磁盘环境中添加交换空间\n如果你的磁盘使用 LVM ，更改交换空间将相当容易。同样，假设当前交换卷所在的卷组中有可用空间。默认情况下，LVM 环境中的 Fedora Linux 在安装过程将交换分区创建为逻辑卷。您可以非常简单地增加交换卷的大小。<p/>\n以下是在 LVM 环境中增加交换空间大小的步骤:<p/>\n关闭所有交换空间。\n增加指定用于交换空间的逻辑卷的大小。\n为交换空间调整大小的卷配置。\n启用交换空间。\n首先，让我们使用 lvs 命令（列出逻辑卷）来验证交换空间是否存在以及交换空间是否是逻辑卷。<p/>\r\n12345678910\r\n[root@studentvm1 ~]# lvs  LV     VG                Attr       LSize  Pool   Origin Data%  Meta%  Move Log Cpy%Sync Convert  home   fedora_studentvm1 -wi-ao----  2.00g                                                        pool00 fedora_studentvm1 twi-aotz--  2.00g               8.17   2.93                              root   fedora_studentvm1 Vwi-aotz--  2.00g pool00        8.17                                     swap   fedora_studentvm1 -wi-ao----  8.00g                                                        tmp    fedora_studentvm1 -wi-ao----  5.00g                                                        usr    fedora_studentvm1 -wi-ao---- 15.00g                                                        var    fedora_studentvm1 -wi-ao---- 10.00g                                                      [root@studentvm1 ~]#\r\n你可以看到当前的交换空间大小为 8GB。在这种情况下，我们希望将 2GB 添加到此交换卷中。首先，停止现有的交换空间。如果交换空间正在使用，终止正在运行的程序。<p/>\r\nswapoff -a\r\n现在增加逻辑卷的大小。<p/>\r\n1234\r\n[root@studentvm1 ~]# lvextend -L +2G /dev/mapper/fedora_studentvm1-swap  Size of logical volume fedora_studentvm1/swap changed from 8.00 GiB (2048 extents) to 10.00 GiB (2560 extents).  Logical volume fedora_studentvm1/swap successfully resized.[root@studentvm1 ~]#\r\n运行 mkswap 命令将整个 10GB 分区变成交换空间。<p/>\r\n12345\r\n[root@studentvm1 ~]# mkswap /dev/mapper/fedora_studentvm1-swapmkswap: /dev/mapper/fedora_studentvm1-swap: warning: wiping old swap signature.Setting up swapspace version 1, size = 10 GiB (10737414144 bytes)no label, UUID=3cc2bee0-e746-4b66-aa2d-1ea15ef1574a[root@studentvm1 ~]#\r\n重新启用交换空间。<p/>\r\n[root@studentvm1 ~]# swapon -a[root@studentvm1 ~]#\r\n现在，使用 lsblk 命令验证新交换空间是否存在。同样，不需要重新启动机器。<p/>\r\n1234567891011121314151617181920\r\n[root@studentvm1 ~]# lsblkNAME                                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda                                    8:0    0   60G  0 disk|-sda1                                 8:1    0    1G  0 part /boot`-sda2                                 8:2    0   59G  0 part  |-fedora_studentvm1-pool00_tmeta   253:0    0    4M  0 lvm    | `-fedora_studentvm1-pool00-tpool 253:2    0    2G  0 lvm    |   |-fedora_studentvm1-root       253:3    0    2G  0 lvm  /  |   `-fedora_studentvm1-pool00     253:6    0    2G  0 lvm    |-fedora_studentvm1-pool00_tdata   253:1    0    2G  0 lvm    | `-fedora_studentvm1-pool00-tpool 253:2    0    2G  0 lvm    |   |-fedora_studentvm1-root       253:3    0    2G  0 lvm  /  |   `-fedora_studentvm1-pool00     253:6    0    2G  0 lvm    |-fedora_studentvm1-swap           253:4    0   10G  0 lvm  [SWAP]  |-fedora_studentvm1-usr            253:5    0   15G  0 lvm  /usr  |-fedora_studentvm1-home           253:7    0    2G  0 lvm  /home  |-fedora_studentvm1-var            253:8    0   10G  0 lvm  /var  `-fedora_studentvm1-tmp            253:9    0    5G  0 lvm  /tmpsr0                                   11:0    1 1024M  0 rom  [root@studentvm1 ~]#\r\n您也可以使用 swapon -s 命令或 top、free 或其他几个命令来验证这一点。<p/>\r\n12345\r\n[root@studentvm1 ~]# free              total        used        free      shared  buff/cache   availableMem:        4038808      382404     2754072        4152      902332     3404184Swap:      10485756           0    10485756[root@studentvm1 ~]#\r\n请注意，不同的命令以不同的形式显示或要求输入设备文件。在 /dev 目录中访问特定设备有多种方式。在我的文章 在 Linux 中管理设备 中有更多关于 /dev 目录及其内容说明。<p/>\n <p/>\n 1 收藏\n", "front_img_path": "full/daa241c211f453f9b4c5a81e5c318f6d7431a02b.jpg"},{"title": "Ubuntu 上更改 MySQL 数据库数据存储目录", "url": "http://blog.jobbole.com/114485/", "create_date": "2018/11/07", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2015/11/e78e36715813f49e9e62fe0c6050075c.png"], "fav_nums": 1, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,MySQL,数据库", "object_id": "c2756a5fefdd2d0ac6c67a7d153278d5", "content": "root@mylnx2:/etc/mysql/mysql.conf.d# service mysql start Job for mysql.service failed because the control process exited with error code. See \"systemctl status mysql.service\" and \"journalctl -xe\" for details.\r\n连MySQL的错误日志也未生成，使用service mysql status命令可以输出一些较详细的信息，如下所示：<p/>\r\n123456789101112131415161718192021222324\r\nroot@mylnx2:/etc/mysql/mysql.conf.d# service mysql status● mysql.service - MySQL Community Server   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)   Active: activating (start-post) (Result: exit-code) since Mon 2018-10-15 22:33:00 CST; 28s ago  Process: 12947 ExecStart=/usr/sbin/mysqld (code=exited, status=1/FAILURE)  Process: 12932 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 12947 (code=exited, status=1/FAILURE);         : 12948 (mysql-systemd-s)    Tasks: 2   Memory: 1.9M      CPU: 367ms   CGroup: /system.slice/mysql.service           └─control             ├─12948 /bin/bash /usr/share/mysql/mysql-systemd-start post             └─13045 sleep 1 Oct 15 22:33:00 mylnx2 systemd[1]: Starting MySQL Community Server...Oct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.013763Z 0 [Warning] Changed limits: max_open_files: 1024 (requested 5000)Oct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.013836Z 0 [Warning] Changed limits: table_open_cache: 431 (requested 2000)Oct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.207533Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicitOct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.207663Z 0 [Warning] Can't create test file /mysql_data/mysql/mylnx2.lower-testOct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.207717Z 0 [Note] /usr/sbin/mysqld (mysqld 5.7.23-0ubuntu0.16.04.1-log) starting as process 129Oct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.215413Z 0 [Warning] Can't create test file /mysql_data/mysql/mylnx2.lower-testOct 15 22:33:01 mylnx2 mysqld[12947]: 2018-10-15T14:33:01.215445Z 0 [Warning] Can't create test file /mysql_data/mysql/mylnx2.lower-testOct 15 22:33:01 mylnx2 systemd[1]: mysql.service: Main process exited, code=exited, status=1/FAILURE\r\n错误信息为“[Warning] Cant create test file /mysql_data/mysql/mylnx2.lower-test”。其实这里是踩到了“AppArmor”这个坑，之前对Ubuntu了解不多，所以直到遇到这个问题，才了解、知道这么个概念。下面是百科对AppArmor的介绍：<p/>\nAppArmor是一个高效和易于使用的Linux系统安全应用程序。AppArmor对操作系统和应用程序所受到的威胁进行从内到外的保护，甚至是未被发现的0day漏洞和未知的应用程序漏洞所导致的攻击。AppArmor安全策略可以完全定义个别应用程序可以访问的系统资源与各自的特权。AppArmor包含大量的默认策略，它将先进的静态分析和基于学习的工具结合起来，AppArmor甚至可以使非常复杂的应用可以使用在很短的时间内应用成功。<p/>\nAppArmor对MySQL所能使用的目录权限做了限制，如下截图所示，规定了MySQL使用的数据文件路径权限。<p/>\n# cat /etc/apparmor.d/usr.sbin.mysqld<p/>\n<img class=\"aligncenter\" title=\"clip_image001\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/c52fe66b891ce5d266ba7650b63b04c9.png\" alt=\"clip_image001\" width=\"650\" height=\"564\" /><p/>\n我将MySQL的数据库数据存储目录从/var/lib/mysql 切换到/mysql_data/mysql下面。所以就遇到了上面错误，需要修改或新增两条记录，从而使mysqld可以使用/mysql_data/mysql这个目录<p/>\n/mysql_data/mysql/ r,<p/>\n/mysql_data/mysql/** rwk,<p/>\n然后重启AppArmor服务后，然后就可以重启MySQL服务了。<p/>\nsudo service apparmor restart<p/>\n当然/etc/apparmor.d/usr.sbin.mysqld还有Allow plugin access需要调整，这个不是重点，在此略过。<p/>\n犹豫了一会，还是记录一下这个小小案例！虽然网上已有不少人总结这个，但是自己动手总结一下，印象也深刻一点！<p/>\n 1 收藏\n", "front_img_path": "full/35011d6168be00e949624c665041dc724e3ad786.jpg"},{"title": "如何在 Linux 中找到并删除重复文件", "url": "http://blog.jobbole.com/114455/", "create_date": "2018/10/19", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/10/d1aa5a9343099d2decbb150c0a46356d.png"], "fav_nums": 1, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,Linux", "object_id": "7d7ff91d7ad7e2d2a0d34d3c83959d6c", "content": "原文出处： SK   译文出处：Linux中国/Andy Luo   <img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/d1aa5a9343099d2decbb150c0a46356d.png\" alt=\"\" /><p/>\n在编辑或修改配置文件或旧文件前，我经常会把它们备份到硬盘的某个地方，因此我如果意外地改错了这些文件，我可以从备份中恢复它们。但问题是如果我忘记清理备份文件，一段时间之后，我的磁盘会被这些大量重复文件填满 —— 我觉得要么是懒得清理这些旧文件，要么是担心可能会删掉重要文件。如果你们像我一样，在类 Unix 操作系统中，大量多版本的相同文件放在不同的备份目录，你可以使用下面的工具找到并删除重复文件。<p/>\n提醒一句：<p/>\n在删除重复文件的时请尽量小心。如果你不小心，也许会导致意外丢失数据。我建议你在使用这些工具的时候要特别注意。<p/>\n在 Linux 中找到并删除重复文件\n出于本指南的目的，我将讨论下面的三个工具：<p/>\nRdfind\nFdupes\nFSlint\n这三个工具是自由开源的，且运行在大多数类 Unix 系统中。<p/>\n1. Rdfind\nRdfind 意即 redundant data find（冗余数据查找），是一个通过访问目录和子目录来找出重复文件的自由开源的工具。它是基于文件内容而不是文件名来比较。Rdfind 使用排序算法来区分原始文件和重复文件。如果你有两个或者更多的相同文件，Rdfind 会很智能的找到原始文件并认定剩下的文件为重复文件。一旦找到副本文件，它会向你报告。你可以决定是删除还是使用硬链接或者符号（软）链接代替它们。<p/>\n安装 Rdfind<p/>\nRdfind 存在于 AUR 中。因此，在基于 Arch 的系统中，你可以像下面一样使用任一如 Yay AUR 程序助手安装它。<p/>\r\n$ yay -S rdfind\r\n在 Debian、Ubuntu、Linux Mint 上：<p/>\r\n$ sudo apt-get install rdfind\r\n在 Fedora 上：<p/>\r\n$ sudo dnf install rdfind\r\n在 RHEL、CentOS 上：<p/>\r\n$ sudo yum install epel-release$ sudo yum install rdfind\r\n用法<p/>\n一旦安装完成，仅带上目录路径运行 Rdfind 命令就可以扫描重复文件。<p/>\r\n$ rdfind ~/Downloads\r\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/741174a33e731dd7e7a282b5302e9808.png\" alt=\"\" /><p/>\n正如你看到上面的截屏，Rdfind 命令将扫描 ~/Downloads 目录，并将结果存储到当前工作目录下一个名为 results.txt 的文件中。你可以在 results.txt 文件中看到可能是重复文件的名字。<p/>\r\n12345678910\r\n$ cat results.txt# Automatically generated# duptype id depth size device inode priority nameDUPTYPE_FIRST_OCCURRENCE 1469 8 9 2050 15864884 1 /home/sk/Downloads/tor-browser_en-US/Browser/TorBrowser/Tor/PluggableTransports/fte/tests/dfas/test5.regexDUPTYPE_WITHIN_SAME_TREE -1469 8 9 2050 15864886 1 /home/sk/Downloads/tor-browser_en-US/Browser/TorBrowser/Tor/PluggableTransports/fte/tests/dfas/test6.regex[...]DUPTYPE_FIRST_OCCURRENCE 13 0 403635 2050 15740257 1 /home/sk/Downloads/Hyperledger(1).pdfDUPTYPE_WITHIN_SAME_TREE -13 0 403635 2050 15741071 1 /home/sk/Downloads/Hyperledger.pdf# end of file \r\n通过检查 results.txt 文件，你可以很容易的找到那些重复文件。如果愿意你可以手动的删除它们。<p/>\n此外，你可在不修改其他事情情况下使用 -dryrun 选项找出所有重复文件，并在终端上输出汇总信息。<p/>\r\n$ rdfind -dryrun true ~/Downloads\r\n一旦找到重复文件，你可以使用硬链接或符号链接代替他们。<p/>\n使用硬链接代替所有重复文件，运行：<p/>\r\n$ rdfind -makehardlinks true ~/Downloads\r\n使用符号链接/软链接代替所有重复文件，运行：<p/>\r\n$ rdfind -makesymlinks true ~/Downloads\r\n目录中有一些空文件，也许你想忽略他们，你可以像下面一样使用 -ignoreempty 选项：<p/>\r\n$ rdfind -ignoreempty true ~/Downloads\r\n如果你不再想要这些旧文件，删除重复文件，而不是使用硬链接或软链接代替它们。<p/>\n删除重复文件，就运行：<p/>\r\n$ rdfind -deleteduplicates true ~/Downloads\r\n如果你不想忽略空文件，并且和所哟重复文件一起删除。运行：<p/>\r\n$ rdfind -deleteduplicates true -ignoreempty false ~/Downloads\r\n更多细节，参照帮助部分：<p/>\r\n$ rdfind --help\r\n手册页：<p/>\r\n$ man rdfind\r\n2. Fdupes\nFdupes 是另一个在指定目录以及子目录中识别和移除重复文件的命令行工具。这是一个使用 C 语言编写的自由开源工具。Fdupes 通过对比文件大小、部分 MD5 签名、全部 MD5 签名，最后执行逐个字节对比校验来识别重复文件。<p/>\n与 Rdfind 工具类似，Fdupes 附带非常少的选项来执行操作，如：<p/>\n在目录和子目录中递归的搜索重复文件\n从计算中排除空文件和隐藏文件\n显示重复文件大小\n出现重复文件时立即删除\n使用不同的拥有者/组或权限位来排除重复文件\n安装 Fdupes<p/>\nFdupes 存在于大多数 Linux 发行版的默认仓库中。<p/>\n在 Arch Linux 和它的变种如 Antergos、Manjaro Linux 上，如下使用 Pacman 安装它。<p/>\r\n$ sudo pacman -S fdupes\r\n在 Debian、Ubuntu、Linux Mint 上:<p/>\r\n$ sudo apt-get install fdupes\r\n在 Fedora 上：<p/>\r\n$ sudo dnf install fdupes\r\n在 RHEL、CentOS 上：<p/>\r\n$ sudo yum install epel-release$ sudo yum install fdupes\r\n用法<p/>\nFdupes 用法非常简单。仅运行下面的命令就可以在目录中找到重复文件，如：~/Downloads。<p/>\r\n$ fdupes ~/Downloads\r\n我系统中的样例输出：<p/>\r\n/home/sk/Downloads/Hyperledger.pdf/home/sk/Downloads/Hyperledger(1).pdf\r\n你可以看到，在 /home/sk/Downloads/ 目录下有一个重复文件。它仅显示了父级目录中的重复文件。如何显示子目录中的重复文件？像下面一样，使用 -r 选项。<p/>\r\n$ fdupes -r ~/Downloads\r\n现在你将看到 /home/sk/Downloads/ 目录以及子目录中的重复文件。<p/>\nFdupes 也可用来从多个目录中迅速查找重复文件。<p/>\r\n$ fdupes ~/Downloads ~/Documents/ostechnix\r\n你甚至可以搜索多个目录，递归搜索其中一个目录，如下：<p/>\r\n$ fdupes ~/Downloads -r ~/Documents/ostechnix\r\n上面的命令将搜索 ~/Downloads 目录，~/Documents/ostechnix 目录和它的子目录中的重复文件。<p/>\n有时，你可能想要知道一个目录中重复文件的大小。你可以使用 -S 选项，如下：<p/>\r\n1234\r\n$ fdupes -S ~/Downloads403635 bytes each:/home/sk/Downloads/Hyperledger.pdf/home/sk/Downloads/Hyperledger(1).pdf\r\n类似的，为了显示父目录和子目录中重复文件的大小，使用 -Sr 选项。<p/>\n我们可以在计算时分别使用 -n 和 -A 选项排除空白文件以及排除隐藏文件。<p/>\r\n$ fdupes -n ~/Downloads$ fdupes -A ~/Downloads\r\n在搜索指定目录的重复文件时，第一个命令将排除零长度文件，后面的命令将排除隐藏文件。<p/>\n汇总重复文件信息，使用 -m 选项。<p/>\r\n$ fdupes -m ~/Downloads1 duplicate files (in 1 sets), occupying 403.6 kilobytes\r\n删除所有重复文件，使用 -d 选项。<p/>\r\n$ fdupes -d ~/Downloads\r\n样例输出：<p/>\r\n1234\r\n[1] /home/sk/Downloads/Hyperledger Fabric Installation.pdf[2] /home/sk/Downloads/Hyperledger Fabric Installation(1).pdf Set 1 of 1, preserve files [1 - 2, all]:\r\n这个命令将提示你保留还是删除所有其他重复文件。输入任一号码保留相应的文件，并删除剩下的文件。当使用这个选项的时候需要更加注意。如果不小心，你可能会删除原文件。<p/>\n如果你想要每次保留每个重复文件集合的第一个文件，且无提示的删除其他文件，使用 -dN 选项（不推荐）。<p/>\r\n$ fdupes -dN ~/Downloads\r\n当遇到重复文件时删除它们，使用 -I 标志。<p/>\r\n$ fdupes -I ~/Downloads\r\n关于 Fdupes 的更多细节，查看帮助部分和 man 页面。<p/>\r\n$ fdupes --help$ man fdupes\r\n3. FSlint\nFSlint 是另外一个查找重复文件的工具，有时我用它去掉 Linux 系统中不需要的重复文件并释放磁盘空间。不像另外两个工具，FSlint 有 GUI 和 CLI 两种模式。因此对于新手来说它更友好。FSlint 不仅仅找出重复文件，也找出坏符号链接、坏名字文件、临时文件、坏的用户 ID、空目录和非精简的二进制文件等等。<p/>\n安装 FSlint<p/>\nFSlint 存在于 AUR，因此你可以使用任一 AUR 助手安装它。<p/>\r\n$ yay -S fslint\r\n在 Debian、Ubuntu、Linux Mint 上：<p/>\r\n$ sudo apt-get install fslint\r\n在 Fedora 上：<p/>\r\n$ sudo dnf install fslint\r\n在 RHEL，CentOS 上：<p/>\r\n$ sudo yum install epel-release$ sudo yum install fslint\r\n一旦安装完成，从菜单或者应用程序启动器启动它。<p/>\nFSlint GUI 展示如下：<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/8a48bf58f4a93c1173539c7f6d778dea.png\" alt=\"\" /><p/>\n如你所见，FSlint 界面友好、一目了然。在 “Search path” 栏，添加你要扫描的目录路径，点击左下角 “Find” 按钮查找重复文件。验证递归选项可以在目录和子目录中递归的搜索重复文件。FSlint 将快速的扫描给定的目录并列出重复文件。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/8310df5486050ddd9935ad837eb6f0be.png\" alt=\"\" /><p/>\n从列表中选择那些要清理的重复文件，也可以选择 “Save”、“Delete”、“Merge” 和 “Symlink” 操作他们。<p/>\n在 “Advanced search parameters” 栏，你可以在搜索重复文件的时候指定排除的路径。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/9418bc053dbccdb0e6cbe292defd02d6.png\" alt=\"\" /><p/>\nFSlint 命令行选项<p/>\nFSlint 提供下面的 CLI 工具集在你的文件系统中查找重复文件。<p/>\nfindup — 查找重复文件\nfindnl — 查找名称规范（有问题的文件名）\nfindu8 — 查找非法的 utf8 编码的文件名\nfindbl — 查找坏链接（有问题的符号链接）\nfindsn — 查找同名文件（可能有冲突的文件名）\nfinded — 查找空目录\nfindid — 查找死用户的文件\nfindns — 查找非精简的可执行文件\nfindrs — 查找文件名中多余的空白\nfindtf — 查找临时文件\nfindul — 查找可能未使用的库\nzipdir — 回收 ext2 目录项下浪费的空间\n所有这些工具位于 /usr/share/fslint/fslint/fslint 下面。<p/>\n例如，在给定的目录中查找重复文件，运行：<p/>\r\n$ /usr/share/fslint/fslint/findup ~/Downloads/\r\n类似的，找出空目录命令是：<p/>\r\n$ /usr/share/fslint/fslint/finded ~/Downloads/\r\n获取每个工具更多细节，例如：findup，运行：<p/>\r\n$ /usr/share/fslint/fslint/findup --help\r\n关于 FSlint 的更多细节，参照帮助部分和 man 页。<p/>\r\n$ /usr/share/fslint/fslint/fslint --help$ man fslint\r\n现在你知道在 Linux 中，使用三个工具来查找和删除不需要的重复文件。这三个工具中，我经常使用 Rdfind。这并不意味着其他的两个工具效率低下，因为到目前为止我更喜欢 Rdfind。好了，到你了。你的最喜欢哪一个工具呢？为什么？在下面的评论区留言让我们知道吧。<p/>\n就到这里吧。希望这篇文章对你有帮助。更多的好东西就要来了，敬请期待。<p/>\n谢谢！<p/>\n <p/>\n 1 收藏\n", "front_img_path": "full/cc46810de4d4f03e9a6d0efb47fea756cb975a55.jpg"},{"title": "2018 年最好的 Linux 发行版", "url": "http://blog.jobbole.com/114473/", "create_date": "2018/10/28", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/10/22b50c4f6378466a0259ad8ceeedd1ed.jpg"], "fav_nums": 4, "comment_nums": 1, "vote_nums": 3, "tags": "IT技术,,Linux", "object_id": "d19d16854cc8aac90eb50785ce0e89f5", "content": "原文出处： Jack Wallen   译文出处：Linux中国/dianbanjiu   <img class=\"aligncenter\" title=\"Linux distros 2018\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/22b50c4f6378466a0259ad8ceeedd1ed.jpg\" alt=\"Linux distros 2018\" /><p/>\nJack Wallen 分享他挑选的 2018 年最好的 Linux 发行版。<p/>\n这是新的一年，Linux 仍有无限可能。而且许多 Linux 发行版在 2017 年都带来了许多重大的改变，我相信在 2018 年它在服务器和桌面上将会带来更加稳定的系统和市场份额的增长。<p/>\n对于那些期待迁移到开源平台（或是那些想要切换到）的人对于即将到来的一年，什么是最好的选择？如果你去 Distrowatch 找一下，你可能会因为众多的发行版而感到头晕，其中一些的排名在上升，而还有一些则恰恰相反。<p/>\n因此，哪个 Linux 发行版将在 2018 年得到偏爱？我有我的看法。事实上，我现在就要和你们分享它。<p/>\n跟我做的 去年清单 相似，我将会打破那张清单，使任务更加轻松。普通的 Linux 用户，至少包含以下几个类别：系统管理员，轻量级发行版，桌面，为物联网和服务器发行的版本。<p/>\n根据这些，让我们开始 2018 年最好的 Linux 发行版清单吧。<p/>\n对系统管理员最好的发行版\nDebian 不常出现在“最好的”列表中。但它应该出现，为什么呢？如果了解到 Ubuntu 是基于 Debian 构建的（其实有很多的发行版都基于 Debian），你就很容易理解为什么这个发行版应该在许多“最好”清单中。但为什么是对管理员最好的呢？我想这是由于两个非常重要的原因：<p/>\n因为 Debain 使用 dpkg 和 apt 包管理，它使得使用该环境非常简单。而且因为 Debian 提供了最稳定的 Linux 平台之一，它为许多事物提供了理想的环境：桌面、服务器、测试、开发。虽然 Debian 可能不包括去年本分类的优胜者 Parrot Linux 所带有的大量应用程序，但添加完成任务所需的任何或全部必要的应用程序都非常容易。而且因为 Debian 可以根据你的选择安装不同的桌面（Cinnamon、GNOME、KDE、LXDE、Mate 或者 Xfce），肯定可以满足你对桌面的需求。<p/>\n<img class=\"aligncenter\" title=\"debian\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/ba0f86c5581334983ac2255d8ab23c37.jpg\" alt=\"debian\" /><p/>\n图 1：在 Debian 9.3 上运行的 GNOME 桌面。<p/>\n同时，Debain 在 Distrowatch 上名列第二。下载、安装，然后让它为你的工作而服务吧。Debain 尽管不那么华丽，但是对于管理员的工作来说十分有用。<p/>\n最轻量级的发行版\n轻量级的发行版有其特殊的用途：给予一些老旧或是性能低下的机器以新生。但是这不意味着这些特别的发行版仅仅只为了老旧的硬件机器而生。如果你想要的是运行速度，你可能会想知道在你的现代机器上这类发行版的运行速度能有多快。<p/>\n在 2018 年上榜的最轻量级的发行版是 Lubuntu。尽管在这个类别里还有很多选择，而且尽管 Lubuntu 的资源占用与 Puppy Linux 一样小，但得益于它是 Ubuntu 家庭的一员，其易用性为它加了分。但是不要担心，Lubuntu 对于硬件的要求并不高：<p/>\nCPU：奔腾 4 或者奔腾 M 或者 AMD K8 以上\n对于本地应用，512 MB 的内存就可以了，对于网络使用（Youtube、Google+、Google Drive、Facebook），建议 1 GB 以上。\nLubuntu 使用的是 LXDE 桌面（图 2），这意味着新接触 Linux 的用户在使用这个发行版时不会有任何问题。这份简短清单中包含的应用（例如：Abiword、Gnumeric 和 Firefox）都是非常轻量的，且对用户友好的。<p/>\n<img class=\"aligncenter\" title=\"Lubuntu\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/817db13f279a7f8434f1b862d97d7aab.jpg\" alt=\"Lubuntu\" /><p/>\n图 2：LXDE桌面。<p/>\nLubuntu 能让十年以上的电脑如获新生。<p/>\n最好的桌面发行版\nElementary OS 连续两年都是我清单中最好的桌面发行版。对于许多人，Linux Mint （也是一个非常棒的分支）都是桌面发行版的领袖。但是，于我来说，它在易用性和稳定性上很难打败 Elementary OS。例如，我确信是 Ubuntu 17.10 的发布让我迁移回了 Canonical 的发行版。迁移到新的使用 GNOME 桌面的 Ubuntu 不久之后，我发现我缺少了 Elementary OS 外观、可用性和感觉（图 3）。在使用 Ubuntu 两周以后，我又换回了 Elementary OS。<p/>\n<img class=\"aligncenter\" title=\"Elementary OS\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/0383cb05ef31b9fc2b3d39b5633c7359.jpg\" alt=\"Elementary OS\" /><p/>\n图 3：Pantheon 桌面是一件像艺术品一样的桌面。<p/>\n使用 Elementary OS 的任何一个人都会觉得宾至如归。Pantheon 桌面是将操作顺滑和用户友好结合的最完美的桌面。每次更新，它都会变得更好。<p/>\n尽管 Elementary OS 在 Distrowatch 页面访问量中排名第六，但我预计到 2018 年末，它将至少上升至第三名。Elementary 开发人员非常关注用户的需求。他们倾听并且改进，这个发行版目前的状态是如此之好，似乎他们一切都可以做的更好。 如果您需要一个具有出色可靠性和易用性的桌面，Elementary OS 就是你的发行版。<p/>\n能够证明自己的最好的发行版\n很长一段时间内，Gentoo 都稳坐“展现你技能”的发行版的首座。但是，我认为现在 Gentoo 是时候让出“证明自己”的宝座给 Linux From Scratch（LFS）。你可能认为这不公平，因为 LFS 实际上不是一个发行版，而是一个帮助用户创建自己的 Linux 发行版的项目。但是，有什么能比你自己创建一个自己的发行版更能证明自己所学的 Linux 知识的呢？在 LFS 项目中，你可以从头开始构建自定义的 Linux 系统，而且是从源代码开始。 所以，如果你真的想证明些什么，请下载 Linux From Scratch Book 并开始构建。<p/>\n对于物联网最好的发行版\nUbuntu Core 已经是第二年赢得了该项的冠军。Ubuntu Core 是 Ubuntu 的一个小型的、事务型版本，专为嵌入式和物联网设备而构建。使 Ubuntu Core 如此完美支持物联网的原因在于它将重点放在 snap 包上 —— 这种通用包可以安装到一个平台上而不会干扰其基本系统。这些 snap 包包含它们运行所需的所有内容（包括依赖项），因此不必担心安装它会破坏操作系统（或任何其他已安装的软件）。 此外，snap 包非常容易升级，并运行在隔离的沙箱中，这使它们成为物联网的理想解决方案。<p/>\nUbuntu Core 内置的另一个安全领域是登录机制。Ubuntu Core 使用Ubuntu One ssh密钥，这样登录系统的唯一方法是通过上传的 ssh 密钥到 Ubuntu One帐户（图 4）。这为你的物联网设备提供了更高的安全性。<p/>\n<img class=\"aligncenter\" title=\" Ubuntu Core\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/c3d0565fd56cafc674f19bd6172e2f48.jpg\" alt=\" Ubuntu Core\" /><p/>\n图 4：Ubuntu Core屏幕指示通过Ubuntu One用户启用远程访问。<p/>\n最好的服务器发行版\n这里有点意见不统一。主要原因是支持。如果你需要商业支持，乍一看，你最好的选择可能是 Red Hat Enterprise Linux。红帽年复一年地证明了自己不仅是全球最强大的企业服务器平台之一，而且是单一最赚钱的开源业务（年收入超过 20 亿美元）。<p/>\n但是，Red Hat 并不是唯一的服务器发行版。 实际上，Red Hat 甚至并不能垄断企业服务器计算的各个方面。如果你关注亚马逊 Elastic Compute Cloud 上的云统计数据，Ubuntu 就会打败红帽企业 Linux。根据云市场的报告，EC2 统计数据显示 RHEL 的部署率低于 10 万，而 Ubuntu 的部署量超过 20 万。<p/>\n最终的结果是，Ubuntu 几乎已经成为云计算的领导者。如果你将它与 Ubuntu 对容器的易用性和可管理性结合起来，就会发现 Ubuntu Server 是服务器类别的明显赢家。而且，如果你需要商业支持，Canonical 将为你提供 Ubuntu Advantage。<p/>\n对使用 Ubuntu Server 的一个警告是它默认为纯文本界面（图 5）。如果需要，你可以安装 GUI，但使用 Ubuntu Server 命令行非常简单（每个 Linux 管理员都应该知道）。<p/>\n<img title=\"Ubuntu server\" src=\"http://jbcdn2.b0.upaiyun.com/2018/10/abf254081e34927b1644ea7fcc30e563.jpg\" alt=\"Ubuntu server\" /><p/>\n图 5：Ubuntu 服务器登录，通知更新。<p/>\n正如我之前所说，这些选择都非常主观，但如果你正在寻找一个好的开始，那就试试这些发行版。每一个都可以用于非常特定的目的，并且比大多数做得更好。虽然你可能不同意我的个别选择，但你可能会同意 Linux 在每个方面都提供了惊人的可能性。并且，请继续关注下周更多“最佳发行版”选秀。<p/>\n通过 Linux 基金会和 edX 的免费“Linux 简介”课程了解有关Linux的更多信息。<p/>\n <p/>\n 4 收藏\n 1 评论\n", "front_img_path": "full/835dea16bfc384fe2e59c8297ac5166f44525df3.jpg"},{"title": "软件工程师生存指南：面试准备、工作经验和实用工具", "url": "http://blog.jobbole.com/114496/", "create_date": "2018/11/09", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2017/11/90129c6662feee86738bd3663ce83108.png"], "fav_nums": 2, "comment_nums": 1, "vote_nums": 1, "tags": "职场,,开发,面试", "object_id": "6fbd4af91eb7fe092be2ce62b423bea1", "content": "原文出处： Valeri Alexiev   译文出处：36kr   编者按：软件工程师是令人羡慕的职业。但是如何才能拿到这份工作？又如何才能做好这份工作呢？拥有相关经验的 Valeri Alexiev 提供了相关建议和工具。其中包括了如何准备面试、如何以软件工程师的身份工作以及如何持续改进方面的经验之谈。<p/>\n我刚开始工作的头几年是紧张学习的时间。<p/>\n我得面对现实，成为软件工程师需要有很多技能，这些我之前都不知道。回顾过去，显然学会那些东西是很好的。<p/>\n所以我就根据自己及其他人的经验写了这篇指南来帮助入行的新人。<p/>\n本文将覆盖以下内容：<p/>\n如何尽力做好面试\n如何在软件工程师的角色中生存（及发展）\n考虑持续改进时可以参考的资源\n　　面试<p/>\n当你开始软件工程职业生涯时，你得面对一个无可争议的事实。面试很恶心。<p/>\n参与其中的每个人都觉得很恶心。既被人面试过又面试过别人的我可以证明，面试是一项极其耗时、极其有压力的工作，并且面试其实并不是将来工作表现一个的好的指示器。但不管怎样，这都是一个必要之恶，你和你的简历最好还是做好准备为妥。<p/>\n　　做好战斗准备<p/>\n如果你考虑做软件过程，确保了解一些最常见的编程面试问题，比如“FizzBuzz”：<p/>\n写一个程序打印从 1 到 100 的数字。但是如果数字是 3 的倍数的话则打印“Fizz”，如果数字是 5 的倍数则打印“Buzz”。如果同时是 3 和 5 的倍数则打印“FizzBuzz”。<p/>\n来自 Coding Horror<p/>\n听起来很简单，是吧？<p/>\n好吧，但其实绝大部分面试者都没能通过这一简单的测试，且不说更复杂的变种了。<p/>\n我个人曾经见过很多角逐资深岗位的候选人在拥有完全互联网访问的情况下没能通过这一测试。所以如果你的简历上面列有编程语言的话，确保你知道如何用它来编写实现 FizzBuzz 程序。否则的话，你只不过是在浪费所有人的时间，包括你自己的。<p/>\n当然，为了在面试过后生存，你需要知道的不仅仅只有 FizzBuzz。你还需要确保你知道：<p/>\n基础的数据结构和算法：比如链表、数组、树以及排序。\n要知道所选择语言的常见解决办法，比如字符串是否恒定，内存是如何管理的。\n类似类与对象，以及继承等面向对象编程的概念。\n在职业生涯开始时，你需要就这些问题做好准备，因为你并没有经历去证明自己能做好这份工作。在准备面试的时候有两个资源我会经常推荐：<p/>\n《Cracking the Coding Interview（破解面试代码）》，这是一本非常好的书，里面介绍了很多的编码问题和解决方案，同时还总结了解决这些问题需要了解的东西。\nCodeWars ，这个网站收集了大量的编程问题，你可以运用各种语言在浏览器里面去解决这些问题。最有用的部分是看看别的用户是如何解决同样问题的。这样你就可以看到解决相同问题的不同办法，并且学到你所选语言的新工具。\n　　赋予自身额外优势<p/>\n为了让自己取得那点额外优势，有几件事情你可以去做。<p/>\n首先，学会如何沟通你的经验。你应该进行一次电梯演讲来将你的简历总结成连贯的、打动人的个人介绍。<p/>\n此外，要了解自己的简历！听起来很蠢是吧，但我就见过很多面试者连解释清楚自己简历上的特定事项都很困难。你应该能够回答任何有关你列上简历的经历方面的问题，并且解释清楚这一经历如何让你成为本工作更好的候选人。<p/>\n接着，要在 GitHub（或者其他的公共代码库）上面有一些编码的例子。<p/>\n眼见为实，面试官能够看到你的代码将创造奇迹。此外，这还证明了你对版本控制系统有了解。<p/>\n你的代码例子不需要太复杂，但是一定要整洁，能够显示出好的编码实践。这是你展示自己在没有编码面试所带来的时间压力情况下代码写得如何的机会。<p/>\n一旦你做完了上面的事情后，就得考虑参与一个开源项目了。参加开源项目能表明你能够在已有代码库基础上工作并且可以与其他程序员一些协作。<p/>\n这是你在无需实际进入一个行业环境的情况下最接近在行业环境下编程的方式了。这也是目前为止最难最耗时的一项任务，所以等到你把前面我提到的比较容易取得的果实都摘完之后再干这件事。<p/>\n　　面试你的面试官<p/>\n在找工作的匆忙与压力之下，很多候选人都忘了面试是一个双向的过程。在公司努力寻找这份工作的合适人选时，你也应该设法弄清楚这家公司适不适合你。<p/>\n确保你也要提出以下一些问题，哪怕对方是以电子邮件的形式回复你。要意识到公司经常把不遵循最佳实践说成是一项技能，所以要体会其言外之意。<p/>\n以下是一些你可以去提问的例子：<p/>\n　　“对我来说典型的工作日会是什么样的？”<p/>\n知道特定岗位预期的样子很重要，因为软件工程工作差别相当大。比方说你的工作既可能是维护服务器，也可能是直接跟客户沟通。<p/>\n危险信号：“我不大肯定。” → 意味着面试你的那个人不在你的团队，或者他们对为什么要招你并没有明确的想法。<p/>\n　“你们是如何测试软件的？”<p/>\n理想情况下，验证代码质量应该是单元测试、人工测试以及自动化测试的结合。<p/>\n危险信号：“我们都写不出 bug，哈哈。” → 那些人正是会写出 bug 的人。<p/>\n　“你们使用什么样的版本控制系统？”<p/>\n版本控制系统对于协作极其有用，在职业环境下没有理由不使用。<p/>\n危险信号 #1：“额，版本控制系统？” → 快跑，跑得越远越好。<p/>\n永远记得使用版本控制。<p/>\n危险信号 #2：“<插入不知名的或者定制的 VCS>” → 这表明他们很有可能没有跟上时代并且很久没有升级自己的基础设施了。<p/>\n　　“你们进行同行评审吗？”<p/>\n同行评审，或者让别人看看你的代码再把它放进代码库，这是识别愚蠢错误的极好办法，同时也是开始你的职业生涯时一个关键的培训机会。<p/>\n危险信号：“我们相互信任！”→很有可能那些资深开发者对自己的代码非常警惕不想给人看也不擅长接受反馈。<p/>\n　　“你们的继续教育计划是什么样的？”<p/>\n作为一名软件工程师意味着当新技术出现、成熟并以令人眼花缭乱的速度走向过时的时候要不断学习。因此，很多公司都有培训预算用来买大学和在线课程、会议或者内部交流。<p/>\n危险信号：“你是说在闲暇时间读读网上的东西？” →这家公司要么资金紧张，要么把开发者视为可替代的，而不是长期投资。<p/>\n　　“你们采用的软件开发流程是什么？”<p/>\n无论实际的细节是什么，流程对于软件工程都至关重要。至于哪些东西对于优化流程做出了贡献可能大家会有不同的看法，但仅就项目的工作方式达成一致就能将混乱最小化并且确保每个人都能达成共识。<p/>\n危险信号：“我们的流程受到了自由风格的爵士的影响。” → 很有可能整个部门都处在救火模式，总是不断地从紧急跳到另一个紧急状态而缺乏任何明确的目标。<p/>\n　　“你们是如何处理技术债务的？”<p/>\n技术债务是过时技术以及代码库中临时应急的解决方案的累积。处理好技术债务对于代码的长期健康很重要，这件事情应该持续地做。<p/>\n危险信号：“我们只关注新功能。” → 他们的代码库一团糟或者很快就会一团糟。<p/>\n　　“你们的公司文化是什么样的？”<p/>\n公司文化也许是个非常含糊的概念，但即便像开放办公室还是小隔间这样的小事情都会显著改变你与同事的日常互动。这方面没有普遍性的危险信号，但是要确保他们的答案是你可以按照每周 40+ 小时的节奏持续相处数年的东西。<p/>\n　　以软件工程师的身份工作<p/>\n在这个阶段，如果你面试过程中表现不错并且喜欢面试官回答你问题的方式，你被录用的可能性就很高了。<p/>\n祝贺，你正式成为一名工程师了！<p/>\n那现在又该如何呢？好吧，现在是时候重新学习大量编码和工作方面的东西了。既然我们是程序员，我们就从讨论代码开始。<p/>\n　　好的行业代码<p/>\n好的行业代码有以下属性，依序是：<p/>\n可读性，因为代码用来读和维护的频次要高于写。代码的意图必须清晰，让其他开发者在多年后依然理解。\n防御性，就是要遵循防御性编码的最佳实践。防御性编码本身就是一个课题，不过其要义是：你必须确保自己所写的类和方法的不恰当使用不会导致你的代码搞得软件都崩溃。\n优化，位列清单的最后未知，因为大多数时候你并不需要真正去担心这个。这并不意味着你应该编写糟糕代码，在存在线性解决方案的情况下以O(n³)的效率去做某个东西。但开发者通常渴望尝试并且会在不需要的情况下过度优化，却牺牲了代码的可读性和防御性。你永远都应该能够证明牺牲了这些属性的特定优化是值得的。\n现在你了解了如何去编写良好的行业代码了。<p/>\n　　编码的工作你不会干太多的<p/>\n说出来也许有点令人吃惊，但是大多数时候你都不用写新代码，而是相反，要做：<p/>\n读已有代码\n开会或者写电子邮件\n研究该怎么做以便不用写代码\n因此编码以外的技能对你的职业一样关键。<p/>\n调试和阅读代码<p/>\n调试远不仅仅是用打印语句。一切使用广泛的语言和技术栈都有各种强大的工具。学会使用它们，因为这些会让调试轻而易举，节省你无数的时间。\n理解代码库。大多数技术栈都有某种代码图谱生成工具来帮助你理解代码库的结构。企业级的 IDE 通常都内置了那种功能。你还可以利用 ReSharper、grep 或者 Sourcegraph 之类的工具来探索代码。\n理解产品。你会对居然有这么多开发者在试图“修复”软件前不知道软件应该是怎么工作的感到惊讶。先看看文档再说吧。\n　　组织你的思路<p/>\n既然你的大量时间都是用在沟通、研究和多任务上，你需要一些工具来帮助一切井然有序。<p/>\nTODO 清单/任务工具：你的公司应该已经有了某种任务管理软件了，但你自己也有类似的个人系统是有帮助的。使用便利贴或者像 Trello 或者 Todoist 之类的软件。\n笔记：开会一定要记笔记，要致力于改进现有文档并且建立个人的知识库。使用 Evernote、OneNote 或者笔记本。使用这些工具似乎有用力过度之嫌，但日后在回顾这一耗掉了你 3 天时间才想清楚的晦涩的开发过程时你会感谢自己的。不做丰富笔记的好的软件开发者我一个都没见过。\n图表/可视化：人是视觉动物，创建流程图和架构可帮助你和其他人理解复杂的话题。在跟非技术人员沟通时图解尤其有用。可使用 Lucidchart、 Visio 或者白板。\n　　知道何时使用库<p/>\n简短回答：随时都要。<p/>\n详细回答：99% 的时间内你都不应该重新发明轮子。在大多数的软件工程岗位，实现特定类型的东西都属于纯粹浪费时间。这并不意味着你不应该知道所使用的算法和数据结构是怎么工作的，因为这可以帮助你决定用什么以及什么时候用。<p/>\n为了成为一名高效的软件工程师，你需要理解自己可以任意支配使用的那些库。大多数流行语言的标准库都是极其有用的，其规模比你想象的要大。此外，代码库也许也会利用了额外的特殊库。阅读其文档，知道什么使用去使用它们。<p/>\n你还应该不要害怕去建议额外的库，如果它们将节省时间的话。然而，你需要确保自己选择了一个好的库供行业使用。好的库的标准是：<p/>\n开源，这样你就可以验证自身代码的质量，并有可能修补对应用非常关键的 bug。\n按照 MIT 和 BSD 等方式进行的授权，这样你的公司使用起来就不会遇到任何问题。要小心 GPL，因为它会让你不小心就将整个代码库都开源出去。\n成熟，比方说出来已经有一段时间了，并且功能集非常丰富。\n维护性强，新版本推出很密集。\n别的公司或者项目也使用，这个可以充当品质认证确保有行业支持，能持续维护下去。\n　　持续改进<p/>\n为了替自己创建新的职业机会，除了学习会让你更擅长日常工作的技能以外，你还需要持续改进自身技能并且学习新技能。<p/>\n其实学习的机会有很多，而且其中很多都是你可以负担得起的：<p/>\n在线课程：向领域内最好的教授学习的机会，而且方式灵活，不容错过。现有技能的补充性教程可以去可以看看 Coursera、 Udacity 以及 edX 等。\n在线硕士学位：在线硕士学位是最近在顶级大学流行起来的一个趋势，这种方式可以灵活地继续你的正规教育。相比之下，这种继续教育方式费用没那么昂贵，修完整个学位大多数在 1 万美元左右。乔治亚理工大学、UT 以及加州大学圣地亚哥分校等大学均提供此类学位。我个人推荐乔治亚理工大学的在线硕士虚伪，我去年刚从这里毕业。\n博客：博客是开发者社区的重要组成部分。诸如 Coding Horror、Joel on Software 等博客或者甚至更加诙谐的网站如 The Daily WTF 等都可以为你提供信息，了解到作为软件工程师该干什么不该干什么。浏览 Medium、r/programming, HackerNews 等新闻流也能让你找到好博客和好文章。\n会议：最后但并非最不重要的一个，会议时令人赞叹的学习机会，你绝对应该利用公司的培训预算去参加会议。以下是不完全的好会议清单：GOTO（通用）， Strange Loop（通用）， PyCon （Pytho），CPPCon （C++），DEF CON （安全），Fluent （Web 开发）。上述所有的会议在 YouTube 上都有视频，所以你哪怕不出席也能学到东西！\n希望这篇文章能够用相关知识把你武装起来，让你了解到作为软件工程师的职业生涯伊始应该期待什么，并且提供合适的工具给你在开启这段令人兴奋的旅程中助你一臂之力！<p/>\n 2 收藏\n 1 评论\n", "front_img_path": "full/f49c8437c74dd90a906b7a9579e6520f3044c962.jpg"},{"title": "人人都能读懂的编译器原理", "url": "http://blog.jobbole.com/114466/", "create_date": "2018/11/01", "front_img_url_download": ["http://wx1.sinaimg.cn/mw690/7cc829d3ly1fwjgiwii9lj20qn0gqdgl.jpg"], "fav_nums": 13, "comment_nums": 2, "vote_nums": 4, "tags": "IT技术,,编译器", "object_id": "66d853efc56bcb1bdc594f4a6e4f1542", "content": "这是我在 Medium 上的第二篇文章的再版，上一版有超过 21000 的阅读量。很高兴我能够帮助到各位的学习，因此我根据上一版的评论，完完全全重写了。<p/>\n我选择 Rust 作为这篇文章的主要语言。它是一种详尽的、高效的、现代的而且看起来特意使得设计编译器变得简单。我很喜欢使用它。 https://www.rust-lang.org/<p/>\n写这篇文章的目的主要是吸引读者的注意力，而不是提供 20 多页的令人头皮发麻的阅读材料。对于那些你感兴趣的更深层次的话题，文章中有许多链接会引导你找到相关的资料。大多数链接到维基百科 。<p/>\n感谢你的关注，我希望你能够喜欢这些我花费了超过 20 个小时的写出的文章。欢迎在文章底部评论处留下任何问题或者建议。<p/>\n编译器是什么？\n你口中所说的编程语言本质上只是一个软件，这个软件叫做编译器，编译器读入一个文本文件，经过大量的处理，最终产生一个二进制文件。 编译器的语言部分就是它处理的文本样式。因为电脑只能读取 1 和 0 ，而人们编写 Rust 程序要比直接编写二进制程序简单地多，因此编译器就被用来把人类可读的文本转换成计算机可识别的机器码。<p/>\n编译器可以是任何可以把文本文件转换成其他文件的程序。例如，下面有一个用 Rust 语言写的编译器把 0 转换成 1，把 1 转换成 0 ：<p/>\r\n1234567891011121314\r\n// An example compiler that turns 0s into 1s, and 1s into 0s. fn main() {    let input = \"1 0 1 A 1 0 1 3\";     // iterate over every character `c` in input    let output: String = input.chars().map(|c|        if c == '1' { '0' }        else if c == '0' { '1' }        else { c } // if not 0 or 1, leave it alone    ).collect();     println!(\"{}\", output); // 0 1 0 A 0 1 0 3}\r\n编译器是做什么的？\n简言之，编译器获取源代码，产生一个二进制文件。因为从复杂的、人类可读的代码直接转化成0/1二进制会很复杂，所以编译器在产生可运行程序之前有多个步骤：<p/>\n从你给定的源代码中读取单个词。\n把这些词按照单词、数字、符号、运算符进行分类。\n通过模式匹配从分好类的单词中找出运算符，明确这些运算符想进行的运算，然后产生一个运算符的树（表达式树）。\n最后一步遍历表达式树中的所有运算符，产生相应的二进制数据。\n尽管我说编译器直接从表达式树转换到二进制，但实际上它会产生汇编代码，之后汇编代码会被汇编/编译到二进制数据。汇编程序就好比是一种高级的、人类可读的二进制。更多关于汇编语言的阅读资料在这里。<p/>\n<img class=\"alignnone\" src=\"http://wx1.sinaimg.cn/mw690/7cc829d3ly1fwjgiwii9lj20qn0gqdgl.jpg\" width=\"690\" height=\"433\" /><p/>\n解释器是什么？\n解释器 非常像编译器，它也是读入编程语言的代码，然后处理这些代码。尽管如此，解释器会跳过了代码生成，然后即时编译并执行 AST。 解释器最大的优点就在于在你 debug 期间运行程序所消耗的时间。编译器编译一个程序可能在一秒到几分钟不等，然而解释器可以立即开始执行程序，而不必编译。解释器最大的缺点在于它必须安装在用户电脑上，程序才可以执行。<p/>\n<img id=\"pic\" class=\"\" src=\"http://wx3.sinaimg.cn/large/7cc829d3ly1fwjgj08fnsj20xu06v0t4.jpg\" /><p/>\n虽然这篇文章主要是关于编译器的，但是对于编译器和解释器之间的区别和编译器相关的内容一定要弄清楚。<p/>\n1. 词法分析\n第一步是把输入一个词一个词的拆分开。这一步被叫做 词法分析,或者说是分词。这一步的关键就在于 我们把字符组合成我们需要的单词、标识符、符号等等。 词法分析大多都不需要处理逻辑运算像是算出 2+2  其实这个表达式只有三种 标记：一个数字：2,一个加号，另外一个数字：2。<p/>\n让我们假设你正在解析一个像是 12+3 这样的字符串：它会读入字符 1，2，+，和 3。我们已经把这些字符拆分开了，但是现在我们必须把他们组合起来；这是分词器的主要任务之一。举个例子，我们得到了两个单独的字符 1 和 2，但是我们需要把它们放到一起，然后把它们解析成为一个整数。至于 +也需要被识别为加号，而不是它的字符值  字符值是43 。<p/>\n如果你可以阅读过上面的代码，并且弄懂了这样做的含义，接下来的 Rust 分词器会组合数字为32位整数，加号就最后了标记值 Plus（加）.<p/>\nrust playground<p/>\n你可以点击 Rust playgroud 左上角的 “Run 按钮来编译和执行你浏览器中的代码。<p/>\n在一种编程语言的编译器中，词法解析器可能需要许多不同类型的标记。例如：符号，数字，标识符，字符串，操作符等。想知道要从源文件中提取怎样的标记完全取决于编程语言本身。<p/>\r\n12345678910\r\nint main() {    int a;    int b;    a = b = 4;    return a - b;} Scanner production:[Keyword(Int), Id(\"main\"), Symbol(LParen), Symbol(RParen), Symbol(LBrace), Keyword(Int), Id(\"a\"), Symbol(Semicolon), Keyword(Int), Id(\"b\"), Symbol(Semicolon), Id(\"a\"), Operator(Assignment), Id(\"b\"),Operator(Assignment), Integer(4), Symbol(Semicolon), Keyword(Return), Id(\"a\"), Operator(Minus), Id(\"b\"), Symbol(Semicolon), Symbol(RBrace)]\r\nC 语言的样例代码已经进行过词法分析，并且输出了它的标记。<p/>\n2. 解析\n解析器确实是语法解析的核心。解析器提取由词法分析器产生的标记，并尝试判断它们是否符合特定的模式，然后把这些模式与函数调用，变量调用，数学运算之类的表达式关联起来。 解析器逐词地定义编程语言的语法。<p/>\nint a = 3 和 a: int = 3 的区别在于解析器的处理上面。解析器决定了语法的外在形式是怎样的。它确保括号和花括号的左右括号是数量平衡的，每个语句结尾都有一个分号，每个函数都有一个名称。当标记不符合预期的模式时，解析器就会知道标记的顺序不正确。<p/>\n你可以写好几种不同类型的解析器。最常见的解析器之一是从上到下的，递归降解的解析器。递归降解的解析器是用起来最简单也是最容易理解的解析器。我写的所有解析器样例都是基于递归降解的。<p/>\n解析器解析的语法可以使用一种 语法 表示出来。像 EBNF 这样的语法就可以描述一个解析器用于解析简单的数学运算，像是这样 12+3 :<p/>\r\nexpr = additive_expr ;additive_expr = term, ('+' | '-'), term ;term = number ;\r\n简单加法和减法表达式的 EBNF 语法。请记住语法文件并不是解析器，但是它确实是解析器的一种表达形式。你可以围绕上面的语法创建一个解析器。语法文件可以被人使用并且比起直接阅读和理解解析器的代码要简单许多。<p/>\n那种语法的解析器应该是 expr 解析器，因为它直接与所有内容都相关的顶层。唯一有效的输入必须是任意数字，加号或减号，任意数字。expr 需要一个 additive_expr,这主要出现在加法和减法表达式中。additive_expr 首先需要一个 term （一个数字），然后是加号或者减号，最后是另一个 term 。<p/>\n <p/>\n<img id=\"pic\" class=\"\" src=\"http://wx3.sinaimg.cn/large/7cc829d3ly1fwjgj2x9uij20ky0cljs4.jpg\" />\n解析 12+3 产生的样例 AST解析器在解析时产生的树状结构被称为 抽象的语法树，或者称之为 AST。 ast 中包含了所有要进行操作。解析器不会计算这些操作，它只是以正确的顺序来收集其中的标记。<p/>\n我之前补充了我们的词法分析器代码，以便它与我们的语法想匹配，并且可以产生像图表一样的 AST。我用 // BEGIN PARSER // 和 // END PARSER // 的注释标记出了新的解析器代码的开头和结尾。<p/>\nrust playground<p/>\n我们可以再深入一点。假设我们想要支持只有数字没有运算符的输入，或者添加除法和乘法，甚至添加优先级。只要简单地修改一下语法文件，这些都是完全有可能的，任何调整都会直接反映在我们的解析器代码中。<p/>\r\n1234\r\nexpr = additive_expr ;additive_expr = multiplicative_expr, { ('+' | '-'), multiplicative_expr } ;multiplicative_expr = term, { (\"*\" | \"/\"), term } ;term = number ;\r\n新的语法。https://play.rust-lang.org/?gist=1587a5dd6109f70cafe68818a8c1a883&version=nightly&mode=debug&edition=2018<p/>\n<img id=\"pic\" class=\"M_cur_zoom_out\" src=\"http://wx1.sinaimg.cn/large/7cc829d3ly1fwjgj5uo1sg20sp0jv749.gif\" /><p/>\n <p/>\n针对 C 语言语法编写的解析器（又叫做词法分析器）和解析器样例。从字符序列的开始 if(net>0.0)total+=net(1.0+tax/100.0);,扫描器组成了一系列标记，并且对它们进行分类，例如，标识符，保留字，数字，或者运算符。后者的序列由解析器转换成语法树，然后由其他的编译器分阶段进行处理。扫描器和解析器分别处理 C 语法中的规则和与上下文无关的部分。引自：Jochen Burghardt.来源.<p/>\n3. 生成代码\n代码生成器 接收一个 AST ,然后生成相应的代码或者汇编代码。代码生成器必须以递归下降的顺序遍历AST中的所有内容-就像是解析器的工作方式一样-之后生成相应的内容，只不过这里生成的不再是语法树，而是代码了。<p/>\nhttps://godbolt.org/z/K8416_<p/>\n如果打开上面的链接，你就可以看到左侧样例代码产生的汇编代码。汇编代码的第三行和第四行展示了编译器在AST中遇到常量的时候是怎样为这些常量生成相应的代码的。<p/>\nGodbolt Compiler Explorer 是一个很棒的工具，允许你用高级语言编写代码，并查看它产生的汇编代码。你可以有点晕头转向了，想知道产生的是哪种代码，但不要忘记给你的编程语言编译器添加优化选项来看看它到底有多智能。（对于 Rust 是 -O ）<p/>\n如果你对于编译器是在汇编语言中怎样把一个本地变量保存到内存中感兴趣的话，这篇文章 （“代码生成”部分）非常详细地解释了堆栈的相关知识。大多数情况下，当变量不是本地变量的时候，高级编译器会在堆区为变量分配空间，并把它们保存到堆区，而不是栈区。你可以从这个 StackOverflow 的回答上阅读更多关于变量存储的内容。<p/>\n因为汇编是一个完全不同的，而且复杂的主题，因此这里我不会过多地讨论它。我只是想强调代码生成器的重要性和它的作用。此外，代码生成器不仅可以产生汇编代码。Haxe 编译器有一个可以产生 6 种以上不同的编程语言的后端：包括 C++,Java,和 Python。<p/>\n后端指的是编译器的代码生成器或者表达式解析器；因此前端是词法分析器和解析器。同样也有一个中间端，它通常与优化和 IR 有关，这部分会在稍后解释。后端通常与前端无关，后端只关心它接收到的 AST。这意味着可以为几种不同的前端或者语言重用相同的后端。大名鼎鼎的 GNU Compiler Collection 就属于这种情况。<p/>\n我找不到比我的 C 编译器后端更好的代码生成器示例了；你可以在这里查看。<p/>\n在生成汇编代码之后，这些汇编代码会被写入到一个新的汇编文件中 (.s 或 .asm)。然后该文件会被传递给汇编器，汇编器是汇编语言的编译器，它会生成相应的二进制代码。之后这些二进制代码会被写入到一个新的目标文件中 (.o) 。<p/>\n目标文件是机器码，但是它们并不可以被执行。 为了让它们变成可执行文件，目标文件需要被链接到一起。链接器读取通用的机器码，然后使它变为一个可执行文件、共享库或是 静态库。更多关于链接器的知识在这里。<p/>\n链接器是因操作系统而不同的应用程序。随便一个第三方的链接器都应该可以编译你后端产生的目标代码。因此在写编译器的时候不需要创建你自己的链接器。<p/>\n<img id=\"pic\" class=\"aligncenter\" src=\"http://wx2.sinaimg.cn/large/7cc829d3ly1fwjgj8xkx9j20ad0bl74g.jpg\" /><p/>\n编译器可能有 中间表示,或者简称 IR 。IR 主要是为了在优化或者翻译成另一门语言的时候，无损地表示原来的指令。 IR 不再是原来的代码；IR 是为了寻找代码中潜在的优化而进行的无损简化。循环展开 和 向量化 都是利用 IR 完成的。更多关于 IR 相关的优化可以在这个 PDF 中找到。<p/>\n当你理解了编译器的时候，你就可以更有效地使用你的编程语言。或许有一天你会对创建你自己的编程语言感兴趣？我希望这能够帮到你。<p/>\n资源&更深入的阅读资料<p/>\nhttp://craftinginterpreters.com/  指导你编写一个 C 和 Java 的解释器。\nhttps://norasandler.com/2017/11/29/Write-a-Compiler.html  大概是对我来说最有用的 “编写编译器” 的教程了\n我的 C 编译器和科学计算解析器可以在这里和这里找到。\n另一类的解析器，被称作自底向上的解析器,可以在这里找到。来源于：Wesley Norris.\n 13 收藏\n 2 评论\n", "front_img_path": "full/1a3b782e3cc89fc8c0111e8f9fde255c403dc1bd.jpg"},{"title": "反对薪酬保密，一程序员公开了硅谷秘密", "url": "http://blog.jobbole.com/114488/", "create_date": "2018/11/07", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/11/89f21f4108d88a3eabb105a24c0f4fad.jpeg"], "fav_nums": 2, "comment_nums": 0, "vote_nums": 2, "tags": "职场,硅谷,薪酬", "object_id": "b8895923b325bd572e1e15a4aae6a066", "content": "原文出处： Jackie Luo   译文出处：36kr   讨论报酬水平是职场的大忌。因为这样可能会被公司炒鱿鱼。但是硅谷的软件工程师 Jackie Luo 提出，为了报酬的公平性需要提高透明度，因为只有员工才能提供公司所需的技能和经验，公司不能一手遮天。而透明性的提高需要所有员工的积极参与。她的呼吁得到了很多技术员工的响应，这篇文章就是她发现的硅谷员工的薪酬秘密。<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/89f21f4108d88a3eabb105a24c0f4fad.jpeg\" /><p/>\n我是一名软件工程师，有 3 年的工作经验，在 Square 工作，这是一家总部位于旧金山的上市公司。我每年能赚 13 万美元，外加价值 47500 美元的股票，也就是每年 177500 美元。<p/>\n基本工资我没有跟他们谈。不过我的确把 4 年股票赠与从 15 万美元谈到了 19 万美元。我是在 2 月 5 号入职的。那笔赠与目前的价值是 412390.02 美元（煤炭都会波动）。如果按照这个股价行权第一年的话，我真正的报酬是 233097.51 美元。<p/>\n透露这一切令我感到害怕。陌生人和同行看到我挣的钱后认为我拿得太多了。（“开玩笑吧？她都干了什么能拿到这样的工资水平？”）要么他们也许会认为我拿得太少。（“如果她拿这么少的话那一定是工作不怎样。”）将来打算雇我的公司必然都会看到我之前的薪水然后将我未来的薪水锚定之前的水平，从而限制了我换工作时的涨薪水平，或者因为害怕我这个人太贵而将我排除在面试名单之外。<p/>\n既然这样我为什么还要分享这些数字？因为我们需要更多地讨论有关我们拿多少报酬的事情。公平报酬始于更大的透明度。<p/>\n在今天，性别、种族、阶层以及无数其他的身份标识在技术业的系统性偏见中都扮演着自己的角色。2018 年的一份雇佣报告发现，男性在技术业拿到的报酬要高于女性，在同一家公司担任相同角色的情况下 63% 的时候男性都要高于女性。54% 的技术女性报告说自己拿到的钱要比担任同一角色的男性要少。<p/>\n但我们有一个有关报酬的真诚对话可以缩小差距的乐观理由。意识到存在报酬差异的人当中有 66% 者只是在跟同事聊到报酬的情况时才知道这件事情的存在。<p/>\n硅谷真正的财富是通过股权产生的。<p/>\n通常当大家讨论报酬——该接受什么，该拒绝什么，如何协商的时候，建议总是“知道你的价值。”但怎么才能知道？对于大多数员工来说，报酬被锁在一个黑箱里面，而处在职场最上层阶梯的人掌握着所有的信息。只需要把他们支付的报酬数据给收集众多客户相关数字的机构，他们通常能获取到有关市场给特定角色开出的平均薪酬的信息，并且还能拿到聚合的分析。这种信息不对称造成了权力的失衡，只有一端，也即是更有权力的一方能够做出关于报酬支付的知情决定。<p/>\n典型的场景是这样的：你接受了一家公司的面试，在电话筛选上花费了几个小时，并且完成了现场面试。你表现很好。准备拿到 offer。当然了，招聘人员会打电话给你，问你想开出的报酬是多少。<p/>\n这个时候你就开始恐慌了。如果你报出的数字太高怎么办？你不希望公司因此不给你这个 offer 了——你需要一份工作并且已经走完了整个面试流程，所以你不希望破坏这个机会。可如果你报得太低呢？你不希望贱卖自己，而你现在协商的报酬，在你拿到那份工作之前，将决定你未来几年的报酬。拥有类似资格的其他人拿到的报酬如何？你完全就是在黑暗中乱开枪。<p/>\n纠正这种不平衡的努力的确有（比如 Glassdoor、Comparably 以及 Levels 这类的服务），但是聚合报酬数据的匿名平台并不能替代我们的真正需要：一种将公开讨论报酬正常化的文化。<p/>\n因为有感于作家 Carina Hsieh 以及#talkpay 运动，在三八国际妇女节那天我发了一条推特。它向技术界的男性发出来公开邀请，请他们与我分享自己的工资好让我可以匿名地发布出去。<p/>\n即便在今天，相对于自己的同行，女性以及被忽视的少数族裔还是频繁地拿到过低的报酬，而他们甚至都没有意识到这一点，因为他们没有太多的比较点。我以为会有少数人愿意直接给我发信息。<p/>\n迄今为止，已经有 50 万人看到了那条推特。成千上万的人已经直接私信我。大家发送了很多的工资信息给我，多到我仍然还没有把我收到的数据公布完，差得还远呢。<p/>\n我们保守薪水的秘密是因为我们害怕会因为分享这些数据而受到惩罚。<p/>\n全世界各种角色的技术员工都把自己的薪水数据发给了我，但因为我是一名湾区的软件工程师，所以我最大的数据集来自这里。那么，湾区的男性软件工程师拿到的薪水有多少呢？以下就是我对这场讨论的贡献：<p/>\n软件工程师开发了你每天使用的各种网站和 app。湾区的软件工程角色往往从 6 位数起步：120000 美元是 4 年大学生涯刚刚毕业的计算机科学专业拿到的典型的基本工资，这是由大型上市技术公司设定的标准。新毕业生去到小一点的初创企业能拿到的薪水可能会比这要低一点，但通常哪怕是初创企业也会支付 6 位数的工资。早期阶段初创企业给的钱最少，而真正令人安逸之心的薪酬数字只会出现在最大型的“初创企业”那里，比如 Uber、Lyft 以及 AirBnb。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/85828ac727177d735db699e2b4d452db.jpeg\" /><p/>\n加州技术业的工资概况<p/>\n初创企业和上市公司的一大差异化因素是股权。初创企业的股权到头来往往一文不值，但是上市公司的股权则代表着报仇的一大部分。基本的工资很少会涨到 40 万美元以上——实际上我还没有见过——但股票则可以令工程师的整体薪酬翻番或者甚至翻 3 番。而且跟工资不一样的是，股票在价值上可以飙涨，如果你是目睹了股价坐火箭的 Google 员工或者你是一家初创企业的第一名工程师而那家公司后来被高价收购的话这一点是真的。硅谷的真正财富是通过股权产生的。<p/>\n尤其是 Facebook 和 Google 对于迫切需要的人才开出的薪酬是最丰厚的。尽管市场竞争激烈，但如果你听说一位工程师的总报酬达到 75 万美元的话，那一定是非常不典型的。很有可能他们是在 Facebook 或者 Google 工作，而且是全世界稍有的几个具备行业经验与专业知识能从事相关工作的人。他们就是戴着“金手套”的人：理论上他们可以在任何地方找到一份工作，但是他们无法离开（或者至少他们认为自己不能离开），因为没有一个竞争对手能配得上他们的报酬。<p/>\n大家通常会担心自己会因为分享自己的收入而被炒鱿鱼。<p/>\n如果让我来猜测湾区软件工程师的平均总薪酬（基本工资、股权加奖金）的话，我想大概在 150000 美元到 200000 美元之间。这个估计得到了我从 Twitter 收到的数据的支持，其中位数是 183750 美元。根据 Glassdoor，工程师的平均基本工资是 137000 美元，奖金为 11000 美元，这还没有包括股票奖励。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/6476651d0ef1c25a8d98cec530d796e5.jpeg\" /><p/>\n美国技术工资概况<p/>\n这个数字已经不少了，所以这个话题的敏感性也不足为奇。尽管如此，我对披露报酬数字给我的人的害怕程度仍然准备不足，很多人一而再再而三地向我强调这一信息只能匿名发出。好几位甚至还要求自己的工作城市和职位细节信息要尽可能写得含糊，以防自己正好被对上号。大家往往会担心自己因为分享收入信息而被炒鱿鱼。<p/>\n对待透明性的态度是文化已经受到严重破坏的迹象。我们如何得到报酬塑造了我们得日常生活的一切：我们可以在哪里生活，可以做什么，可以有多大的自由。在收入方面更清楚了解到我们所处的位置完全是我们的利益所在，但是我们还是要保守这一信息秘密，因为我们害怕会由于分享而受到惩罚。在一个本该极其重视透明性的行业里，我们在最需要透明性的地方却恰恰存在不足。<p/>\n让我们改变这一权力格局吧。跟你的同事讨论，跟你的业界同行讨论，跟你的朋友讨论自己拿到了多少报酬。跟你团队里面的女性和少数族裔讨论。不要仅仅讨论工资，要讨论整体报酬包括股权和奖金在内。讨论相关过程，比如你的报酬如何改变了自己的职业生涯路径以及协商报酬最后对拿到 offer 的影响会怎样。邀请他们也做同样的事情。<p/>\n大家的共同看法是公司在就业市场拥有一切权力因为就业机会是它们赋予的。但员工才是提供公司所需技能和经验的人。知道自己应该拿多少让你在一个通常不钟爱于你的体系里面多少有一点权力——而你应该运用这种权力。<p/>\n", "front_img_path": "full/8d3f910003336d6f3b381d88a58bc579a95eba56.jpg"},{"title": "Python 数据科学入门", "url": "http://blog.jobbole.com/114493/", "create_date": "2018/11/08", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2014/07/6da94dec8f6f96417f14c8291e634580.png"], "fav_nums": 0, "comment_nums": 1, "vote_nums": 1, "tags": "Python,,数据科学", "object_id": "ead0ffea91eda8270471bda2c435344a", "content": "原文出处： Payal Singh   译文出处：Linux中国/MjSeven   不需要昂贵的工具即可领略数据科学的力量，从这些开源工具起步即可。<p/>\n无论你是一个具有数学或计算机科学背景的资深数据科学爱好者，还是一个其它领域的专家，数据科学提供的可能性都在你力所能及的范围内，而且你不需要昂贵的，高度专业化的企业级软件。本文中讨论的开源工具就是你入门时所需的全部内容。<p/>\nPython，其机器学习和数据科学库（pandas、 Keras、 TensorFlow、 scikit-learn、 SciPy、 NumPy 等），以及大量可视化库（Matplotlib、pyplot、 Plotly 等）对于初学者和专家来说都是优秀的自由及开源软件工具。它们易于学习，很受欢迎且受到社区支持，并拥有为数据科学而开发的最新技术和算法。它们是你在开始学习时可以获得的最佳工具集之一。<p/>\n许多 Python 库都是建立在彼此之上的（称为依赖项），其基础是 NumPy 库。NumPy 专门为数据科学设计，经常被用于在其 ndarray 数据类型中存储数据集的相关部分。ndarray 是一种方便的数据类型，用于将关系表中的记录存储为 cvs 文件或其它任何格式，反之亦然。将 scikit 函数应用于多维数组时，它特别方便。SQL 非常适合查询数据库，但是对于执行复杂和资源密集型的数据科学操作，在 ndarray 中存储数据可以提高效率和速度（但请确保在处理大量数据集时有足够的 RAM）。当你使用 pandas 进行知识提取和分析时，pandas 中的 DataFrame 数据类型和 NumPy 中的 ndarray 之间的无缝转换分别为提取和计算密集型操作创建了一个强大的组合。<p/>\n作为快速演示，让我们启动 Python shell 并在 pandas DataFrame 变量中加载来自巴尔的摩的犯罪统计数据的开放数据集，并查看加载的一部分 DataFrame：<p/>\r\n>>>  import pandas as pd>>>  crime_stats = pd.read_csv('BPD_Arrests.csv')>>>  crime_stats.head()\r\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/0361eaa686cb78f2be6148ec6b911a3e.jpg\" alt=\"\" /><p/>\n我们现在可以在这个 pandas DataFrame 上执行大多数查询，就像我们可以在数据库中使用 SQL 一样。例如，要获取 Description 属性的所有唯一值，SQL 查询是：<p/>\r\n$ SELECT unique(“Description”) from crime_stats;\r\n利用 pandas DataFrame 编写相同的查询如下所示：<p/>\r\n12345\r\n>>>  crime_stats['Description'].unique()['COMMON   ASSAULT'   'LARCENY'   'ROBBERY   - STREET'   'AGG.   ASSAULT''LARCENY   FROM   AUTO'   'HOMICIDE'   'BURGLARY'   'AUTO   THEFT''ROBBERY   - RESIDENCE'   'ROBBERY   - COMMERCIAL'   'ROBBERY   - CARJACKING''ASSAULT   BY  THREAT'   'SHOOTING'   'RAPE'   'ARSON']\r\n它返回的是一个 NumPy 数组（ndarray 类型）：<p/>\r\n>>>  type(crime_stats['Description'].unique())<class 'numpy.ndarray'> \r\n接下来让我们将这些数据输入神经网络，看看它能多准确地预测使用的武器类型，给出的数据包括犯罪事件，犯罪类型以及发生的地点：<p/>\r\n12345678910\r\n>>>  from   sklearn.neural_network   import   MLPClassifier>>>  import   numpy   as np>>>>>>  prediction   =  crime_stats[[‘Weapon’]]>>>  predictors   =  crime_stats['CrimeTime',   ‘CrimeCode’,   ‘Neighborhood’]>>>>>>  nn_model   =  MLPClassifier(solver='lbfgs',   alpha=1e-5,   hidden_layer_sizes=(5,2),   random_state=1)>>>>>>predict_weapon   =  nn_model.fit(prediction,   predictors)\r\n现在学习模型准备就绪，我们可以执行一些测试来确定其质量和可靠性。对于初学者，让我们输入一个训练集数据（用于训练模型的原始数据集的一部分，不包括在创建模型中）：<p/>\r\n>>>  predict_weapon.predict(training_set_weapons)array([4,   4,   4,   ..., 0,   4,   4])\r\n如你所见，它返回一个列表，每个数字预测训练集中每个记录的武器。我们之所以看到的是数字而不是武器名称，是因为大多数分类算法都是用数字优化的。对于分类数据，有一些技术可以将属性转换为数字表示。在这种情况下，使用的技术是标签编码，使用 sklearn 预处理库中的 LabelEncoder 函数：preprocessing.LabelEncoder()。它能够对一个数据和其对应的数值表示来进行变换和逆变换。在这个例子中，我们可以使用 LabelEncoder() 的 inverse_transform 函数来查看武器 0 和 4 是什么:<p/>\r\n>>>  preprocessing.LabelEncoder().inverse_transform(encoded_weapons)array(['HANDS',   'FIREARM',   'HANDS',   ..., 'FIREARM',   'FIREARM',   'FIREARM']\r\n这很有趣，但为了了解这个模型的准确程度，我们将几个分数计算为百分比：<p/>\r\n>>>  nn_model.score(X,   y)0.81999999999999995 \r\n这表明我们的神经网络模型准确度约为 82%。这个结果似乎令人印象深刻，但用于不同的犯罪数据集时，检查其有效性非常重要。还有其它测试来做这个，如相关性、混淆、矩阵等。尽管我们的模型有很高的准确率，但它对于一般犯罪数据集并不是非常有用，因为这个特定数据集具有不成比例的行数，其列出 FIREARM 作为使用的武器。除非重新训练，否则我们的分类器最有可能预测 FIREARM，即使输入数据集有不同的分布。<p/>\n在对数据进行分类之前清洗数据并删除异常值和畸形数据非常重要。预处理越好，我们的见解准确性就越高。此外，为模型或分类器提供过多数据（通常超过 90%）以获得更高的准确度是一个坏主意，因为它看起来准确但由于过度拟合而无效。<p/>\nJupyter notebooks 相对于命令行来说是一个很好的交互式替代品。虽然 CLI 对于大多数事情都很好，但是当你想要运行代码片段以生成可视化时，Jupyter 会很出色。它比终端更好地格式化数据。<p/>\n这篇文章 列出了一些最好的机器学习免费资源，但是还有很多其它的指导和教程。根据你的兴趣和爱好，你还会发现许多开放数据集可供使用。作为起点，由 Kaggle 维护的数据集，以及在州政府网站上提供的数据集是极好的资源。<p/>\n 1 评论\n", "front_img_path": "full/c1733c5700c6954fd681752d045e347fb5982322.jpg"},{"title": "SQL优化指南", "url": "http://blog.jobbole.com/114482/", "create_date": "2018/11/06", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/11/b47d2553927cc1dd89135026466da892.png"], "fav_nums": 4, "comment_nums": 0, "vote_nums": 3, "tags": "IT技术,SQL", "object_id": "525873ae47480e5115c1da9dee9c61ac", "content": "以我刚安装的mysql5.7为例 查询结果是这样子的：<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/b47d2553927cc1dd89135026466da892.png\" alt=\"\" /><p/>\nslow_launch_time：表示如果建立线程花费了比这个值更长的时间,slow_launch_threads 计数器将增加\nslow_query_log：是否开启慢查询日志 ON开启，OFF关闭 默认没有开启\nslow_query_log_file：日志保存路径<p/>\nSHOW VARIABLES LIKE 'long%'\r\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/7efab1464f0d01da6b196aaf75f30cd0.png\" alt=\"\" /><p/>\nlong_query_time：达到多少秒的sql就记录日志<p/>\n客户端可以用set设置变量的方式让慢查询开启，但是个人不推荐，因为真实操作起来会有一些问题，比如说，重启MySQL后就失效了，或者是开启了慢查询，我又去改变量值，它就不生效了。<p/>\n编辑MySQL的配置文件：<p/>\nvim /etc/my.cnf\r\n加入如下三行：<p/>\n　　slow_query_log=ON　　slow_query_log_file=/var/lib/mysql/localhost-centos-slow.log　　long_query_time=3\r\n我这里设置的是3秒<p/>\n重启MySQL<p/>\nsystemctl restart mysqld;\r\n服务器开一个监控：<p/>\ntail -f /var/lib/mysql/localhost-centos-slow.log\r\n客户端走一条SQL：<p/>\nSELECT SLEEP(3)\r\n此时发现sql已经被记录到日志里了。（有时候不一定，我看到很多博客讲的是超过指定秒数，但我实验得出的结果是达到指定秒数）<p/>\nEXPLAIN 点对点分析你\nexplain是一个神奇的命令，可以查看sql的具体的执行计划。<p/>\n以一条联查sql为例：<p/>\n1234\r\nSELECT a.id,a.cn_name,a.role_id,r.nameFROM tb_usr_admins aINNER JOIN tb_base_roles r ON r.id=a.role_idWHERE a.cn_name=\"接单人员\"\r\n查询结果是：<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/3524bf8887485fb2352526656ee7d865.png\" alt=\"\" /><p/>\n加上explain命令来执行：<p/>\n12345\r\nEXPLAINSELECT a.id,a.cn_name,a.role_id,r.nameFROM tb_usr_admins aINNER JOIN tb_base_roles r ON r.id=a.role_idWHERE a.cn_name=\"接单人员\"\r\n查询结果是：<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/39a293f947f0416994a434564c9440ad.png\" alt=\"\" /><p/>\n这就是这条SQL的执行计划，下面来说明一下这个执行计划怎么看<p/>\nid：代表优先级  id值越大，越先执行，id值相同，从上往下执行。（比如示例的这条sql的执行计划，就是先执行第一行，再执行第二行）<p/>\nselect_type：表示select类型 取值如下<p/>\nsimple 简单表 即不使用表连接或者子查询\nprimary 包含union或者子查询的主查询 即外层的查询\nunion UNION中的第二个或者后面的查询语句\nsubquery 一般子查询中的子查询被标记为subquery，也就是位于select列表中的查询\nderived 派生表 该临时表是从子查询派生出来的\n等等<p/>\ntype：表示MySQL在表中查找数据的方式，或者叫访问类型，以下对于type取值的说明 从上往下性能由最差到最好<p/>\nall:全表扫描，MySQL遍历全表来找到匹配的行\nindex：索引全扫描，MySQL遍历挣个索引来查询匹配的行\nrange：索引范围扫描，常见于<、<=、>、>=、between等操作符\nref：使用非唯一索引或唯一索引的前缀扫描，返回匹配的单行数据\neq_ref：类似ref，区别就在于使用的索引是唯一索引，简单来说，就是多表连接中使用primary key或者unique index作为关联条件。\nconst/system：单表中最多有一个匹配行，查询起来非常迅速，常见于根据primary key或者唯一索引unique index进行的单表查询\nnull：mysql不用访问表或者索引，直接就能够得到查询的结果，例如select 1+2 as result。<p/>\npossible_keys：表示查询时可能使用的索引<p/>\nkey：表示实际使用的索引<p/>\nkey_len：使用到索引字段的长度<p/>\nrows：扫描数量<p/>\nExtra：执行情况的说明和描述，包含不适合在其他列中显示但是对执行计划非常重要的额外信息，常用取值如下：<p/>\nUsing index：直接访问索引就取到了数据，高性能的表现。\nUsing where：直接在主键索引上过滤数据，必带where子句，而且用不上索引\nUsing index condition：先条件过滤索引，再查数据，\nUsing filesort：使用了外部文件排序 只要见到这个 就要优化掉\nUsing temporary：创建了临时表来处理查询 只要见到这个 也要尽量优化掉<p/>\n优化争议无数的count()\n统计列与统计行？\nCOUNT()是一个特殊的函数，有两种不同的作用，它可以统计某个列值的数量，也可以统计行数。<p/>\n在统计列值的时候要求列值是非空的，也就是不统计null。<p/>\n当我们统计行的时候，常见的是COUNT(*)，这种情况下，通配符*并不会像我们猜想的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数<p/>\n解密MyiSAM的‘快’\n这是一个容易产生误解的事情：MyiSAM的count()函数总是非常快。<p/>\n不过它是有前提条件的，条件是没有任何where条件的count(*)才非常快，因为此时无须实际的去计算表的行数，mysql可以利用存储引擎的特性直接获得这个值，如果mysql知道某列不可能有null值，那么mysql内部会将count(列)表达式优化为count(*)。<p/>\n当统计带有where条件的查询，那么mysql的count()和其他存储引擎就没有什么不同了。<p/>\nCOUNT(1)、COUNT(*)、COUNT(列)\n（先提前申明，本人是在innodb库里做的实验。）<p/>\n1.count(1)和count(*)直接就是统计主键，他们两个的效率是一样的。如果删除主键，他们都走全表扫描。<p/>\n2.如果count(列)中的字段是索引的话，count(列)和count(*)一样快，否则count(列)走全表扫描。<p/>\n优化order by 语句\nMySQL的排序方式\n优化order by语句就不得不了解mysql的排序方式。<p/>\n1.第一种通过有序索引返回数据，这种方式的extra显示为Using Index,不需要额外的排序，操作效率较高。<p/>\n2.第二种是对返回的数据进行排序，也就是通常看到的Using filesort，filesort是通过相应的排序算法，将数据放在sort_buffer_size系统变量设置的内存排序区中进行排序，如果内存装载不下，它就会将磁盘上的数据进行分块，再对各个数据块进行排序，然后将各个块合并成有序的结果集。<p/>\nfilesort的优化\n了解了MySQL排序的方式，优化目标就清晰了：尽量减少额外的排序，通过索引直接返回有序数据。where条件和order by使用相同的索引。<p/>\n1.创建合适的索引减少filesort的出现。<p/>\n2.查询时尽量只使用必要的字段，select 具体字段的名称，而不是select * 选择所有字段，这样可以减少排序区的使用，提高SQL性能。<p/>\n优化group by 语句\n为什么order by后面不能跟group by ?\n事实上，MySQL在所有的group by 后面隐式的加了order by ，也就是说group by语句的结果会默认进行排序。<p/>\n如果你要在order by后面加group by ，那结果执行的SQL是不是这样：select * from tb order by  group by  order by  ？ 这不是搞笑吗？<p/>\n既然知道问题了，那么就容易优化了，如果查询包括group by但又不关心结果集的顺序，而这种默认排序又导致了需要文件排序，则可以指定order by null 禁止排序。<p/>\n例如：<p/>\nselect * from tb group by name order by null;\r\n优化limit 分页\n一个非常常见又非常头痛的场景：‘limit 1000,20’。<p/>\n这时MySQL需要查询1020条记录然后只返回最后20条，前面的1000条都将被抛弃，这样的代价非常高。如果所有页面的访问频率都相同，那么这样的查询平均需要访问半个表的数据。<p/>\n第一种思路 在索引上分页\n在索引上完成分页操作，最后根据主键关联回原表查询所需要的其他列的内容。<p/>\n例如：<p/>\nSELECT * FROM tb_user LIMIT 1000,10\r\n可以优化成这样：<p/>\nSELECT * FROM tb_user u INNER JOIN (SELECT id FROM tb_user LIMIT 1000,10) AS b ON b.id=u.id\r\n第二种思路 将limit转换成位置查询\n这种思路需要加一个参数来辅助，标记分页的开始位置：<p/>\nSELECT * FROM tb_user WHERE id > 1000 LIMIT 10\r\n优化子查询\n子查询，也就是查询中有查询，常见的是where后面跟一个括号里面又是一条查询sql<p/>\n尽可能的使用join关联查询来代替子查询。<p/>\n当然 这不是绝对的，比如某些非常简单的子查询就比关联查询效率高，事实效果如何还要看执行计划。<p/>\n只能说大部分的子查询都可以优化成Join关联查询。<p/>\n改变执行计划\n提高索引优先级\nuse index 可以让MySQL去参考指定的索引，但是无法强制MySQL去使用这个索引，当MySQL觉得这个索引效率太差，它宁愿去走全表扫描。。。<p/>\nSELECT * FROM tb_user USE INDEX (user_name)\r\n注意：必须是索引，不能是普通字段，（亲测主键也不行）。<p/>\nignore index 可以让MySQL忽略一个索引<p/>\nSELECT * FROM tb_user IGNORE INDEX (user_name) WHERE user_name=\"张学友\"\r\n强制使用索引\nforce index 使用了force index 之后 尽管效率非常低，MySQL也会照你的话去执行<p/>\nSELECT * FROM tb_user FORCE INDEX (user_name) WHERE user_name=\"张学友\"\r\n查看执行计划时建议依次观察以下几个要点：<p/>\n1.SQL内部的执行顺序。\n2.查看select的查询类型。\n3.实际有没有使用索引。\n4.Extra描述信息<p/>\nPS:一定要养成查看执行计划的习惯，这个习惯非常重要。<p/>\n 4 收藏\n", "front_img_path": "full/5e1320055e1d116508c18b94fc1b894a60b8b67d.jpg"},{"title": "awk 入门教程", "url": "http://blog.jobbole.com/114490/", "create_date": "2018/11/07", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2015/02/58e81f3ff36189eef0d754b03017307d.png"], "fav_nums": 4, "comment_nums": 0, "vote_nums": 2, "tags": "IT技术,awk,Linux", "object_id": "02ee80b49bc0077872a5c8909cf91ee3", "content": "原文出处： 阮一峰   awk是处理文本文件的一个应用程序，几乎所有 Linux 系统都自带这个程序。<p/>\n它依次处理文件的每一行，并读取里面的每一个字段。对于日志、CSV 那样的每行格式相同的文本文件，awk可能是最方便的工具。<p/>\n<img class=\"aligncenter\" title=\"\" src=\"https://www.wangbase.com/blogimg/asset/201811/bg2018110702.jpg\" alt=\"\" /><p/>\nawk其实不仅仅是工具软件，还是一种编程语言。不过，本文只介绍它的命令行用法，对于大多数场合，应该足够用了。<p/>\n一、基本用法\nawk的基本用法就是下面的形式。<p/>\r\n12345\r\n# 格式$ awk 动作 文件名 # 示例$ awk '{print $0}' demo.txt\r\n上面示例中，demo.txt是awk所要处理的文本文件。前面单引号内部有一个大括号，里面就是每一行的处理动作print $0。其中，print是打印命令，$0代表当前行，因此上面命令的执行结果，就是把每一行原样打印出来。<p/>\nawk也可以处理标准输入（stdin）。<p/>\r\n$ echo 'this is a test' | awk '{print $0}'this is a test\r\n上面代码中，print $0就是把标准输入this is a test，重新打印了一遍。<p/>\nawk会根据空格和制表符，将每一行分成若干字段，依次用$1、$2、$3代表第一个字段、第二个字段、第三个字段等等。<p/>\r\n$ echo 'this is a test' | awk '{print $3}'a\r\n上面代码中，$3代表this is a test的第三个字段a。<p/>\n下面，为了便于举例，我们把/etc/passwd文件保存成demo.txt。<p/>\r\n12345\r\nroot:x:0:0:root:/root:/usr/bin/zshdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologinsync:x:4:65534:sync:/bin:/bin/sync\r\n这个文件的字段分隔符是冒号（:），所以要用-F参数指定分隔符为冒号。然后，才能提取到它的第一个字段。<p/>\r\n123456\r\n$ awk -F ':' '{ print $1 }' demo.txtrootdaemonbinsyssync\r\n除了$ + 数字表示某个字段，awk还提供其他一些变量。<p/>\n变量NF表示当前行有多少个字段，因此$NF就代表最后一个字段。<p/>\r\n$ echo 'this is a test' | awk '{print $NF}'test\r\n$(NF-1)代表倒数第二个字段。<p/>\r\n123456\r\n$ awk -F ':' '{print $1, $(NF-1)}' demo.txtroot /rootdaemon /usr/sbinbin /binsys /devsync /bin\r\n上面代码中，print命令里面的逗号，表示输出的时候，两个部分之间使用空格分隔。<p/>\n变量NR表示当前处理的是第几行。<p/>\r\n123456\r\n$ awk -F ':' '{print NR \") \" $1}' demo.txt1) root2) daemon3) bin4) sys5) sync\r\n上面代码中，print命令里面，如果原样输出字符，要放在双引号里面。<p/>\nawk的其他内置变量如下。<p/>\nFILENAME：当前文件名\nFS：字段分隔符，默认是空格和制表符。\nRS：行分隔符，用于分割每一行，默认是换行符。\nOFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。\nORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。\nOFMT：数字输出的格式，默认为％.6g。\nawk还提供了一些内置函数，方便对原始数据的处理。<p/>\n函数toupper()用于将字符转为大写。<p/>\r\n123456\r\n$ awk -F ':' '{ print toupper($1) }' demo.txtROOTDAEMONBINSYSSYNC\r\n上面代码中，第一个字段输出时都变成了大写。<p/>\n其他常用函数如下。<p/>\ntolower()：字符转为小写。\nlength()：返回字符串长度。\nsubstr()：返回子字符串。\nsin()：正弦。\ncos()：余弦。\nsqrt()：平方根。\nrand()：随机数。\nawk内置函数的完整列表，可以查看手册。<p/>\nawk允许指定输出条件，只输出符合条件的行。<p/>\n输出条件要写在动作的前面。<p/>\r\n$ awk '条件 动作' 文件名\r\n请看下面的例子。<p/>\r\n12345\r\n$ awk -F ':' '/usr/ {print $1}' demo.txtrootdaemonbinsys\r\n上面代码中，print命令前面是一个正则表达式，只输出包含usr的行。<p/>\n下面的例子只输出奇数行，以及输出第三行以后的行。<p/>\r\n12345678910\r\n# 输出奇数行$ awk -F ':' 'NR % 2 == 1 {print $1}' demo.txtrootbinsync # 输出第三行以后的行$ awk -F ':' 'NR >3 {print $1}' demo.txtsyssync\r\n下面的例子输出第一个字段等于指定值的行。<p/>\r\n123456\r\n$ awk -F ':' '$1 == \"root\" {print $1}' demo.txtroot $ awk -F ':' '$1 == \"root\" || $1 == \"bin\" {print $1}' demo.txtrootbin\r\n五、if 语句\nawk提供了if结构，用于编写复杂的条件。<p/>\r\n1234\r\n$ awk -F ':' '{if ($1 > \"m\") print $1}' demo.txtrootsyssync\r\n上面代码输出第一个字段的第一个字符大于m的行。<p/>\nif结构还可以指定else部分。<p/>\r\n123456\r\n$ awk -F ':' '{if ($1 > \"m\") print $1; else print \"---\"}' demo.txtroot------syssync\r\n六、参考链接\nAn Awk tutorial by Example, Greg Grothaus\n30 Examples for Awk Command in Text Processing, Mokhtar Ebrahim\n（完）<p/>\n 4 收藏\n", "front_img_path": "full/c097748ffe50bc014febec4bd6df583967165370.jpg"},{"title": "在 Linux 命令行中使用 tcpdump 抓包", "url": "http://blog.jobbole.com/114505/", "create_date": "2018/11/13", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2015/09/7fcd641cb5f24d14a62f52453c975c3a.jpg"], "fav_nums": 4, "comment_nums": 2, "vote_nums": 3, "tags": "IT技术,,Linux", "object_id": "105183b2fd20dec85aab802e7e7a8eaf", "content": "原文出处： Ricardo Gerardi   译文出处：Linux中国/jrg   tcpdump 是一款灵活、功能强大的抓包工具，能有效地帮助排查网络故障问题。<p/>\n以我作为管理员的经验，在网络连接中经常遇到十分难以排查的故障问题。对于这类情况，tcpdump 便能派上用场。<p/>\ntcpdump 是一个命令行实用工具，允许你抓取和分析经过系统的流量数据包。它通常被用作于网络故障分析工具以及安全工具。<p/>\ntcpdump 是一款强大的工具，支持多种选项和过滤规则，适用场景十分广泛。由于它是命令行工具，因此适用于在远程服务器或者没有图形界面的设备中收集数据包以便于事后分析。它可以在后台启动，也可以用 cron 等定时工具创建定时任务启用它。<p/>\n本文中，我们将讨论 tcpdump 最常用的一些功能。<p/>\n1、在 Linux 中安装 tcpdump\ntcpdump 支持多种 Linux 发行版，所以你的系统中很有可能已经安装了它。用下面的命令检查一下是否已经安装了 tcpdump：<p/>\r\n$ which tcpdump/usr/sbin/tcpdump\r\n如果还没有安装 tcpdump，你可以用软件包管理器安装它。 例如，在 CentOS 或者 Red Hat Enterprise 系统中，用如下命令安装 tcpdump：<p/>\r\n$ sudo yum install -y tcpdump\r\ntcpdump 依赖于 libpcap，该库文件用于捕获网络数据包。如果该库文件也没有安装，系统会根据依赖关系自动安装它。<p/>\n现在你可以开始抓包了。<p/>\n2、用 tcpdump 抓包\n使用 tcpdump 抓包，需要管理员权限，因此下面的示例中绝大多数命令都是以 sudo 开头。<p/>\n首先，先用 tcpdump -D 命令列出可以抓包的网络接口：<p/>\r\n123456\r\n$ sudo tcpdump -D1.eth02.virbr03.eth14.any (Pseudo-device that captures on all interfaces)5.lo [Loopback]\r\n如上所示，可以看到我的机器中所有可以抓包的网络接口。其中特殊接口 any 可用于抓取所有活动的网络接口的数据包。<p/>\n我们就用如下命令先对 any 接口进行抓包：<p/>\r\n12345678910111213141516171819202122232425\r\n$ sudo tcpdump -i anytcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes09:56:18.293641 IP rhel75.localdomain.ssh > 192.168.64.1.56322: Flags [P.], seq 3770820720:3770820916, ack 3503648727, win 309, options [nop,nop,TS val 76577898 ecr 510770929], length 19609:56:18.293794 IP 192.168.64.1.56322 > rhel75.localdomain.ssh: Flags [.], ack 196, win 391, options [nop,nop,TS val 510771017 ecr 76577898], length 009:56:18.295058 IP rhel75.59883 > gateway.domain: 2486+ PTR? 1.64.168.192.in-addr.arpa. (43)09:56:18.310225 IP gateway.domain > rhel75.59883: 2486 NXDomain* 0/1/0 (102)09:56:18.312482 IP rhel75.49685 > gateway.domain: 34242+ PTR? 28.64.168.192.in-addr.arpa. (44)09:56:18.322425 IP gateway.domain > rhel75.49685: 34242 NXDomain* 0/1/0 (103)09:56:18.323164 IP rhel75.56631 > gateway.domain: 29904+ PTR? 1.122.168.192.in-addr.arpa. (44)09:56:18.323342 IP rhel75.localdomain.ssh > 192.168.64.1.56322: Flags [P.], seq 196:584, ack 1, win 309, options [nop,nop,TS val 76577928 ecr 510771017], length 38809:56:18.323563 IP 192.168.64.1.56322 > rhel75.localdomain.ssh: Flags [.], ack 584, win 411, options [nop,nop,TS val 510771047 ecr 76577928], length 009:56:18.335569 IP gateway.domain > rhel75.56631: 29904 NXDomain* 0/1/0 (103)09:56:18.336429 IP rhel75.44007 > gateway.domain: 61677+ PTR? 98.122.168.192.in-addr.arpa. (45)09:56:18.336655 IP gateway.domain > rhel75.44007: 61677* 1/0/0 PTR rhel75. (65)09:56:18.337177 IP rhel75.localdomain.ssh > 192.168.64.1.56322: Flags [P.], seq 584:1644, ack 1, win 309, options [nop,nop,TS val 76577942 ecr 510771047], length 1060 ---- SKIPPING LONG OUTPUT ----- 09:56:19.342939 IP 192.168.64.1.56322 > rhel75.localdomain.ssh: Flags [.], ack 1752016, win 1444, options [nop,nop,TS val 510772067 ecr 76578948], length 0^C9003 packets captured9010 packets received by filter7 packets dropped by kernel$\r\ntcpdump 会持续抓包直到收到中断信号。你可以按 Ctrl+C 来停止抓包。正如上面示例所示，tcpdump 抓取了超过 9000 个数据包。在这个示例中，由于我是通过 ssh 连接到服务器，所以 tcpdump 也捕获了所有这类数据包。-c 选项可以用于限制 tcpdump 抓包的数量：<p/>\r\n123456789101112\r\n$ sudo tcpdump -i any -c 5tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes11:21:30.242740 IP rhel75.localdomain.ssh > 192.168.64.1.56322: Flags [P.], seq 3772575680:3772575876, ack 3503651743, win 309, options [nop,nop,TS val 81689848 ecr 515883153], length 19611:21:30.242906 IP 192.168.64.1.56322 > rhel75.localdomain.ssh: Flags [.], ack 196, win 1443, options [nop,nop,TS val 515883235 ecr 81689848], length 011:21:30.244442 IP rhel75.43634 > gateway.domain: 57680+ PTR? 1.64.168.192.in-addr.arpa. (43)11:21:30.244829 IP gateway.domain > rhel75.43634: 57680 NXDomain 0/0/0 (43)11:21:30.247048 IP rhel75.33696 > gateway.domain: 37429+ PTR? 28.64.168.192.in-addr.arpa. (44)5 packets captured12 packets received by filter0 packets dropped by kernel$\r\n如上所示，tcpdump 在抓取 5 个数据包后自动停止了抓包。这在有些场景中十分有用 —— 比如你只需要抓取少量的数据包用于分析。当我们需要使用过滤规则抓取特定的数据包（如下所示）时，-c 的作用就十分突出了。<p/>\n在上面示例中，tcpdump 默认是将 IP 地址和端口号解析为对应的接口名以及服务协议名称。而通常在网络故障排查中，使用 IP 地址和端口号更便于分析问题；用 -n 选项显示 IP 地址，-nn 选项显示端口号：<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes23:56:24.292206 IP 192.168.64.28.22 > 192.168.64.1.35110: Flags [P.], seq 166198580:166198776, ack 2414541257, win 309, options [nop,nop,TS val 615664 ecr 540031155], length 19623:56:24.292357 IP 192.168.64.1.35110 > 192.168.64.28.22: Flags [.], ack 196, win 1377, options [nop,nop,TS val 540031229 ecr 615664], length 023:56:24.292570 IP 192.168.64.28.22 > 192.168.64.1.35110: Flags [P.], seq 196:568, ack 1, win 309, options [nop,nop,TS val 615664 ecr 540031229], length 37223:56:24.292655 IP 192.168.64.1.35110 > 192.168.64.28.22: Flags [.], ack 568, win 1400, options [nop,nop,TS val 540031229 ecr 615664], length 023:56:24.292752 IP 192.168.64.28.22 > 192.168.64.1.35110: Flags [P.], seq 568:908, ack 1, win 309, options [nop,nop,TS val 615664 ecr 540031229], length 3405 packets captured6 packets received by filter0 packets dropped by kernel\r\n如上所示，抓取的数据包中显示 IP 地址和端口号。这样还可以阻止 tcpdump 发出 DNS 查找，有助于在网络故障排查中减少数据流量。<p/>\n现在你已经会抓包了，让我们来分析一下这些抓包输出的含义吧。<p/>\n3、理解抓取的报文\ntcpdump 能够抓取并解码多种协议类型的数据报文，如 TCP、UDP、ICMP 等等。虽然这里我们不可能介绍所有的数据报文类型，但可以分析下 TCP 类型的数据报文，来帮助你入门。更多有关 tcpdump 的详细介绍可以参考其 帮助手册。tcpdump 抓取的 TCP 报文看起来如下：<p/>\r\n08:41:13.729687 IP 192.168.64.28.22 > 192.168.64.1.41916: Flags [P.], seq 196:568, ack 1, win 309, options [nop,nop,TS val 117964079 ecr 816509256], length 372\r\n具体的字段根据不同的报文类型会有不同，但上面这个例子是一般的格式形式。<p/>\n第一个字段 08:41:13.729687 是该数据报文被抓取的系统本地时间戳。<p/>\n然后，IP 是网络层协议类型，这里是 IPv4，如果是 IPv6 协议，该字段值是 IP6。<p/>\n192.168.64.28.22 是源 ip 地址和端口号，紧跟其后的是目的 ip 地址和其端口号，这里是 192.168.64.1.41916。<p/>\n在源 IP 和目的 IP 之后，可以看到是 TCP 报文标记段 Flags [P.]。该字段通常取值如下：<p/>\n该字段也可以是这些值的组合，例如 [S.] 代表 SYN-ACK 数据包。<p/>\n接下来是该数据包中数据的序列号。对于抓取的第一个数据包，该字段值是一个绝对数字，后续包使用相对数值，以便更容易查询跟踪。例如此处 seq 196:568 代表该数据包包含该数据流的第 196 到 568 字节。<p/>\n接下来是 ack 值：ack 1。该数据包是数据发送方，ack 值为 1。在数据接收方，该字段代表数据流上的下一个预期字节数据，例如，该数据流中下一个数据包的 ack 值应该是 568。<p/>\n接下来字段是接收窗口大小 win 309，它表示接收缓冲区中可用的字节数，后跟 TCP 选项如 MSS（最大段大小）或者窗口比例值。更详尽的 TCP 协议内容请参考 Transmission Control Protocol(TCP) Parameters。<p/>\n最后，length 372 代表数据包有效载荷字节长度。这个长度和 seq 序列号中字节数值长度是不一样的。<p/>\n现在让我们学习如何过滤数据报文以便更容易的分析定位问题。<p/>\n4、过滤数据包\n正如上面所提，tcpdump 可以抓取很多种类型的数据报文，其中很多可能和我们需要查找的问题并没有关系。举个例子，假设你正在定位一个与 web 服务器连接的网络问题，就不必关系 SSH 数据报文，因此在抓包结果中过滤掉 SSH 报文可能更便于你分析问题。<p/>\ntcpdump 有很多参数选项可以设置数据包过滤规则，例如根据源 IP 以及目的 IP 地址，端口号，协议等等规则来过滤数据包。下面就介绍一些最常用的过滤方法。<p/>\n在命令中指定协议便可以按照协议类型来筛选数据包。比方说用如下命令只要抓取 ICMP 报文：<p/>\r\n$ sudo tcpdump -i any -c5 icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\r\n然后再打开一个终端，去 ping 另一台机器：<p/>\r\n$ ping opensource.comPING opensource.com (54.204.39.132) 56(84) bytes of data.64 bytes from ec2-54-204-39-132.compute-1.amazonaws.com (54.204.39.132): icmp_seq=1 ttl=47 time=39.6 ms\r\n回到运行 tcpdump 命令的终端中，可以看到它筛选出了 ICMP 报文。这里 tcpdump 并没有显示有关 opensource.com 的域名解析数据包：<p/>\r\n12345678\r\n09:34:20.136766 IP rhel75 > ec2-54-204-39-132.compute-1.amazonaws.com: ICMP echo request, id 20361, seq 1, length 6409:34:20.176402 IP ec2-54-204-39-132.compute-1.amazonaws.com > rhel75: ICMP echo reply, id 20361, seq 1, length 6409:34:21.140230 IP rhel75 > ec2-54-204-39-132.compute-1.amazonaws.com: ICMP echo request, id 20361, seq 2, length 6409:34:21.180020 IP ec2-54-204-39-132.compute-1.amazonaws.com > rhel75: ICMP echo reply, id 20361, seq 2, length 6409:34:22.141777 IP rhel75 > ec2-54-204-39-132.compute-1.amazonaws.com: ICMP echo request, id 20361, seq 3, length 645 packets captured5 packets received by filter0 packets dropped by kernel\r\n用 host 参数只抓取和特定主机相关的数据包：<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nn host 54.204.39.132tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes09:54:20.042023 IP 192.168.122.98.39326 > 54.204.39.132.80: Flags [S], seq 1375157070, win 29200, options [mss 1460,sackOK,TS val 122350391 ecr 0,nop,wscale 7], length 009:54:20.088127 IP 54.204.39.132.80 > 192.168.122.98.39326: Flags [S.], seq 1935542841, ack 1375157071, win 28960, options [mss 1460,sackOK,TS val 522713542 ecr 122350391,nop,wscale 9], length 009:54:20.088204 IP 192.168.122.98.39326 > 54.204.39.132.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 122350437 ecr 522713542], length 009:54:20.088734 IP 192.168.122.98.39326 > 54.204.39.132.80: Flags [P.], seq 1:113, ack 1, win 229, options [nop,nop,TS val 122350438 ecr 522713542], length 112: HTTP: GET / HTTP/1.109:54:20.129733 IP 54.204.39.132.80 > 192.168.122.98.39326: Flags [.], ack 113, win 57, options [nop,nop,TS val 522713552 ecr 122350438], length 05 packets captured5 packets received by filter0 packets dropped by kernel\r\n如上所示，只抓取和显示与 54.204.39.132 有关的数据包。<p/>\ntcpdump 可以根据服务类型或者端口号来筛选数据包。例如，抓取和 HTTP 服务相关的数据包：<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nn port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes09:58:28.790548 IP 192.168.122.98.39330 > 54.204.39.132.80: Flags [S], seq 1745665159, win 29200, options [mss 1460,sackOK,TS val 122599140 ecr 0,nop,wscale 7], length 009:58:28.834026 IP 54.204.39.132.80 > 192.168.122.98.39330: Flags [S.], seq 4063583040, ack 1745665160, win 28960, options [mss 1460,sackOK,TS val 522775728 ecr 122599140,nop,wscale 9], length 009:58:28.834093 IP 192.168.122.98.39330 > 54.204.39.132.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 122599183 ecr 522775728], length 009:58:28.834588 IP 192.168.122.98.39330 > 54.204.39.132.80: Flags [P.], seq 1:113, ack 1, win 229, options [nop,nop,TS val 122599184 ecr 522775728], length 112: HTTP: GET / HTTP/1.109:58:28.878445 IP 54.204.39.132.80 > 192.168.122.98.39330: Flags [.], ack 113, win 57, options [nop,nop,TS val 522775739 ecr 122599184], length 05 packets captured5 packets received by filter0 packets dropped by kernel\r\nIP 地址/主机名\n同样，你也可以根据源 IP 地址或者目的 IP 地址或者主机名来筛选数据包。例如抓取源 IP 地址为 192.168.122.98 的数据包：<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nn src 192.168.122.98tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes10:02:15.220824 IP 192.168.122.98.39436 > 192.168.122.1.53: 59332+ A? opensource.com. (32)10:02:15.220862 IP 192.168.122.98.39436 > 192.168.122.1.53: 20749+ AAAA? opensource.com. (32)10:02:15.364062 IP 192.168.122.98.39334 > 54.204.39.132.80: Flags [S], seq 1108640533, win 29200, options [mss 1460,sackOK,TS val 122825713 ecr 0,nop,wscale 7], length 010:02:15.409229 IP 192.168.122.98.39334 > 54.204.39.132.80: Flags [.], ack 669337581, win 229, options [nop,nop,TS val 122825758 ecr 522832372], length 010:02:15.409667 IP 192.168.122.98.39334 > 54.204.39.132.80: Flags [P.], seq 0:112, ack 1, win 229, options [nop,nop,TS val 122825759 ecr 522832372], length 112: HTTP: GET / HTTP/1.15 packets captured5 packets received by filter0 packets dropped by kernel\r\n注意此处示例中抓取了来自源 IP 地址 192.168.122.98 的 53 端口以及 80 端口的数据包，它们的应答包没有显示出来因为那些包的源 IP 地址已经变了。<p/>\n相对的，使用 dst 就是按目的 IP/主机名来筛选数据包。<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nn dst 192.168.122.98tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes10:05:03.572931 IP 192.168.122.1.53 > 192.168.122.98.47049: 2248 1/0/0 A 54.204.39.132 (48)10:05:03.572944 IP 192.168.122.1.53 > 192.168.122.98.47049: 33770 0/0/0 (32)10:05:03.621833 IP 54.204.39.132.80 > 192.168.122.98.39338: Flags [S.], seq 3474204576, ack 3256851264, win 28960, options [mss 1460,sackOK,TS val 522874425 ecr 122993922,nop,wscale 9], length 010:05:03.667767 IP 54.204.39.132.80 > 192.168.122.98.39338: Flags [.], ack 113, win 57, options [nop,nop,TS val 522874436 ecr 122993972], length 010:05:03.672221 IP 54.204.39.132.80 > 192.168.122.98.39338: Flags [P.], seq 1:643, ack 113, win 57, options [nop,nop,TS val 522874437 ecr 122993972], length 642: HTTP: HTTP/1.1 302 Found5 packets captured5 packets received by filter0 packets dropped by kernel\r\n多条件筛选\n当然，可以使用多条件组合来筛选数据包，使用 and 以及 or 逻辑操作符来创建过滤规则。例如，筛选来自源 IP 地址 192.168.122.98 的 HTTP 数据包：<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nn src 192.168.122.98 and port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes10:08:00.472696 IP 192.168.122.98.39342 > 54.204.39.132.80: Flags [S], seq 2712685325, win 29200, options [mss 1460,sackOK,TS val 123170822 ecr 0,nop,wscale 7], length 010:08:00.516118 IP 192.168.122.98.39342 > 54.204.39.132.80: Flags [.], ack 268723504, win 229, options [nop,nop,TS val 123170865 ecr 522918648], length 010:08:00.516583 IP 192.168.122.98.39342 > 54.204.39.132.80: Flags [P.], seq 0:112, ack 1, win 229, options [nop,nop,TS val 123170866 ecr 522918648], length 112: HTTP: GET / HTTP/1.110:08:00.567044 IP 192.168.122.98.39342 > 54.204.39.132.80: Flags [.], ack 643, win 239, options [nop,nop,TS val 123170916 ecr 522918661], length 010:08:00.788153 IP 192.168.122.98.39342 > 54.204.39.132.80: Flags [F.], seq 112, ack 643, win 239, options [nop,nop,TS val 123171137 ecr 522918661], length 05 packets captured5 packets received by filter0 packets dropped by kernel\r\n你也可以使用括号来创建更为复杂的过滤规则，但在 shell 中请用引号包含你的过滤规则以防止被识别为 shell 表达式：<p/>\r\n1234567891011\r\n$ sudo tcpdump -i any -c5 -nn \"port 80 and (src 192.168.122.98 or src 54.204.39.132)\"tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes10:10:37.602214 IP 192.168.122.98.39346 > 54.204.39.132.80: Flags [S], seq 871108679, win 29200, options [mss 1460,sackOK,TS val 123327951 ecr 0,nop,wscale 7], length 010:10:37.650651 IP 54.204.39.132.80 > 192.168.122.98.39346: Flags [S.], seq 854753193, ack 871108680, win 28960, options [mss 1460,sackOK,TS val 522957932 ecr 123327951,nop,wscale 9], length 010:10:37.650708 IP 192.168.122.98.39346 > 54.204.39.132.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 123328000 ecr 522957932], length 010:10:37.651097 IP 192.168.122.98.39346 > 54.204.39.132.80: Flags [P.], seq 1:113, ack 1, win 229, options [nop,nop,TS val 123328000 ecr 522957932], length 112: HTTP: GET / HTTP/1.110:10:37.692900 IP 54.204.39.132.80 > 192.168.122.98.39346: Flags [.], ack 113, win 57, options [nop,nop,TS val 522957942 ecr 123328000], length 05 packets captured5 packets received by filter0 packets dropped by kernel\r\n该例子中我们只抓取了来自源 IP 为 192.168.122.98 或者 54.204.39.132 的 HTTP （端口号80）的数据包。使用该方法就很容易抓取到数据流中交互双方的数据包了。<p/>\n5、检查数据包内容\n在以上的示例中，我们只按数据包头部的信息来建立规则筛选数据包，例如源地址、目的地址、端口号等等。有时我们需要分析网络连接问题，可能需要分析数据包中的内容来判断什么内容需要被发送、什么内容需要被接收等。tcpdump 提供了两个选项可以查看数据包内容，-X 以十六进制打印出数据报文内容，-A 打印数据报文的 ASCII 值。<p/>\n例如，HTTP 请求报文内容如下：<p/>\r\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263\r\n$ sudo tcpdump -i any -c10 -nn -A port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes13:02:14.871803 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [S], seq 2546602048, win 29200, options [mss 1460,sackOK,TS val 133625221 ecr 0,nop,wscale 7], length 0E..<..@.@.....zb6.'....P...@......r........................................13:02:14.910734 IP 54.204.39.132.80 > 192.168.122.98.39366: Flags [S.], seq 1877348646, ack 2546602049, win 28960, options [mss 1460,sackOK,TS val 525532247 ecr 133625221,nop,wscale 9], length 0E..<..@./..a6.'...zb.P..o..&...A..q a...........R.W.......     ................13:02:14.910832 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 133625260 ecr 525532247], length 0E..4..@.@.....zb6.'....P...Ao..'................R.W................13:02:14.911808 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [P.], seq 1:113, ack 1, win 229, options [nop,nop,TS val 133625261 ecr 525532247], length 112: HTTP: GET / HTTP/1.1E.....@.@..1..zb6.'....P...Ao..'................R.WGET / HTTP/1.1User-Agent: Wget/1.14 (linux-gnu)Accept: */*Host: opensource.comConnection: Keep-Alive................13:02:14.951199 IP 54.204.39.132.80 > 192.168.122.98.39366: Flags [.], ack 113, win 57, options [nop,nop,TS val 525532257 ecr 133625261], length 0E..4.F@./..\"6.'...zb.P..o..'.......9.2......R.a....................13:02:14.955030 IP 54.204.39.132.80 > 192.168.122.98.39366: Flags [P.], seq 1:643, ack 113, win 57, options [nop,nop,TS val 525532258 ecr 133625261], length 642: HTTP: HTTP/1.1 302 FoundE....G@./...6.'...zb.P..o..'.......9........R.b....HTTP/1.1 302 FoundServer: nginxDate: Sun, 23 Sep 2018 17:02:14 GMTContent-Type: text/html; charset=iso-8859-1Content-Length: 207X-Content-Type-Options: nosniffLocation: https://opensource.com/Cache-Control: max-age=1209600Expires: Sun, 07 Oct 2018 17:02:14 GMTX-Request-ID: v-6baa3acc-bf52-11e8-9195-22000ab8cf2dX-Varnish: 632951979Age: 0Via: 1.1 varnish (Varnish/5.2)X-Cache: MISSConnection: keep-alive<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"><html><head><title>302 Found</title></head><body><h1>Found</h1><p>The document has moved <a href=\"https://opensource.com/\">here</a>.</p></body></html>................13:02:14.955083 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [.], ack 643, win 239, options [nop,nop,TS val 133625304 ecr 525532258], length 0E..4..@.@.....zb6.'....P....o...................R.b................13:02:15.195524 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [F.], seq 113, ack 643, win 239, options [nop,nop,TS val 133625545 ecr 525532258], length 0E..4..@.@.....zb6.'....P....o...................R.b................13:02:15.236592 IP 54.204.39.132.80 > 192.168.122.98.39366: Flags [F.], seq 643, ack 114, win 57, options [nop,nop,TS val 525532329 ecr 133625545], length 0E..4.H@./.. 6.'...zb.P..o..........9.I......R......................13:02:15.236656 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [.], ack 644, win 239, options [nop,nop,TS val 133625586 ecr 525532329], length 0E..4..@.@.....zb6.'....P....o...................R..................10 packets captured10 packets received by filter0 packets dropped by kernel\r\nFound\nThe document has moved here.<p/>\r\n123456789101112131415161718\r\n  ................13:02:14.955083 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [.], ack 643, win 239, options [nop,nop,TS val 133625304 ecr 525532258], length 0E..4..@.@.....zb6.'....P....o...................R.b................13:02:15.195524 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [F.], seq 113, ack 643, win 239, options [nop,nop,TS val 133625545 ecr 525532258], length 0E..4..@.@.....zb6.'....P....o...................R.b................13:02:15.236592 IP 54.204.39.132.80 > 192.168.122.98.39366: Flags [F.], seq 643, ack 114, win 57, options [nop,nop,TS val 525532329 ecr 133625545], length 0E..4.H@./.. 6.'...zb.P..o..........9.I......R......................13:02:15.236656 IP 192.168.122.98.39366 > 54.204.39.132.80: Flags [.], ack 644, win 239, options [nop,nop,TS val 133625586 ecr 525532329], length 0E..4..@.@.....zb6.'....P....o...................R..................10 packets captured10 packets received by filter0 packets dropped by kernel\r\n这对定位一些普通 HTTP 调用 API 接口的问题很有用。当然如果是加密报文，这个输出也就没多大用了。<p/>\n6、保存抓包数据\ntcpdump 提供了保存抓包数据的功能以便后续分析数据包。例如，你可以夜里让它在那里抓包，然后早上起来再去分析它。同样当有很多数据包时，显示过快也不利于分析，将数据包保存下来，更有利于分析问题。<p/>\n使用 -w 选项来保存数据包而不是在屏幕上显示出抓取的数据包：<p/>\r\n123456\r\n$ sudo tcpdump -i any -c10 -nn -w webserver.pcap port 80[sudo] password for ricardo:tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes10 packets captured10 packets received by filter0 packets dropped by kernel\r\n该命令将抓取的数据包保存到文件 webserver.pcap。后缀名 pcap 表示文件是抓取的数据包格式。<p/>\n正如示例中所示，保存数据包到文件中时屏幕上就没有任何有关数据报文的输出，其中 -c10 表示抓取到 10 个数据包后就停止抓包。如果想有一些反馈来提示确实抓取到了数据包，可以使用 -v 选项。<p/>\ntcpdump 将数据包保存在二进制文件中，所以不能简单的用文本编辑器去打开它。使用 -r 选项参数来阅读该文件中的报文内容：<p/>\r\n12345678910111213\r\n$ tcpdump -nn -r webserver.pcapreading from file webserver.pcap, link-type LINUX_SLL (Linux cooked)13:36:57.679494 IP 192.168.122.98.39378 > 54.204.39.132.80: Flags [S], seq 3709732619, win 29200, options [mss 1460,sackOK,TS val 135708029 ecr 0,nop,wscale 7], length 013:36:57.718932 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [S.], seq 1999298316, ack 3709732620, win 28960, options [mss 1460,sackOK,TS val 526052949 ecr 135708029,nop,wscale 9], length 013:36:57.719005 IP 192.168.122.98.39378 > 54.204.39.132.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 135708068 ecr 526052949], length 013:36:57.719186 IP 192.168.122.98.39378 > 54.204.39.132.80: Flags [P.], seq 1:113, ack 1, win 229, options [nop,nop,TS val 135708068 ecr 526052949], length 112: HTTP: GET / HTTP/1.113:36:57.756979 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [.], ack 113, win 57, options [nop,nop,TS val 526052959 ecr 135708068], length 013:36:57.760122 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [P.], seq 1:643, ack 113, win 57, options [nop,nop,TS val 526052959 ecr 135708068], length 642: HTTP: HTTP/1.1 302 Found13:36:57.760182 IP 192.168.122.98.39378 > 54.204.39.132.80: Flags [.], ack 643, win 239, options [nop,nop,TS val 135708109 ecr 526052959], length 013:36:57.977602 IP 192.168.122.98.39378 > 54.204.39.132.80: Flags [F.], seq 113, ack 643, win 239, options [nop,nop,TS val 135708327 ecr 526052959], length 013:36:58.022089 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [F.], seq 643, ack 114, win 57, options [nop,nop,TS val 526053025 ecr 135708327], length 013:36:58.022132 IP 192.168.122.98.39378 > 54.204.39.132.80: Flags [.], ack 644, win 239, options [nop,nop,TS val 135708371 ecr 526053025], length 0$\r\n这里不需要管理员权限 sudo 了，因为此刻并不是在网络接口处抓包。<p/>\n你还可以使用我们讨论过的任何过滤规则来过滤文件中的内容，就像使用实时数据一样。 例如，通过执行以下命令从源 IP 地址 54.204.39.132 检查文件中的数据包：<p/>\r\n123456\r\n$ tcpdump -nn -r webserver.pcap src 54.204.39.132reading from file webserver.pcap, link-type LINUX_SLL (Linux cooked)13:36:57.718932 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [S.], seq 1999298316, ack 3709732620, win 28960, options [mss 1460,sackOK,TS val 526052949 ecr 135708029,nop,wscale 9], length 013:36:57.756979 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [.], ack 113, win 57, options [nop,nop,TS val 526052959 ecr 135708068], length 013:36:57.760122 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [P.], seq 1:643, ack 113, win 57, options [nop,nop,TS val 526052959 ecr 135708068], length 642: HTTP: HTTP/1.1 302 Found13:36:58.022089 IP 54.204.39.132.80 > 192.168.122.98.39378: Flags [F.], seq 643, ack 114, win 57, options [nop,nop,TS val 526053025 ecr 135708327], length 0\r\n下一步做什么？\n以上的基本功能已经可以帮助你使用强大的 tcpdump 抓包工具了。更多的内容请参考 tcpdump 网站 以及它的 帮助文件。<p/>\ntcpdump 命令行工具为分析网络流量数据包提供了强大的灵活性。如果需要使用图形工具来抓包请参考 Wireshark。<p/>\nWireshark 还可以用来读取 tcpdump 保存的 pcap 文件。你可以使用 tcpdump 命令行在没有 GUI 界面的远程机器上抓包然后在 Wireshark 中分析数据包。<p/>\n <p/>\n 4 收藏\n 2 评论\n", "front_img_path": "full/d9d38887b3603810e70bc381bee5eeda66a714b0.jpg"},{"title": "5 个适合系统管理员使用的告警可视化工具", "url": "http://blog.jobbole.com/114503/", "create_date": "2018/11/12", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2018/11/23a2beb8acbfad03600ee14aa07ce9ce.png"], "fav_nums": 1, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,Linux", "object_id": "e99433ceedf57d06ee6762ea2097e38e", "content": "原文出处： Dan Barker   译文出处：Linux中国/Hank Chow   这些开源的工具能够通过输出帮助用户了解系统的运行状况，并对可能发生的潜在问题作出告警。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/23a2beb8acbfad03600ee14aa07ce9ce.png\" alt=\"\" /><p/>\n你大概已经知道（或猜到）告警可视化alerting and visualization工具是用来做什么的了。下面我们就要来说一下，为什么要讨论这样的工具，甚至某些系统专门将可视化作为特有的功能。<p/>\n可观察性Observability的概念来自控制理论control theory，这个概念描述了我们通过对系统的输入和输出来了解其的能力。本文将重点介绍具有可观察性的输出组件。<p/>\n告警可视化工具可以对其它系统的输出进行分析，进而对输出的信息进行结构化表示。告警实际上是对系统异常状态的描述，而可视化则是让用户能够直观理解的结构化表示。<p/>\n常见的可视化告警\n首先要明确一下告警alert的含义。在人员无法响应告警内容情况下，不应该发送告警 —— 包括那些发给多个人但只有其中少数人可以响应的告警，以及系统中的每个异常都触发的告警。因为这样会产生告警疲劳，告警接收者也往往会对这些过多的告警采取忽视的态度 —— 直到系统恶化到以少见的方式告警。<p/>\n例如，如果管理员每天都会收到告警系统发来的数百封告警邮件，他就很容易会忽略告警系统的所有邮件。除非他真的看到问题发生，或者受到了客户或上级的询问时，管理员才会重新重视告警信息。在这种情况下，告警已经失去了原有的意义和用途。<p/>\n告警不是一个持续的信息流或者状态更新。告警的目的在于暴露系统无法自动恢复的问题，而且告警应该只发送给最有可能解决问题的人员。超出这个定义的内容都不应该作为告警，否则将会对实际工作造成不良的影响。<p/>\n不同的告警体系都会有各自的告警类型，因此不能用优先级（P1-P5）或者诸如“信息”、“警告”、“严重”之类的字眼来一概而论，下面我会介绍一些新兴的复杂系统的事件响应中出现的通用分类方式。<p/>\n刚才我提到了一个“信息”这个告警类型，但实际上告警不应该是一个信息，尽管有些人可能会不这样认为。但我觉得如果一个告警没有发送给任何一个人，它就不应该是警报，而只是一些在许多系统中被视为警报的数据点，代表了一些应该知晓但不需要响应的事件。它更应该作为告警可视化工具的一部分，而不是会导致触发告警的事件。《实用监控》是这个领域的必读书籍，其作者 Mike Julian 在书中就介绍了他自己关于告警的看法。<p/>\n而非信息警报则代表告警需要被响应以及需要相关的操作。我将这些告警大致分为内部故障和外部故障两种类型，而对于大多数公司来说，通常会有两个以上的级别来确定响应告警的优先级。系统性能下降就是一种故障，因为其对用户的影响通常都是未知的。<p/>\n内部故障比外部故障的优先级低，但也需要快速响应。内部故障通常包括公司员工使用的内部系统或仅对公司员工可见的应用故障。<p/>\n外部故障则包括任何马上会产生业务影响的系统故障，但不包括影响系统更新的故障。外部故障一般包括客户所面临的应用故障、数据库故障和导致系统可用性或一致性失效的网络故障，这些都会影响用户的正常使用。对于不直接影响用户的依赖组件故障也属于外部故障，随着应用程序的不断运行，一旦依赖组件发生故障，系统的性能也会受到波及。这种情况对于使用某些外部服务或数据源的系统来说很常见，尽管这些外部服务或数据源对于可能不涉及到系统的主要功能，但是当系统在处理相关依赖组件的错误时可能会出现较明显的延迟。<p/>\n可视化的种类有很多，我就不一一赘述了。这是一个有趣的研究领域，在我这些年的数据分析经历当中，学习和应用可视化方面的知识可以说是相当有挑战性。我们需要将复杂的系统输出通过直观的方式来向他人展示，才能有效地把信息传播出去。Google Charts 和 Tableau 都提供了很多可视化方面的工具。下面将会介绍一些最常见的可视化创新解决方案。<p/>\n折线图可能是最常见的可视化方式了，它可以让用户很直观地按照时间维度了解系统的情况。系统中每个单一或聚合的指标都会以一条折线在图表中体现。但当同一个图表中同时存在多条折线时，就可能会对阅读有所影响（如下图所示），所以大多数情况下都可以选择仅查看其中的少数几条折线，而不是让所有折线同时显示。如果某个指标的数值产生了大于正常范围的波动，就会很容易发现。例如下图中异常的紫线、黄线、浅蓝线。<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/0dd2069cff1b7d981327188af587df81.png\" alt=\"\" /><p/>\n折线图的另一个用法是可以将多条折线堆叠起来以显示它们之间的关系。例如对于通过折线图反映服务器的请求数量，可以单独看到每台服务器上的请求，也可以聚合在一起看。这就可以在同一个图表中灵活查看整个系统以及每个实例的情况了。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/796cb63969e047d1f69842a4da986e19.png\" width=\"625\" height=\"300\" /><p/>\n另一种常见的可视化方式是热力图。热力图与条形图比较类似，还可以在条形图的基础上显示某部分在整体中占比的变化情况。例如在查看网络请求延时的时候，就可以使用热力图快速查看到所有网络请求的总体趋势和分布情况，另外，它可以使用不同颜色来表示不同部分的数值。<p/>\n在以下这个热力图中，通过竖直方向上每个时间段的色块数量分布，可以清楚地看到大部分数据集中在整个范围的中心位置。我们还可以发现，大多数时间段的色块分布都是比较宽松的，而 14:00 到 15:00 这一段则分布得很密集，这样的分布有可能意味着一种不健康的状态。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/42d5fcced43e99b28fa87680ad770ee8.png\" alt=\"\" /><p/>\n还有一种常见的可视化方式是仪表图，用户可以通过仪表图快速了解单个指标。仪表一般用于单个指标的显示，例如车速表代表汽车的行驶速度、油量表代表油箱中的汽油量等等。大多数的仪表图都有一个共通点，就是会划分出所示指标的对应状态。如下图所示，绿色表示正常的状态，橙色表示不良的状态，而红色则表示极差的状态。下图中间一行模拟了真实仪表的显示情况。<p/>\n<img class=\"aligncenter\" src=\"http://jbcdn2.b0.upaiyun.com/2018/11/d6e213ac8cdcd5d7953f9ff984f8d9db.png\" alt=\"\" /><p/>\n上面图表中，除了常规仪表样式的显示方式之外，还有较为直接的数据显示方式，配合相同的配色方案，一眼就可以看出各个指标所处的状态，这一点与和仪表的特点类似。所以，最下面一行可能是仪表图的最佳显示方式，用户不需要仔细阅读，就可以大致了解各个指标的不同状态。这种类型的可视化是我最常用的类型，在数秒钟之间，我就可以全面地总览系统各方面地运行情况。<p/>\n由 Netflix 的 Brendan Gregg 在 2011 年开始使用的火焰图是一种较为少见地可视化方式。它不像仪表图那样可以从图表中快速得到关键信息，通常只会在需要解决某个应用的问题的时候才会用到这种图表。火焰图主要用于 CPU、内存和相关帧方面的表示，X 轴按字母顺序将帧一一列出，而 Y 轴则表示堆栈的深度。图中每个矩形都是一个标明了调用的函数的堆栈帧。矩形越宽，就表示它在堆栈中出现越频繁。在分析系统性能问题的时候，火焰图能够起到很大的作用，大家不妨尝试一下。<p/>\n<img src=\"http://jbcdn2.b0.upaiyun.com/2018/11/3a6cff91e2bfae58e7c2045e8321f67f.png\" alt=\"\" /><p/>\n工具的选择\n在告警工具方面，有几个商用的工具相当不错。但由于这是一篇介绍开源技术的文章，我只会介绍那些已经被广泛使用的免费工具。希望你也能够为这些工具贡献你自己的代码，让它们更加完善。<p/>\nBosun\n如果你的电脑出现问题，得多亏 Stack Exchange 你才能在网上查到解决办法。Stack Exchange 以众包问答的模式运营着很多不同类型的网站。其中就有广受开发者欢迎的 Stack Overflow，以及运维方面有名的 Super User。除此以外，从育儿经验到科幻小说、从哲学讨论到单车论坛，Stack Exchange 都有涉猎。<p/>\nStack Exchange 开源了它的告警管理系统 Bosun，同时也发布了 Prometheus 及其 AlertManager 系统。这两个系统有共通点。Bosun 和 Prometheus 一样使用 Golang 开发，但 Bosun 比 Prometheus 更为强大，因为它可以使用指标聚合metrics aggregation以外的方式与系统交互。Bosun 还可以从日志和事件收集系统中提取数据，并且支持 Graphite、InfluxDB、OpenTSDB 和 Elasticsearch。<p/>\nBosun 的架构包括一个单一的服务器的二进制文件，一个诸如 OpenTSDB 的后端、Redis 以及 scollector 代理。 scollector 代理会自动检测主机上正在运行的服务，并反馈这些进程和其它的系统资源的情况。这些数据将发送到后端。随后 Bosun 的二进制服务文件会向后端发起查询，确定是否需要触发告警。也可以通过 Grafana 这些工具通过一个通用接口查询 Bosun 的底层后端。而 Redis 则用于存储 Bosun 的状态信息和元数据。<p/>\nBosun 有一个非常巧妙的功能，就是可以根据历史数据来测试告警。这是我几年前在使用 Prometheus 的时候就非常需要的功能，当时我有一个异常的数据需要产生告警，但没有一个可以用于测试的简便方法。为了确保告警能够正常触发，我不得不造出对应的数据来进行测试。而 Bosun 让这个步骤的耗时大大缩短。<p/>\nBosun 更是涵盖了所有常用过的功能，包括简单的图形化表示和告警的创建。它还带有强大的用于编写告警规则的表达式语言。但 Bosun 默认只带有电子邮件通知配置和 HTTP 通知配置，因此如果需要连接到 Slack 或其它工具，就需要对配置作出更大程度的定制化（其文档中有）。类似于 Prometheus，Bosun 还可以使用模板通知，你可以使用 HTML 和 CSS 来创建你所需要的电子邮件通知。<p/>\nCabot\nCabot 由 Arachnys 公司开发。你或许对 Arachnys 公司并不了解，但它很有影响力：Arachnys 公司构建了一个基于云的先进解决方案，用于防范金融犯罪。在之前的公司时，我也曾经参与过类似“了解你的客户（KYC）”的工作。大多数公司都认为与恐怖组织产生联系会造成相当不好的影响，因为恐怖组织可能会利用自己的系统来筹集资金。而这些解决方案将有助于防范欺诈类犯罪，尽管这类犯罪情节相对较轻，但仍然也会对机构产生风险。<p/>\nArachnys 公司为什么要开发 Cabot 呢？其实只是因为 Arachnys 的开发人员对 Nagios 不太熟悉。Cabot 的出现对很多人来说都是一个好消息，它基于 Django 和 Bootstrap 开发，因此如果想对这个项目做出自己的贡献，门槛并不高。（另外值得一提的是，Cabot 这个名字来源于开发者的狗。）<p/>\n与 Bosun 类似，Cabot 也不对数据进行收集，而是使用监控对象的 API 提供的数据。因此，Cabot 告警的模式是拉取而不是推送。它通过访问每个监控对象的 API，根据特定的指标检索所需的数据，然后将告警数据使用 Redis 缓存，进而持久化存储到 Postgres 数据库。<p/>\nCabot 的一个较为少见的特点是，它原生支持 Graphite，同时也支持 Jenkins。Jenkins 在这里被视为一个集中式的定时任务，它会以对待故障的方式去对待构建失败的状况。构建失败当然没有系统故障那么紧急，但一旦出现构建失败，还是需要团队采取措施去处理，毕竟并不是每个人在收到构建失败的电子邮件时都会亲自去检查 Jenkins。<p/>\nCabot 另一个有趣的功能是它可以接入 Google 日历安排值班人员，这个称为 Rota 的功能用处很大，希望其它告警系统也能加入类似的功能。Cabot 目前仅支持安排主备联系人，但还有继续改进的空间。它自己的文档也提到，如果需要全面的功能，更应该考虑付费的解决方案。<p/>\nStatsAgg\nPearson 作为一家开发了 StatsAgg 告警平台的出版公司，这是极为罕见的，当然也很值得敬佩。除此以外，Pearson 还运营着另外几个网站以及和 OReilly Media 合资的企业。但我仍然会将它视为出版教学书籍的公司。<p/>\nStatsAgg 除了是一个告警平台，还是一个指标聚合平台，甚至也有点类似其它系统的代理。StatsAgg 支持通过 Graphite、StatsD、InfluxDB 和 OpenTSDB 输入数据，也支持将其转发到各种平台。但随着中心服务的负载不断增加，风险也不断增大。尽管如此，如果 StatsAgg 的基础架构足够强壮，即使后端存储平台出现故障，也不会对它产生告警的过程造成影响。<p/>\nStatsAgg 是用 Java 开发的，为了尽可能降低复杂性，它仅包括主服务和一个 UI。StatsAgg 支持基于正则表达式匹配来发送告警，而且它更注重于服务方面的告警，而不是服务器基础告警。我认为它填补了开源监控工具方面的空白，而这正式它自己的目标。<p/>\n可视化工具\nGrafana\nGrafana 的知名度很高，它也被广泛采用。每当我需要用到数据面板的时候，我总是会想到它，因为它比我使用过的任何一款类似的产品都要好。Grafana 由 Torkel Ödegaard 开发的，像 Cabot 一样，也是在圣诞节期间开发的，并在 2014 年 1 月发布。在短短几年之间，它已经有了长足的发展。Grafana 基于 Kibana 开发，Torkel 开启了新的分支并将其命名为 Grafana。<p/>\nGrafana 着重体现了实用性以及数据呈现的美观性。它天生就可以从 Graphite、Elasticsearch、OpenTSDB、Prometheus 和 InfluxDB 收集数据。此外有一个 Grafana 商用版插件可以从更多数据源获取数据，但是其他数据源插件也并非没有开源版本，Grafana 的插件生态系统已经提供了各种数据源。<p/>\nGrafana 能做什么呢？Grafana 提供了一个中心化的了解系统的方式。它通过 web 来展示数据，任何人都有机会访问到相关信息，当然也可以使用身份验证来对访问进行限制。Grafana 使用各种可视化方式来提供对系统一目了然的了解。Grafana 还支持不同类型的可视化方式，包括集成告警可视化的功能。<p/>\n现在你可以更直观地设置告警了。通过 Grafana，可以查看图表，还可以查看由于系统性能下降而触发告警的位置，单击要触发报警的位置，并告诉 Grafana 将告警发送何处。这是一个对告警平台非常强大的补充。告警平台不一定会因此而被取代，但告警系统一定会由此得到更多启发和发展。<p/>\nGrafana 还引入了很多团队协作的功能。不同用户之间能够共享数据面板，你不再需要为 Kubernetes 集群创建独立的数据面板，因为由 Kubernetes 开发者和 Grafana 开发者共同维护的一些数据面板已经可用了。<p/>\n团队协作过程中一个重要的功能是注释。注释功能允许用户将上下文添加到图表当中，其他用户就可以通过上下文更直观地理解图表。当团队成员在处理某个事件，并且需要沟通和理解时，这个功能就十分重要了。将所有相关信息都放在需要的位置，可以让整个团队中快速达成共识。在团队需要调查故障原因和定位事件责任时，这个功能就可以发挥作用了。<p/>\nVizceral\nVizceral 由 Netflix 开发，用于在故障发生时更有效地了解流量的情况。Grafana 是一种通用性更强的工具，而 Vizceral 则专用于某些领域。 尽管 Netflix 表示已经不再在内部使用 Vizceral，也不再主动对其展开维护，但 Vizceral 仍然会定期更新。我在这里介绍这个工具，主要是为了介绍它的的可视化机制，以及如何利用它来协助解决问题。你可以在样例环境中用它来更好地掌握这一类系统的特性。<p/>\n <p/>\n 1 收藏\n", "front_img_path": "full/07b91797358312619e8066c091039d143bcc5486.jpg"},{"title": "程序员神器 StackOverflow 10 岁了，它长大后想成为什么？", "url": "http://blog.jobbole.com/114499/", "create_date": "2018/11/20", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2016/03/6dd085bf97f82f786b71b4fbcdb37e88.jpg"], "fav_nums": 1, "comment_nums": 1, "vote_nums": 4, "tags": "IT技术,,StackOverflow", "object_id": "c6b94d24afe2779abf58694dfb05b794", "content": "本文由 伯乐在线 - 一汀 翻译。未经许可，禁止转载！英文出处：Jeff Atwood。欢迎加入翻译组。【伯乐在线导读】：今年 9 月，程序员必备神器之一的 Stack Overflow 正式成立 10 周年了。截至目前为止，SO 用户量高达 930 万，也许你经常在 SO 上找解决方案。但你可能还不真正了解 SO。来看看 SO 创始人 Jeff Atwood 是怎么说的。<p/>\n <p/>\n现实生活中常常有人问我，我的工作是什么，对此我有一个 15 秒的回答：<p/>\n我们创建了一个类似维基百科的网站，程序员们可以在上面发表或者解答问题。网站的名字叫 Stack Overflow。<p/>\n截至 2018 年 9 月，Joel Spolsky 与我共同创建的 Stack Overflow 已经走过了 10 个年头。从 2012 年开始我就在做其他的工作了，但是能让人们在我过世之后还能想起我的东西，那肯定还是我的老伙伴 Stack Overflow。<p/>\n这里我好像应该滔滔不绝地说 Stack Overflow 有多么优秀，而我作为创始人是有多么伟大。<p/>\n<img src=\"http://www.poorlydrawnlines.com/wp-content/uploads/2011/09/bragging.jpg\" alt=\"\" /><p/>\n但I这些我都不在乎。<p/>\n我真正在乎的是，Stack Overflow 对程序员们是不是有帮助。对此，让我们看看如今最牛叉的开发者之一，我的偶像 John Carmack 是怎么评价的。<p/>\n<img src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_C8591C5627C21D2C099CE3885D4246D7CFCC30D73E98C53D54E39F0983BC11D8_1541195322340_image.png\" alt=\"Stack Overflow为提升开发者的效率事业差不多贡献了好几亿美元\" /> 在为提升开发者的效率方面，Stack Overflow 可能贡献了好几十亿美元<p/>\n说实话，2013 年 9 月 17 日是很美好的一天。我读到这条推的时候吓了一跳，不光是因为我经常用 Carl Sagan 的方式读 Billions 这个词。我在 Twitter 每隔几天就会读到一些残酷无尽的人间疾苦，以及人们在网络上的互相叫骂。与此相反，那一天是我感觉到的只有喜悦。这也提醒了我，我该查查 Twitter，看看如今谁还对网络抱有不同的理解。<p/>\nStack Overflow 有着如此多用户，也帮助了一个时代的开发者，对此我感到既荣幸又谦卑。但是，实现这一成就的并不是我。<p/>\n是你们，对 Stack Overflow 贡献了经过深入研究后想到的提问；\n是你们，对 Stack Overflow 贡献出了简洁而清晰的答案；\n是你们，编辑了 Stack Overflow 的提问或答案，并使其变得更好；\n世界各地开发者们所贡献的那些大大小小的提问与回答，把 Stack Overflow 变成了一个在开发领域能与维基百科抗衡的创意共享知识库。这实在是…非常的不可思议。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stack-overflow-homepage-oct-2018.png\" alt=\"\" /><p/>\n不过成功的故事都很无聊。这个世界上有很多人，本身运气好，但还时不时的告诉别人是自己的努力以及喝活力汽水换来了成功。我觉得失败的故事更有教育意义，在建立业务与规划未来时，我把自己想象成深渊专家，并开始一场比赛。这是我自己做事的习惯。<p/>\n<img class=\"aligncenter\" src=\"https://blog.codinghorror.com/content/images/2018/10/abyss-oc.jpg\" alt=\"abyss-oc\" /><p/>\n当你在凝视深渊的时候，深渊也正在凝视着你  Friedrich Nietzsche<p/>\n由此，我现在要与耀眼的深渊对视，预测一下Stack Overflow未来十年会遇到的挑战。这之前，我要先澄清以下事实。<p/>\n1、从 2012 年 2 月开始，我就没有再为 Stack Overflow 做事了，也没有对其运营有过任何建议。你问我对如何运作Stack Overflow 竟然能没有建议？额，那你可能不认识我。你问我难道我不会时不时给员工发邮件告诉他们我的想法？我也许会吧，但是我为数不多的归档邮件可以证明，这个事情很少发生。<p/>\n2、Stack 有着优秀的员工，他们中的大多数（包括我离开之前的 Stack Overflow 社区成员）都能对我们的使命给出更好的，不像我那样胡思乱想的阐述。我会用生命信任他们吗？不会。但是我会用 Joel 的生命信任他们！<p/>\n3、Stack Overflow并不属于我或者 Joel，或者其他一些优秀的开发者。Stack Overflow 的运作靠的是世界各地日复一日做开发的人们，就像你或我一样。我觉得 Stack Overflow 就像个家长，它的目标是让孩子们最终能离开家长身边，成为可以独当一面的大人。<p/>\n4、作为 Stack Overflow 的创始人，我在社区成立的最初四年里，花了非常多的时间参与制定规则与规范。你现在阅读的是我所强观点，弱坚持。这只是我的一些想法，我也希望自己的预测是准确的，但是这并不意味着我可以预测未来，或者我有资格去预测未来。不过我并不会以自己是否具有资格而不去做一些事情。<p/>\nStack Overflow首先是一个 wiki\nStack Overflow不仅是个论坛，它跟维基百科有很多相似之处。我们衡量问题和答案是否有意义的方式，不是看那些问题和答案对特定几个人的帮助，而是看随着时间的推移，这些问题和答案能不能帮助到越来越多的人。我从 2008 年 Stack Overflow 上线后就在强调这个关系。来看看下面谁的地位最高。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stack-overflow-venn-diagram.png\" alt=\"\" /><p/>\n为了强调这一核心价值，Stack Overflow添加了一个简洁的功能。那就是在用户资料里会显示，你所贡献的问题与回答帮助到了多少人。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stackoverflow-people-reached-profile-stat-1.png\" alt=\"\" /><p/>\n这些问答内容到底服务于谁？回答问题为何有如此严格的审核过程？对于 Stack Overflow 最常见的抱怨通常来自于对前面这两个问题的误解。<p/>\n<img src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_C8591C5627C21D2C099CE3885D4246D7CFCC30D73E98C53D54E39F0983BC11D8_1541370256957_image.png\" alt=\"我希望更多的人能明白，Stack Overflow并不是一个“回答我的问题”的地方，它是“让我们合作建立一个对未来的开发者们有益的地方”。也许Stack Overflow应该更加努力去帮助用户理解这件事。\" /> 我希望更多的人能明白，Stack Overflow并不是一个“回答我的问题”的地方，它是“让我们合作建立一个对未来的开发者们有益的地方”。也许 Stack Overflow 应该更加努力去帮助用户理解这件事。<p/>\n如今很多用户，甚至泡在 Hacker News 上的技术圈网友，都不知道 Stack Overflow 上有个功能，那就是每一个问题都是可以修改的，即使是没有登录的匿名用户也可以修改。对此我深表惊讶。这个功能不奇怪，对吧，因为 Stack Overflow 就是一种维基百科，这也是维基百科的运行模式，任何人都可以修改任何内容。不信的话，现在就去试试吧，找一个你认为可以提高的问题或者回答，点击“改善这个回答（improve this answer）”或者“改善这个问题（improve this question）”，然后写下你的改良版。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stack-overflow-edit-question.png\" alt=\"\" /><p/>\nStack Overflow 有很多功能（甚至也包括我自己在 2012 年之前的一些所作所为）都容易导致用户误解其核心价值。理论上，“如今每一个开发者都听过，用过以及了解 Stack Overflow”，但我觉得这个假设不准确。毕竟每时每刻都有新的开发者诞生。说得更复杂一点，Stack Overflow 的使用模式有三种，从大到小，以倒金字塔的形式排列如下：<p/>\n1、我在需要的时候去搜索答案<p/>\n用户直接上网搜索，搜索引擎会直接显示出 Stack Overflow 中的高票答案。出现在搜索引擎第一页，这也是Stack Overflow的主要目标。如Stack Overflow正常运行，98%的开发者在他们的整个职业生涯中，不需要主动提出或者回答问题。只要通过网络搜索就可以找Stack Overflow上到他们需要的结果。这是个好事，非常好的事。<p/>\n2、我遇到很困难的问题时会参与Stack Overflow的讨论，因为单纯的搜索找不到我想要的答案<p/>\n只在遇到难以解决的困难时，参与Stack Overflow的讨论，这很合理。然而，我觉得这个阶层的用户最容易感觉到Stack Overflow不是那么容易使用，因为这类用户可能对Stack Overflow很熟悉，但是并不清楚发布问题的流程。并且在他们急切想寻找答案的时候，他们没有时间或心思去应对Stack Overflow对于问题背景、格式、描述以及引用要求。<p/>\n3、为了自己的职业发展，我主动参与Stack Overflow的问题讨论<p/>\n这个阶层的用户很有经验，他们贡献了很多答案，也了解什么样的问题是好问题，是他们感兴趣、愿意回答的问题。他们不经常提问，因为他们知道如何去全面搜索他们想要的答案。但是他们一旦提问，那一定是个示范性的好问题。<p/>\n（理论上这里还有个第四阶层用户，他们无私的贡献了很多提问与回答，目的只是为了推动软件开发行业的发展，造福于新一代的开发者们。但是我们没空提这些大神，你们只会让我们显得更加平凡，所以我们就此打住吧）<p/>\n第一阶层的用户在社区里开心地逛了好几年，却在变为第二阶层用户时，一下子有了不开心的用户体验。对此我一点儿也不惊讶。我认为解决这个问题最主要的方式，就是改变并提高提问页面的用户体验。另外值得注意的一点是，用户在提出了某个问题后，可能收到关于问题信息不足的负面反馈，但是他们也许并不知道，你的问题应该是“有益于其他用户而并不只是你自己”。<p/>\nStack Overflow采用了维基百科的模式，也使其自身受到了很多限制。即使用户在提问前就知道这些，很多时候到底什么是“有用的信息”也很难判断。同理，很多时候我们也不确定到底什么样的话题，人群或者地点需要一份维基百科。Henrietta Lacks 有自己的维基百科页面，这毫无争议，但是他住在奥哈马市的表兄 Dave，那个提出了一个关于 PHP 5.6 的奇怪问题的人，是不是该被写入维基百科呢？<p/>\n随着时间的推移，重复内容像地雷一样遍地都是\n这事我很早就预料到了。老实说，我有点庆幸自己在 2012 年离开了 Stack Overflow，这样我就不用去处理这个难以置信的技术性难题：重复性内容。在我听到的关于 Stack Overflow 的所有抱怨里，重复内容是我觉得最有共鸣的。<p/>\n如果你接受Stack Overflow是个类似于维基百科系统这一前提，那同理你显然不能接受，在维基百科中，对于意大利有五个不同的词条。Stack Overflow不允许对于同一个技术问题有重复的提问。我们确实有很多避免重复问题的功能，比如输入问题时的同步搜索，以及提交问题前，你会看到一个很明显的，鼓励用户先去搜索相关问题的搜索框。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stack-overflow-how-to-ask.jpg\" alt=\"\" /><p/>\n 如何查找并判断重复内容是个非常有难度的问题，即使是Google这样的公司，有着名副其实世界最聪明的工程师团队，专攻了20年也没有解决这个问题。<p/>\n当你在一个不允许重复问题的网站中提问时，系统去重的难度取决于总问题数量，处理一百万的问题总量的去重与一千万甚至一亿相比，是非常不同的。系统处理问题去重的难度，会从不太难处理变为最终的完全无法处理。比如你提出了一个与艺术类相关的问题，那么系统需要根据你的提问内容，在不胜枚举的已有问题中进行筛选，以确保没有看起来相似的提问。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stackoverflow-asking-duplicate-question.png\" alt=\"\" /><p/>\n等会儿，还有个更难的问题！<p/>\n相似问题中有一点内容变化也是可以的，因为十个不同的人在提出同一个问题时，完全可能使用毫不相关的词语来形容这个问题。我知道这听起来很疯狂，但是相信我：人类极其擅长做这样的事。我们希望保留这些重复的问题，并且让他们都指向同一个主问题，以便于用户更好的搜索他们需要的内容，即使这些用户使用了那些平常不太会被用到的词语去描述问题。\n如何判断你提出的问题是不是重复，这是个不小的挑战。多少词语的重叠才能决定一个问题是不是与另一个重复？谁来决定？不同人有不同理论。这是个以人类语言为标准的解析，然后人类吧……不可预知。这个系统无法做到让所有人满意，去重的缺陷会一直存在于系统之中。\n我对于越来越严重的重复问题并没有一个好的解决方案。但是我想指出，早期在 Stack Exchange 有很多先例，它们把网站分为“初级”和“高级”区域，不同区域的规则不同。我们在别的地方也能找到类似的例子，比如 Math 和 MathOverflow，English 和 English Learners， Unix 和 Ubuntu，也许是时候搞一个以初级用户为主的 Stack Overflow了，在那里我们可以允许多一些重复，少一些规则。<p/>\nStack Overflow是个可以同行评审的竞争性系统\nStack Overflow确实是个相当明确的竞争性系统，它的一大标志就是“总会有更好的解决办法”。根据我的多年观察，激励开发者最有效的方式就是…巧妙地暗示出别人的解决方案也许比你的更好。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/geek-hero-motivating-programmers.png\" alt=\"- 你好Randall。医生说你能听到我说话，虽然你看起来像植物人。我是来告诉你，别着急慢慢康复。因为Ross接替了你的工作，并且做的非常好。他甚至找到了你代码里的瓶颈，还说他改过的代码变快了两倍。 - 这不可能！！！！！我现在就回办公室！\" />  你好Randall。医生说你能听到我说话，虽然你看起来像植物人。我是来告诉你，别着急慢慢康复。因为Ross接替了你的工作，并且做的非常好。他甚至找到了你代码里的瓶颈，还说他改过的代码变快了两倍。<p/>\n 这不可能！！！！！我现在就回办公室！<p/>\nStack Overflow的竞争性质体现在了它的公开声望系统上，就是用户名旁边那个拥有神奇力量的数字。所有的声望值都来源于其他用户，而不是所谓的系统。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stack-overflow-top-rep-by-year.png\" alt=\"\" /><p/>\n每当你提出问题或者提交回答时，你的问题或回答都可以被其他用户指指点点，他们可以编辑、标记、关闭、打开、顶、踩或者收起。这样做的目的是让 Stack Overflow 成为一个同行评审和友好竞争的系统，就像在公司里，你的代码被你从没见过的另一个部门的人来评审。有人以友好的方式去质疑你所提问题的提论，也是完全合理的，比如，你真的想用这个正则表达式去匹配 HTML 吗？<p/>\n我完全清楚这种竞争性质的同行评审系统，并不适合每一个人。Stack Overflow 采用维基百科的模式，导致它存在不能接受重复内容这样的限制。那么根据你的情况与背景，同行评审时，你收到的评价可能会让你觉得不舒服。<p/>\n我听部分用户反应，在 Stack Overflow 提问的过程中会感觉到焦虑。对我来说，在 Stack Overflow上提问，应该感受到一种 ”我要展示出我最好的一面“ 的正常焦虑：<p/>\n在你的同事面前演讲的焦虑\n考试要取得好成绩的焦虑\n开始新工作，与你尊敬的优秀同事们一起工作的焦虑\n第一天去学校报到，即将见到新同学的焦虑\n至于那种完全不会感到焦虑的地方，我唯一能想到的就是，从事了很久的工作，已经不再关注与工作本身，因此也没有那种担心有一天就会丢了工作的焦虑。这样怎么会好呢？所以说我不喜欢零焦虑的系统。<p/>\n也许你不喜欢竞争。那么能不能有个少量竞争模式的问答系统呢？一个没有投支持或者反对票功能的系统，这样无论发表什么内容都不会感觉焦虑。这就像是一个全是你的支持者的网络，大家都相信你，希望你成功。这当然也是可以的。我认为应该有类似这样的网站，用户可以根据自己的需求与目标来选择适合自己的体验。那么 Stack 应该建立一个这样模式的社区吗？这样的社区已经有了吗？这是个开放题。也请随意在留言区发表你的看法。<p/>\nStack Overflow是为了日常开发者而设计的\nStack Overflow的目标用户到底是谁，这也是经常容易被混淆的一点。这个回答很直观，而且从从未改变过：<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stackoverflow-for-business-description.png\" alt=\"\" /><p/>\n一个为专业和热情的程序员而存在的问答平台。这是指：<p/>\n当前正在从事程序开发职业的人，或者如果愿意就能立即胜任程序开发工作的人。<p/>\n如果你觉得好奇，这个定义的一部分是公开的商业决策。为了盈利，你的用户群体必须要有一部分拿着开发者薪水的人，或者在找开发者的工作的人。整个 Stack Overflow 社区也许有着知识共享的标签，但是它并不是个非营利组织。我们的出发点是可持续经营，这也是为什么我们在 Stack Overflow 上线一年之后，就成立 Stack Overflow Careers 招聘平台的原因，回顾一下，成立的确实有点过早了。为了实现比 2009 好很多的集成化用户体验，招聘平台被归入了 Stack Overflow，放在了 stackoverflow.com/jobs下面。<p/>\n用户的选择定位并不是说要排斥非开发者，但是 Stack Overflow 确实是一个有着严格同行评审，对已经在从事相关行业的人来说非常优秀的功能，但同时也是对于学生或者初学者来说很不友好的功能。这也是为什么，我每次在推特上，看到有人推荐学生去 Stack Overflow 找答案时，我会小心翼翼的建议不要这样。对于开发领域的新手或者学生来说，他们需要的，与 Stack Overflow 所提供的是完全相反的。他们需要的是：<p/>\n一对一的指导\n实时屏幕共享协作\n理论背景知识课程\n初学者练习\n一个练习与实验的场所\n这些都是对初学者来说，很好很合理的事情，但是 Stack Overflow 一个也不做。你可以通过 Stack Overflow 来从头学习如何编程吗？理论上你可以通过任何软件做任何事情，你甚至可以通过 Reddit 与人进行日常交流，如果你是受虐狂的话。但是答案还是肯定的，理论上你可以通过 Stack Overflow 学习如何编程，如果你是喜欢竞争模式（声望、被关闭、被踩）的神童，也完全能接受要去帮助别人而不只是自己学习知识这一观点。但是我强烈不推荐这样做。对初学者来说，除了 Stack Overflow 外还有很多更好更合适的平台。那么 Stack Overflow 能不能成为一个适合新手和学生的平台呢？我不清楚，我也不能决定。<p/>\n这些就是我要说的。我们可以不再与深渊对视。<p/>\n我希望我的观点对 Stack Overflow 不会有什么负面影响。总的来说，我认为目前的 Stack Overflow 很强大。但是，无论是 2008 年还是2018 年，我怎么想有关系吗？<p/>\nStack Overflow 是你们的。 Stack Overflow 将信念赌在了这一点：信任你的同行。Stack Overflow 的成长离不开那些积极参与社区讨论的开发者们。是你们让我相信开发者社区是最好的学习与成长的地方。是你们让我收获了如此多的对于 Stack Overflow 的赞誉。这不是我的功劳，而是你们的。 很久之前我在 Code Horror 上就知道了合作的力量是多么强大。目前我们的社区已经达到我今生难以企及的高度。我唯一能要求的，或者是我们能要求的，就是大家互相帮助互相成长。 如果有人认可了你的付出，那么你值得为此感到骄傲。<p/>\n开发者社区的力量能够创造以及终结 Stack Overflow。Stack Overflow 长大了会成为什么？它的未来将由我们共同创造。<p/>\n<img src=\"https://blog.codinghorror.com/content/images/2018/10/stackoverflow-none-of-us-is-as-dumb-as-all-of-us.jpg\" alt=\"\" /><p/>\nPS：Stack Overflow 十周年快乐！<p/>\n 1 收藏\n 1 评论\n", "front_img_path": "full/217248f366b3ffdce01fd5df8546529db6cbed0f.jpg"},{"title": "网络应用优化——时延与带宽", "url": "http://blog.jobbole.com/114523/", "create_date": "2018/11/22", "front_img_url_download": ["http://jbcdn2.b0.upaiyun.com/2016/04/149000d5c8b1bb08b8de6914ed749f28.gif"], "fav_nums": 1, "comment_nums": 0, "vote_nums": 1, "tags": "IT技术,带宽,时延", "object_id": "45af5cf8efb28987fbf48ee838c1d6f8", "content": "2.1.糖果包装问题\n某厂某天生产了N颗糖果，需要进行包装和验收。流水线一端的工人A负责包装，包装速度为N1颗/小时，另一端B的验收速度为N2颗/小时，两者通过协调达成某一相同的速度MIN（N1,N2）颗/小时。通过传送带传送到目的地。传送带长L米，速度为V 米/小时。从A开始计时到B验收完成，所需时间T为多少？\n答：T = L / V + N / MIN（N1,N2） + 1 / MIN（N1,N2）\n时间T反映了完成糖果包装并验收完成的总时间。如果糖果想象成可以需要传输的文件，那么就可以把糖果包装问题转换为一个简化的网络传输问题。\n假设服务器A向用户B发送一个大小为100KB的图片（假设HTTP连接已经建立），服务器上行带宽为1Mbps，用户所在下行带宽为100Mbps。已知端对端物理距离为2000 km，光信号在光纤中的传播速度是200000 km/s，求图片从A发出到B完整接收的时间T。代入公式可得： T = 2000/200 + 100 * 8 / 1 = 810 ms（最后一项太小可以被忽略）\n这个时间就是时延，具体的说是单向时延，即一个数据文件从传输到完整接收所花费的时间。\n2.2.时延是什么\n2.1中图片传输的时间叫做时延。时延并没有一个确切的定义。多数情况下是指单向时延，就是在数据通信过程中从A发送数据的第一个比特开始到B接受到数据的最后一个比特为结束产生的时间消耗，在某些场景下也指双向时延，即从网络请求发出到收到完整响应为结束经历的时间。时延常以毫秒为单位来衡量。数据包的大小、链路上传下行速率、通信距离、通信介质的种类、路由器的处理能力都会影响时延。常说的时延是下列这些不同时延的总和：\n传播时延。信号在信道中传输的时间=通信距离/传播速度。\n处理时延。路由器路由、差错控制以及数据包头信息处理的时间。\n队列时延。数据包在队列中等待路由器处理的时间。\n发送时延。将数据包发送到信道中的时间=数据包大小/信道带宽。\n减少时延往往比增加带宽需要更多的成本。2015年9月，Hibernia网络公司为了最大程度上确保纽约和伦敦的通信延时，部署了一条名为“Hibernia Express”的海底光缆，总计耗费达3亿美元。采用新光缆之后，纽约伦敦两地的延时为58.95ms，比现存的所有大西洋光缆少了5ms。这意味着节约的每1毫秒，价值近6千万美元。\n2.3.带宽是什么\n带宽是指数据通信最大的吞吐量，根据传输方向的不同可以分为上行带宽和下行带宽，常用Mbps来进行衡量。对于互联网上的用户，运营商（ISP）提供的带宽就是数据通信的最大吞吐量，并且上下行带宽往往不对称。如中国电信百兆宽带最大下行速度为100Mbps，而最大上行速度只有20Mbps。\n一般来说，核心网络（如海底光缆）的带宽往往可以达到几百Tbps。而终端用户实际可用的带宽，往往是网络服务所在服务器的上行带宽与用户下行带宽的最小值。\n若某一网站部署在上行带宽为1Mbps服务器上，那么即使访问者拥有100Mbps的下行带宽，用户仍然只能以1Mbps的速度下载网页上的内容。\n2.4.联系与区别\n对终端用户而言，延时可以理解为某一网络服务的响应速度，而带宽可以理解为上传下载文件的最大速度，而实际可用的带宽，往往又是由网络服务所在服务器的上行带宽与用户下行带宽的最小值所决定。\n以浏览网页为例子，若响应速度快，用户实际可用的带宽（见2.3节的定义）小，就可能导致页面上的图片以肉眼可见的速度一点点显示出来；若响应速度慢，用户实际可用的带宽大，就可能导致页面上的内容需要等待很久才能有显示，在此之前都是空白。但是当响应完成，会立即显示网页内容。\n有人说带宽和时延没有关系，这句话是有问题的。因为在2.2节中介绍了发送延时，它通常由服务器的上行带宽与用户下行带宽的最小值所决定。准确的说是，目前现实场景中大部分的时延不是由带宽决定，而往往是由传播距离、网络状况等所决定。\n3.性能优化\n了解了带宽和时延，那么就可以更好地理解网站性能优化背后的本质——减少延时，增加带宽。常见的性能优化的方式有合并请求和建立内容分发网络（CDN）：\n合并请求。从优化角度来说，合并请求就是在减少总时延。一个100KB的文件和 10个10KB大小的文件大小相同。若D为传播时延，T为发送10KB文件的发送时延，那么一次发送100KB文件的时延为 D + 10T，而发送10次单个10KB文件的延时为 10D+10T。相同情况下，请求次数越少，总时延就越少。\n建立内容分发网络。内容分发网络通过将网站内容服务器分布在靠近用户的位置，从而使用户就近获取所需内容，减少传播延时，进而显著提升网站的响应速度。\n举例来说，高质量的视频网站必须租用高速的上行带宽，确保能够承担大规模的视频流量，因为没人愿意等待几分钟缓冲一个流媒体视频。同时它也要在各地建立大规模的内容分发网络（CDN）来降低视频内容的传播延迟，这样才不会让用户等待很久才能获得网站的响应。\n本文网络应用优化中最重要的两个目标：时延与带宽。通过定义并区分两者的关系，并引申出网络服务优化的几种基本方法。和所有的教程一样，本文不可能涵盖到网络优化的所有细节，但是若能对你有所启发，那就是再好不过了。\n5.参考链接\nPrimer on Latency and Bandwidth\nBandwidth vs. Latency: What is the Difference?\n 1 收藏\n", "front_img_path": "full/e8f8b7fe8c56ba964bd5eb1fadab627b06618a67.jpg"},{"title": "通过信鸽来解释 HTTPS", "url": "http://blog.jobbole.com/114442/", "create_date": "2018/10/15", "front_img_url_download": ["https://cdn-images-1.medium.com/max/1600/1*vHF6NNdZX9ziiW_uRYzvAA.png"], "fav_nums": 1, "comment_nums": 4, "vote_nums": 1, "tags": "IT技术,,HTTPS", "object_id": "f86d91ab81e8894dc4a919148b70d95c", "content": "本文由 伯乐在线 - tsteho 翻译，艾凌风 校稿。未经许可，禁止转载！英文出处：freecodecamp。欢迎加入翻译组。密码学可能是一个难以理解的主题。它充满了数学证明。除非你真的需要开发密码系统，否则，如果你只想从宏观的角度了解密码学，你并不需要理解这些复杂的内容。<p/>\n如果你抱着能够创建下一个 HTTPS 协议的期望打开这篇文章，我不得不遗憾地表示只有信鸽是不够的。否则，煮一些咖啡，享受这篇文章吧。<p/>\nAlice，Bob 和信鸽？\n<img src=\"https://camo.githubusercontent.com/6723aa4811cb162a1fbb8e4525f43afd5672e7b9/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a764846364e4e645a58397a6969575f7552597a7641412e706e67\" alt=\"\" data-canonical-src=\"https://cdn-images-1.medium.com/max/1600/1*vHF6NNdZX9ziiW_uRYzvAA.png\" /><p/>\n你在互联网上的任何活动（阅读这篇文章，在亚马逊上买东西，上传猫咪的图片）都归结为向服务器发送消息和从服务器接收消息。<p/>\n这么讲听起来可能有点抽象，因此，我们不妨假设这些信息是通过信鸽传递的。我明白这么假设显得很随意，但请相信我：HTTPS 的工作原理就是这样的，只是快得多。<p/>\n本文中我们并不会使用，服务器，客户端和黑客这样的术语，取而代之的是，我们会依次使用人名 Alice、 Bob 和 Mallory来代替它们。如果你不是第一次尝试理解密码概念，你可以认出这些名字，因为它们被广泛用于技术文献中。<p/>\n一次简单的通讯\n如果 Alice 想要给 Bob 传递一条信息，她将信息绑在信鸽的腿上，然后让信鸽传给 Bob。Bob 收到信息，读取信息。一切都正常。<p/>\n但要是 Mallory 途中拦截了 Alice 的信鸽，并且改变了信息的内容？Bob 无法知道 Alice 发送的信息在传递途中被修改了。<p/>\n这就是 HTTP 的工作原理。挺可怕的，对吧？我不会通过 HTTP 协议来发送我的银行凭证，你也不应该这么做。<p/>\n那如果 Alice 和 Bob 都很机灵呢。他们同意将使用密令来写信息。他们将字母表中的每个字母偏移 3 个位置。比如：D -> A, E -> B, F -> C。明文“secret message”将转换成“pbzobq jbppxdb”。<p/>\n现在，如果 Mallory 拦截了信鸽，她既不能把信息改变成一些有意义的信息，也不能明白信息里说的内容，因为她不知道密令。但是Bob可以简单地反向应用密令（A -> D, B -> E, C -> F）将信息解密。密文“pbzobq jbppxdb”将被解密回“secret message”。<p/>\n大功告成！<p/>\n这被称为对称密钥密码术，因为当你知道如何加密一条信息，你也知道如何给信息解密。<p/>\n我在上面介绍的密令通常被称为凯撒密码。在现实生活中，我们使用更高级和复杂的密令，但是主要思路是相同的。<p/>\n我们如何决定密钥是什么？\n如果只有发送方和接收方知道密钥，对称密钥密码术是很安全的。在凯撒密码中，密钥是一个偏移值，这个偏移值决定每个字母应该偏移多少。在我们的例子中，我们使用的偏移值是 3，但是也能是 4 或者 12。<p/>\n不过这么设计会有个问题：在用信鸽传递信息之前，如果 Alice 和 Bob 之前从没见过，他们没有安全的方式创建一个密钥。如果他们将密钥包含在信息之中，Mallory 将拦截信息并且发现密钥。后果就是：无论 Alice 和 Bob 发送的信息是否加密，Mallory 都能读取或者改变拦截到的信息。<p/>\n这是一个典型的中间人攻击例子。避免它的唯一方法是改变之前的密码系统。<p/>\n携带盒子的信鸽\n所以 Alice 和 Bob 想出了一个更好的系统。当 Bob 想要发送信息给 Alice 时，Alice 将遵照下面的流程：<p/>\nBob 向 Alice 传送一只信鸽，信鸽不携带任何信息。\nAlice 将这只信鸽传回给 Bob，信鸽携带一只开着锁的盒子以及密钥。\nBob 把信息放到盒子里，将锁锁上，将盒子传送给 Alice。\nAlice 收到盒子，使用密钥打开盒子，读取信息。\n通过这种方式传递信息，Mallory 不可能通过拦截信鸽的方式来改变信息，因为她没有密钥。当 Alice 想要向 Bob 发送信息时，遵循相同的流程。<p/>\nAlice 和 Bob 刚刚使用了通常所说的非对称密钥密码术。之所以称它为非对称，是因为即使你可以加密一条信息（锁上盒子）但你也不能将它解密（打开锁住的盒子）。<p/>\n我怎么信任这个盒子？\n如果你够仔细的话，你可能已经意识到我们仍然有一个问题。当 Bob 收到那个开着的盒子时，他如何确信这是来自 Alice 的盒子，而不是 Mallory 拦截信鸽后，将来自于 Alice 的盒子替换成 Mallory 自己设置了密钥后的盒子。<p/>\nAlice 决定对盒子进行数字签名，通过这种方式，当 Bob 收到盒子，他通过核对签名的一致性来确定盒子是否来自 Alice。<p/>\n有些人可能就会想 Bob 如何识别 Alice 的签名？不错的问题。Alice 和 Bob 也有同样的疑问。因此他们决定让 Ted 对盒子进行数字签名，而不是 Alice。<p/>\nTed 是谁？Ted 是一个著名且值得信赖的人。每个人都可以从Ted 那里获得签名，每个人都相信Ted 只会为合法的人提供盒子的数字签名服务。<p/>\n只有当Ted 确信正在请求签名的人是 Alice，Ted 才会为 Alice 提供盒子数字签名的服务。因此 Mallory 不能再像之前那样拦截 Alice 的盒子、替换盒子后传送给 Bob 了，因为 Bob 会发现这个盒子在 Ted 那进行数字签名的是 Mallory，而不是 Alice。<p/>\nTed 在技术术语中通常被称为证书颁发机构，你阅读这篇文章所使用的浏览器安装着各种证书颁发机构的签名。<p/>\n因此当你第一次连接到一个网站，你信任它的盒子，因为你信任 Ted，而 Ted 告诉你这个盒子是合法的。<p/>\n盒子太重了\nAlice 和 Bob 现在有了一个可靠的通信系统，但是他们意识到和仅仅携带信息的信鸽相比，携带盒子的信鸽太慢了。<p/>\n他们决定只在传递密钥的时候使用盒子的方法（非对称密码术），加密信息使用对称密码术（记得之前提到的凯撒密码？）。<p/>\n这样的话可谓两全其美：非对称密码术的可靠性和对称密码术的效率都有了。<p/>\n在现实世界中，“信鸽”的传送速度都很快，但尽管这么讲，使用非对称密码技术加密消息比使用对称密码技术慢，所以我们只使用它来交换加密密钥。<p/>\n现在你知道了HTTPS的工作原理，你的咖啡也该煮好了。去喝吧，这是你应得的 <p/>\n 1 收藏\n 4 评论\n", "front_img_path": ""}